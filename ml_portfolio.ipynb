{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOPCsqeuub9is6wW0/c/wLh"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 375,
      "metadata": {
        "id": "e4PiESSn5Wx4"
      },
      "outputs": [],
      "source": [
        "# ÔºÅpip install functorch\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.autograd import Variable\n",
        "import pandas as pd\n",
        "import scipy\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "from os import path\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MA_DAYS = 25\n",
        "trading_days_in_year = 252"
      ],
      "metadata": {
        "id": "26v-Cnl1lg6t"
      },
      "execution_count": 376,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import raw data from yahoo finance"
      ],
      "metadata": {
        "id": "MRi8JtX9gAb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_files_path_prefix = \"/content/drive/MyDrive\"\n",
        "data_files_path = \"ML-Portfolio-Data\"\n",
        "data_files_path = path.join(data_files_path_prefix, data_files_path)\n",
        "\n",
        "high_risk_file = 'SPY.csv'\n",
        "low_risk_file = 'IEF.csv'\n",
        "high_risk = pd.read_csv(path.join(data_files_path, high_risk_file))\n",
        "low_risk = pd.read_csv(path.join(data_files_path, low_risk_file))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puHUCbHEMIoP",
        "outputId": "7ce5b9de-1204-4837-b457-bf690b88c1c4"
      },
      "execution_count": 377,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read files from the same directory\n",
        "#high_risk = pd.read_csv('SPY.csv')\n",
        "#low_risk = pd.read_csv('IEF.csv')"
      ],
      "metadata": {
        "id": "tmNxB-dXPdjh"
      },
      "execution_count": 378,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(high_risk.shape)\n",
        "print(low_risk.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEp0KCBSE3uX",
        "outputId": "de094f91-f1b2-4dcf-ee78-a03623b4ab95"
      },
      "execution_count": 379,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5147, 7)\n",
            "(5147, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "high_risk.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ffmGB06pT8_q",
        "outputId": "b36ddf0d-daa5-46a6-9fea-12772556c20b"
      },
      "execution_count": 380,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Date       Open       High        Low      Close  Adj Close    Volume\n",
              "0  2002-07-30  89.320000  91.400002  88.720001  90.940002  61.380939  47532200\n",
              "1  2002-07-31  90.489998  91.550003  89.250000  91.160004  61.529453  44669900\n",
              "2  2002-08-01  90.879997  91.349998  88.330002  88.779999  59.923054  66571900\n",
              "3  2002-08-02  88.500000  88.910004  85.620003  86.790001  58.579895  51772900\n",
              "4  2002-08-05  86.489998  86.930000  83.550003  83.769997  56.541496  47191300"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9a17a8c9-4712-485f-b65f-19a72a8f694e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2002-07-30</td>\n",
              "      <td>89.320000</td>\n",
              "      <td>91.400002</td>\n",
              "      <td>88.720001</td>\n",
              "      <td>90.940002</td>\n",
              "      <td>61.380939</td>\n",
              "      <td>47532200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2002-07-31</td>\n",
              "      <td>90.489998</td>\n",
              "      <td>91.550003</td>\n",
              "      <td>89.250000</td>\n",
              "      <td>91.160004</td>\n",
              "      <td>61.529453</td>\n",
              "      <td>44669900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2002-08-01</td>\n",
              "      <td>90.879997</td>\n",
              "      <td>91.349998</td>\n",
              "      <td>88.330002</td>\n",
              "      <td>88.779999</td>\n",
              "      <td>59.923054</td>\n",
              "      <td>66571900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2002-08-02</td>\n",
              "      <td>88.500000</td>\n",
              "      <td>88.910004</td>\n",
              "      <td>85.620003</td>\n",
              "      <td>86.790001</td>\n",
              "      <td>58.579895</td>\n",
              "      <td>51772900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2002-08-05</td>\n",
              "      <td>86.489998</td>\n",
              "      <td>86.930000</td>\n",
              "      <td>83.550003</td>\n",
              "      <td>83.769997</td>\n",
              "      <td>56.541496</td>\n",
              "      <td>47191300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9a17a8c9-4712-485f-b65f-19a72a8f694e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9a17a8c9-4712-485f-b65f-19a72a8f694e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9a17a8c9-4712-485f-b65f-19a72a8f694e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 380
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "low_risk.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "DY4FPgNCEssV",
        "outputId": "d6a79697-7e05-4a6d-da6d-0b9e3d3b5093"
      },
      "execution_count": 381,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Date       Open       High        Low      Close  Adj Close  Volume\n",
              "0  2002-07-30  81.940002  82.120003  81.699997  81.769997  45.672558   41300\n",
              "1  2002-07-31  82.050003  82.580002  82.050003  82.519997  46.091442   32600\n",
              "2  2002-08-01  82.540001  82.900002  82.519997  82.860001  46.281376   71400\n",
              "3  2002-08-02  83.019997  83.699997  82.900002  83.500000  46.638828  120300\n",
              "4  2002-08-05  83.680000  83.919998  83.529999  83.919998  46.873459  159300"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-26704486-1c18-4ee6-bb13-2cb8e69e0e83\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2002-07-30</td>\n",
              "      <td>81.940002</td>\n",
              "      <td>82.120003</td>\n",
              "      <td>81.699997</td>\n",
              "      <td>81.769997</td>\n",
              "      <td>45.672558</td>\n",
              "      <td>41300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2002-07-31</td>\n",
              "      <td>82.050003</td>\n",
              "      <td>82.580002</td>\n",
              "      <td>82.050003</td>\n",
              "      <td>82.519997</td>\n",
              "      <td>46.091442</td>\n",
              "      <td>32600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2002-08-01</td>\n",
              "      <td>82.540001</td>\n",
              "      <td>82.900002</td>\n",
              "      <td>82.519997</td>\n",
              "      <td>82.860001</td>\n",
              "      <td>46.281376</td>\n",
              "      <td>71400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2002-08-02</td>\n",
              "      <td>83.019997</td>\n",
              "      <td>83.699997</td>\n",
              "      <td>82.900002</td>\n",
              "      <td>83.500000</td>\n",
              "      <td>46.638828</td>\n",
              "      <td>120300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2002-08-05</td>\n",
              "      <td>83.680000</td>\n",
              "      <td>83.919998</td>\n",
              "      <td>83.529999</td>\n",
              "      <td>83.919998</td>\n",
              "      <td>46.873459</td>\n",
              "      <td>159300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-26704486-1c18-4ee6-bb13-2cb8e69e0e83')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-26704486-1c18-4ee6-bb13-2cb8e69e0e83 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-26704486-1c18-4ee6-bb13-2cb8e69e0e83');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 381
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Dataset for ML Portfolio"
      ],
      "metadata": {
        "id": "RnPRd0TkrMsN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset"
      ],
      "metadata": {
        "id": "IvSntSIA4qiC"
      },
      "execution_count": 382,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enrich data"
      ],
      "metadata": {
        "id": "5EZ0sQ3rTgn4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate daily returns"
      ],
      "metadata": {
        "id": "NraBWrzef4BA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_daily_return(market_data):\n",
        "    market_data[\"Daily Return\"]  = market_data['Close'] - market_data['Open']\n",
        "\n",
        "add_daily_return(high_risk)\n",
        "add_daily_return(low_risk)"
      ],
      "metadata": {
        "id": "2e03XXEgf1r4"
      },
      "execution_count": 383,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate moving average (MA) of daily returns"
      ],
      "metadata": {
        "id": "2dqBMvFWo4oQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_moving_average(market_data, ma_days):\n",
        "    temp_vars = []\n",
        "\n",
        "    # df = market_data\n",
        "    for i in range(0,ma_days):\n",
        "        temp_var = \"M_{0}\".format(i)\n",
        "        market_data[temp_var] = market_data[\"Daily Return\"].shift(i)\n",
        "        temp_vars.append(temp_var)\n",
        "\n",
        "    market_data[\"MA\"] = market_data[temp_vars].mean(axis=1)\n",
        "\n",
        "    for i in range(0,ma_days):\n",
        "        temp_var = \"M_{0}\".format(i)\n",
        "        market_data.drop(temp_var, axis = 1, inplace = True)\n",
        "\n",
        "add_moving_average(high_risk, MA_DAYS)\n",
        "add_moving_average(low_risk, MA_DAYS)\n"
      ],
      "metadata": {
        "id": "p9EW2Fzjo9ly"
      },
      "execution_count": 384,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "high_risk.tail()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "xp5iRKB4FeRZ",
        "outputId": "a347b9d6-410f-462f-ddcf-bf7afbb42a1f"
      },
      "execution_count": 385,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            Date        Open        High         Low       Close   Adj Close  \\\n",
              "5142  2022-12-30  380.640015  382.579987  378.429993  382.429993  382.429993   \n",
              "5143  2023-01-03  384.369995  386.429993  377.829987  380.820007  380.820007   \n",
              "5144  2023-01-04  383.179993  385.880005  380.000000  383.760010  383.760010   \n",
              "5145  2023-01-05  381.720001  381.839996  378.760010  379.380005  379.380005   \n",
              "5146  2023-01-06  382.609985  389.250000  379.410004  388.079987  388.079987   \n",
              "\n",
              "         Volume  Daily Return        MA  \n",
              "5142   83975100      1.789978 -0.368799  \n",
              "5143   74850700     -3.549988 -0.530798  \n",
              "5144   85934100      0.580017 -0.380398  \n",
              "5145   76970500     -2.339996 -0.441199  \n",
              "5146  104041300      5.470002 -0.709999  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bdfe9aeb-3323-457d-8aa7-6b0883331006\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Daily Return</th>\n",
              "      <th>MA</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5142</th>\n",
              "      <td>2022-12-30</td>\n",
              "      <td>380.640015</td>\n",
              "      <td>382.579987</td>\n",
              "      <td>378.429993</td>\n",
              "      <td>382.429993</td>\n",
              "      <td>382.429993</td>\n",
              "      <td>83975100</td>\n",
              "      <td>1.789978</td>\n",
              "      <td>-0.368799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5143</th>\n",
              "      <td>2023-01-03</td>\n",
              "      <td>384.369995</td>\n",
              "      <td>386.429993</td>\n",
              "      <td>377.829987</td>\n",
              "      <td>380.820007</td>\n",
              "      <td>380.820007</td>\n",
              "      <td>74850700</td>\n",
              "      <td>-3.549988</td>\n",
              "      <td>-0.530798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5144</th>\n",
              "      <td>2023-01-04</td>\n",
              "      <td>383.179993</td>\n",
              "      <td>385.880005</td>\n",
              "      <td>380.000000</td>\n",
              "      <td>383.760010</td>\n",
              "      <td>383.760010</td>\n",
              "      <td>85934100</td>\n",
              "      <td>0.580017</td>\n",
              "      <td>-0.380398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5145</th>\n",
              "      <td>2023-01-05</td>\n",
              "      <td>381.720001</td>\n",
              "      <td>381.839996</td>\n",
              "      <td>378.760010</td>\n",
              "      <td>379.380005</td>\n",
              "      <td>379.380005</td>\n",
              "      <td>76970500</td>\n",
              "      <td>-2.339996</td>\n",
              "      <td>-0.441199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5146</th>\n",
              "      <td>2023-01-06</td>\n",
              "      <td>382.609985</td>\n",
              "      <td>389.250000</td>\n",
              "      <td>379.410004</td>\n",
              "      <td>388.079987</td>\n",
              "      <td>388.079987</td>\n",
              "      <td>104041300</td>\n",
              "      <td>5.470002</td>\n",
              "      <td>-0.709999</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bdfe9aeb-3323-457d-8aa7-6b0883331006')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bdfe9aeb-3323-457d-8aa7-6b0883331006 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bdfe9aeb-3323-457d-8aa7-6b0883331006');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 385
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "low_risk.tail()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "rtkml8ylGG47",
        "outputId": "ec8ef69a-0f84-48f6-ab36-b1af4e4222fe"
      },
      "execution_count": 386,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            Date       Open       High        Low      Close  Adj Close  \\\n",
              "5142  2022-12-30  95.860001  96.269997  95.620003  95.779999  95.779999   \n",
              "5143  2023-01-03  96.910004  97.000000  96.339996  96.529999  96.529999   \n",
              "5144  2023-01-04  97.339996  97.419998  96.989998  97.269997  97.269997   \n",
              "5145  2023-01-05  96.699997  97.220001  96.570000  97.129997  97.129997   \n",
              "5146  2023-01-06  97.169998  98.430000  97.080002  98.379997  98.379997   \n",
              "\n",
              "       Volume  Daily Return        MA  \n",
              "5142  5039800     -0.080002  0.050399  \n",
              "5143  6808300     -0.380005  0.025599  \n",
              "5144  7800100     -0.069999  0.025599  \n",
              "5145  3177900      0.430000  0.043600  \n",
              "5146  6807700      1.209999  0.050399  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b1e4d263-d0f6-42cc-b436-7fd36fd614b1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Daily Return</th>\n",
              "      <th>MA</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5142</th>\n",
              "      <td>2022-12-30</td>\n",
              "      <td>95.860001</td>\n",
              "      <td>96.269997</td>\n",
              "      <td>95.620003</td>\n",
              "      <td>95.779999</td>\n",
              "      <td>95.779999</td>\n",
              "      <td>5039800</td>\n",
              "      <td>-0.080002</td>\n",
              "      <td>0.050399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5143</th>\n",
              "      <td>2023-01-03</td>\n",
              "      <td>96.910004</td>\n",
              "      <td>97.000000</td>\n",
              "      <td>96.339996</td>\n",
              "      <td>96.529999</td>\n",
              "      <td>96.529999</td>\n",
              "      <td>6808300</td>\n",
              "      <td>-0.380005</td>\n",
              "      <td>0.025599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5144</th>\n",
              "      <td>2023-01-04</td>\n",
              "      <td>97.339996</td>\n",
              "      <td>97.419998</td>\n",
              "      <td>96.989998</td>\n",
              "      <td>97.269997</td>\n",
              "      <td>97.269997</td>\n",
              "      <td>7800100</td>\n",
              "      <td>-0.069999</td>\n",
              "      <td>0.025599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5145</th>\n",
              "      <td>2023-01-05</td>\n",
              "      <td>96.699997</td>\n",
              "      <td>97.220001</td>\n",
              "      <td>96.570000</td>\n",
              "      <td>97.129997</td>\n",
              "      <td>97.129997</td>\n",
              "      <td>3177900</td>\n",
              "      <td>0.430000</td>\n",
              "      <td>0.043600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5146</th>\n",
              "      <td>2023-01-06</td>\n",
              "      <td>97.169998</td>\n",
              "      <td>98.430000</td>\n",
              "      <td>97.080002</td>\n",
              "      <td>98.379997</td>\n",
              "      <td>98.379997</td>\n",
              "      <td>6807700</td>\n",
              "      <td>1.209999</td>\n",
              "      <td>0.050399</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b1e4d263-d0f6-42cc-b436-7fd36fd614b1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b1e4d263-d0f6-42cc-b436-7fd36fd614b1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b1e4d263-d0f6-42cc-b436-7fd36fd614b1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 386
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate ROE"
      ],
      "metadata": {
        "id": "O6a-Fc3EZNxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_roe(market_data):    \n",
        "    market_data[\"Next Close\"] = market_data[\"Close\"].shift(-1)\n",
        "    market_data[\"ROE\"] = (market_data[\"Next Close\"] - market_data[\"Close\"]) / market_data['Close']\n",
        "\n",
        "add_roe(high_risk)\n",
        "add_roe(low_risk)"
      ],
      "metadata": {
        "id": "42UscmnQZMpE"
      },
      "execution_count": 387,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_roe_binary(market_data, tau=-0.005):    \n",
        "    market_data[\"ROE Binary\"] = np.where(market_data[\"ROE\"].values < tau, 0, 1)\n",
        "\n",
        "add_roe_binary(high_risk)\n",
        "add_roe_binary(low_risk)"
      ],
      "metadata": {
        "id": "V5kEAXakgkCs"
      },
      "execution_count": 388,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "high_risk.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "lKK4t3UVfl_6",
        "outputId": "de36c6ea-0585-4d4c-b620-761563aa4849"
      },
      "execution_count": 389,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Date       Open       High        Low      Close  Adj Close  \\\n",
              "0  2002-07-30  89.320000  91.400002  88.720001  90.940002  61.380939   \n",
              "1  2002-07-31  90.489998  91.550003  89.250000  91.160004  61.529453   \n",
              "2  2002-08-01  90.879997  91.349998  88.330002  88.779999  59.923054   \n",
              "3  2002-08-02  88.500000  88.910004  85.620003  86.790001  58.579895   \n",
              "4  2002-08-05  86.489998  86.930000  83.550003  83.769997  56.541496   \n",
              "\n",
              "     Volume  Daily Return        MA  Next Close       ROE  ROE Binary  \n",
              "0  47532200      1.620002  1.620002   91.160004  0.002419           1  \n",
              "1  44669900      0.670006  1.145004   88.779999 -0.026108           0  \n",
              "2  66571900     -2.099998  0.063337   86.790001 -0.022415           0  \n",
              "3  51772900     -1.709999 -0.379997   83.769997 -0.034797           0  \n",
              "4  47191300     -2.720001 -0.847998   86.589996  0.033664           1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2f6d9886-750f-4ddc-a30f-8e4864e5bc1a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Daily Return</th>\n",
              "      <th>MA</th>\n",
              "      <th>Next Close</th>\n",
              "      <th>ROE</th>\n",
              "      <th>ROE Binary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2002-07-30</td>\n",
              "      <td>89.320000</td>\n",
              "      <td>91.400002</td>\n",
              "      <td>88.720001</td>\n",
              "      <td>90.940002</td>\n",
              "      <td>61.380939</td>\n",
              "      <td>47532200</td>\n",
              "      <td>1.620002</td>\n",
              "      <td>1.620002</td>\n",
              "      <td>91.160004</td>\n",
              "      <td>0.002419</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2002-07-31</td>\n",
              "      <td>90.489998</td>\n",
              "      <td>91.550003</td>\n",
              "      <td>89.250000</td>\n",
              "      <td>91.160004</td>\n",
              "      <td>61.529453</td>\n",
              "      <td>44669900</td>\n",
              "      <td>0.670006</td>\n",
              "      <td>1.145004</td>\n",
              "      <td>88.779999</td>\n",
              "      <td>-0.026108</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2002-08-01</td>\n",
              "      <td>90.879997</td>\n",
              "      <td>91.349998</td>\n",
              "      <td>88.330002</td>\n",
              "      <td>88.779999</td>\n",
              "      <td>59.923054</td>\n",
              "      <td>66571900</td>\n",
              "      <td>-2.099998</td>\n",
              "      <td>0.063337</td>\n",
              "      <td>86.790001</td>\n",
              "      <td>-0.022415</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2002-08-02</td>\n",
              "      <td>88.500000</td>\n",
              "      <td>88.910004</td>\n",
              "      <td>85.620003</td>\n",
              "      <td>86.790001</td>\n",
              "      <td>58.579895</td>\n",
              "      <td>51772900</td>\n",
              "      <td>-1.709999</td>\n",
              "      <td>-0.379997</td>\n",
              "      <td>83.769997</td>\n",
              "      <td>-0.034797</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2002-08-05</td>\n",
              "      <td>86.489998</td>\n",
              "      <td>86.930000</td>\n",
              "      <td>83.550003</td>\n",
              "      <td>83.769997</td>\n",
              "      <td>56.541496</td>\n",
              "      <td>47191300</td>\n",
              "      <td>-2.720001</td>\n",
              "      <td>-0.847998</td>\n",
              "      <td>86.589996</td>\n",
              "      <td>0.033664</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2f6d9886-750f-4ddc-a30f-8e4864e5bc1a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2f6d9886-750f-4ddc-a30f-8e4864e5bc1a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2f6d9886-750f-4ddc-a30f-8e4864e5bc1a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 389
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "low_risk.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "uurD8aCKfmH3",
        "outputId": "75c121fd-847a-4ed6-99a8-492a3ec29c9a"
      },
      "execution_count": 390,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Date       Open       High        Low      Close  Adj Close  Volume  \\\n",
              "0  2002-07-30  81.940002  82.120003  81.699997  81.769997  45.672558   41300   \n",
              "1  2002-07-31  82.050003  82.580002  82.050003  82.519997  46.091442   32600   \n",
              "2  2002-08-01  82.540001  82.900002  82.519997  82.860001  46.281376   71400   \n",
              "3  2002-08-02  83.019997  83.699997  82.900002  83.500000  46.638828  120300   \n",
              "4  2002-08-05  83.680000  83.919998  83.529999  83.919998  46.873459  159300   \n",
              "\n",
              "   Daily Return        MA  Next Close       ROE  ROE Binary  \n",
              "0     -0.170005 -0.170005   82.519997  0.009172           1  \n",
              "1      0.469994  0.149994   82.860001  0.004120           1  \n",
              "2      0.320000  0.206663   83.500000  0.007724           1  \n",
              "3      0.480003  0.274998   83.919998  0.005030           1  \n",
              "4      0.239998  0.267998   83.239998 -0.008103           0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-30e4ca5f-e345-4010-a9ba-5598bdc99898\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Daily Return</th>\n",
              "      <th>MA</th>\n",
              "      <th>Next Close</th>\n",
              "      <th>ROE</th>\n",
              "      <th>ROE Binary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2002-07-30</td>\n",
              "      <td>81.940002</td>\n",
              "      <td>82.120003</td>\n",
              "      <td>81.699997</td>\n",
              "      <td>81.769997</td>\n",
              "      <td>45.672558</td>\n",
              "      <td>41300</td>\n",
              "      <td>-0.170005</td>\n",
              "      <td>-0.170005</td>\n",
              "      <td>82.519997</td>\n",
              "      <td>0.009172</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2002-07-31</td>\n",
              "      <td>82.050003</td>\n",
              "      <td>82.580002</td>\n",
              "      <td>82.050003</td>\n",
              "      <td>82.519997</td>\n",
              "      <td>46.091442</td>\n",
              "      <td>32600</td>\n",
              "      <td>0.469994</td>\n",
              "      <td>0.149994</td>\n",
              "      <td>82.860001</td>\n",
              "      <td>0.004120</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2002-08-01</td>\n",
              "      <td>82.540001</td>\n",
              "      <td>82.900002</td>\n",
              "      <td>82.519997</td>\n",
              "      <td>82.860001</td>\n",
              "      <td>46.281376</td>\n",
              "      <td>71400</td>\n",
              "      <td>0.320000</td>\n",
              "      <td>0.206663</td>\n",
              "      <td>83.500000</td>\n",
              "      <td>0.007724</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2002-08-02</td>\n",
              "      <td>83.019997</td>\n",
              "      <td>83.699997</td>\n",
              "      <td>82.900002</td>\n",
              "      <td>83.500000</td>\n",
              "      <td>46.638828</td>\n",
              "      <td>120300</td>\n",
              "      <td>0.480003</td>\n",
              "      <td>0.274998</td>\n",
              "      <td>83.919998</td>\n",
              "      <td>0.005030</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2002-08-05</td>\n",
              "      <td>83.680000</td>\n",
              "      <td>83.919998</td>\n",
              "      <td>83.529999</td>\n",
              "      <td>83.919998</td>\n",
              "      <td>46.873459</td>\n",
              "      <td>159300</td>\n",
              "      <td>0.239998</td>\n",
              "      <td>0.267998</td>\n",
              "      <td>83.239998</td>\n",
              "      <td>-0.008103</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-30e4ca5f-e345-4010-a9ba-5598bdc99898')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-30e4ca5f-e345-4010-a9ba-5598bdc99898 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-30e4ca5f-e345-4010-a9ba-5598bdc99898');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 390
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build feature space"
      ],
      "metadata": {
        "id": "_L1SeZ_ggNPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_for_ma(market_data, ma_days):\n",
        "  return market_data[ma_days:]\n",
        "\n",
        "high_risk = remove_for_ma(high_risk, MA_DAYS)\n",
        "low_risk = remove_for_ma(low_risk, MA_DAYS)"
      ],
      "metadata": {
        "id": "TKS0pN_Ola5g"
      },
      "execution_count": 391,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(high_risk.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnHuHNjPmrQO",
        "outputId": "c6cec693-6dbc-4c47-c4ea-9432bbcc9ab1"
      },
      "execution_count": 392,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5122, 12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize_columns(market_data, columns):\n",
        "  for column in columns:\n",
        "    market_data[column] = market_data[column]/market_data[column].std()\n",
        "\n",
        "standardize_columns(high_risk, ['Volume', 'Daily Return', 'MA'])\n",
        "standardize_columns(low_risk, ['Volume', 'Daily Return', 'MA'])"
      ],
      "metadata": {
        "id": "0L7qIEJvdIvp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e0e68e2-e016-4448-d45a-7749e9abd9ee"
      },
      "execution_count": 393,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-393-d0986728a5f2>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  market_data[column] = market_data[column]/market_data[column].std()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "high_risk.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "tqgwASZBIDOa",
        "outputId": "99950709-6769-4043-da3f-d3bef97af4cb"
      },
      "execution_count": 394,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          Date       Open       High        Low      Close  Adj Close  \\\n",
              "25  2002-09-04  88.610001  90.250000  88.059998  89.540001  60.436001   \n",
              "26  2002-09-05  88.489998  89.430000  87.500000  88.779999  59.923054   \n",
              "27  2002-09-06  89.750000  90.570000  89.339996  90.000000  60.746498   \n",
              "28  2002-09-09  89.099998  91.349998  88.800003  90.660004  61.191929   \n",
              "29  2002-09-10  91.139999  91.779999  90.559998  91.699997  61.893936   \n",
              "\n",
              "      Volume  Daily Return        MA  Next Close       ROE  ROE Binary  \n",
              "25  0.550024      0.479605  0.276737   88.779999 -0.008488           0  \n",
              "26  0.723874      0.149555  0.229367   90.000000  0.013742           1  \n",
              "27  0.415721      0.128926  0.522307   90.660004  0.007333           1  \n",
              "28  0.365951      0.804501  0.929931   91.699997  0.011471           1  \n",
              "29  0.445799      0.288793  1.338801   91.129997 -0.006216           0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8a315ef1-7436-494c-8d69-09e8328a05fe\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Daily Return</th>\n",
              "      <th>MA</th>\n",
              "      <th>Next Close</th>\n",
              "      <th>ROE</th>\n",
              "      <th>ROE Binary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>2002-09-04</td>\n",
              "      <td>88.610001</td>\n",
              "      <td>90.250000</td>\n",
              "      <td>88.059998</td>\n",
              "      <td>89.540001</td>\n",
              "      <td>60.436001</td>\n",
              "      <td>0.550024</td>\n",
              "      <td>0.479605</td>\n",
              "      <td>0.276737</td>\n",
              "      <td>88.779999</td>\n",
              "      <td>-0.008488</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>2002-09-05</td>\n",
              "      <td>88.489998</td>\n",
              "      <td>89.430000</td>\n",
              "      <td>87.500000</td>\n",
              "      <td>88.779999</td>\n",
              "      <td>59.923054</td>\n",
              "      <td>0.723874</td>\n",
              "      <td>0.149555</td>\n",
              "      <td>0.229367</td>\n",
              "      <td>90.000000</td>\n",
              "      <td>0.013742</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>2002-09-06</td>\n",
              "      <td>89.750000</td>\n",
              "      <td>90.570000</td>\n",
              "      <td>89.339996</td>\n",
              "      <td>90.000000</td>\n",
              "      <td>60.746498</td>\n",
              "      <td>0.415721</td>\n",
              "      <td>0.128926</td>\n",
              "      <td>0.522307</td>\n",
              "      <td>90.660004</td>\n",
              "      <td>0.007333</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>2002-09-09</td>\n",
              "      <td>89.099998</td>\n",
              "      <td>91.349998</td>\n",
              "      <td>88.800003</td>\n",
              "      <td>90.660004</td>\n",
              "      <td>61.191929</td>\n",
              "      <td>0.365951</td>\n",
              "      <td>0.804501</td>\n",
              "      <td>0.929931</td>\n",
              "      <td>91.699997</td>\n",
              "      <td>0.011471</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>2002-09-10</td>\n",
              "      <td>91.139999</td>\n",
              "      <td>91.779999</td>\n",
              "      <td>90.559998</td>\n",
              "      <td>91.699997</td>\n",
              "      <td>61.893936</td>\n",
              "      <td>0.445799</td>\n",
              "      <td>0.288793</td>\n",
              "      <td>1.338801</td>\n",
              "      <td>91.129997</td>\n",
              "      <td>-0.006216</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8a315ef1-7436-494c-8d69-09e8328a05fe')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8a315ef1-7436-494c-8d69-09e8328a05fe button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8a315ef1-7436-494c-8d69-09e8328a05fe');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 394
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "low_risk.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "q-55dkiIJIal",
        "outputId": "86866e89-c859-47cd-ba28-1ab950789b26"
      },
      "execution_count": 395,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          Date       Open       High        Low      Close  Adj Close  \\\n",
              "25  2002-09-04  85.160004  85.449997  85.080002  85.199997  47.752071   \n",
              "26  2002-09-05  85.599998  85.650002  85.190002  85.540001  47.942638   \n",
              "27  2002-09-06  85.089996  85.250000  84.839996  84.879997  47.572742   \n",
              "28  2002-09-09  84.940002  85.150002  84.750000  84.760002  47.505463   \n",
              "29  2002-09-10  84.709999  85.209999  84.660004  85.059998  47.673588   \n",
              "\n",
              "      Volume  Daily Return        MA  Next Close       ROE  ROE Binary  \n",
              "25  0.023505      0.135391  0.912126   85.540001  0.003991           1  \n",
              "26  0.017606     -0.203112  0.564337   84.879997 -0.007716           0  \n",
              "27  0.009791     -0.710926  0.216542   84.760002 -0.001414           1  \n",
              "28  0.027002     -0.609368 -0.216563   85.059998  0.003539           1  \n",
              "29  0.006507      1.184878 -0.144378   84.750000 -0.003644           1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-da65fc91-b31b-4fdd-a140-6ddf9a3aeb70\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Daily Return</th>\n",
              "      <th>MA</th>\n",
              "      <th>Next Close</th>\n",
              "      <th>ROE</th>\n",
              "      <th>ROE Binary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>2002-09-04</td>\n",
              "      <td>85.160004</td>\n",
              "      <td>85.449997</td>\n",
              "      <td>85.080002</td>\n",
              "      <td>85.199997</td>\n",
              "      <td>47.752071</td>\n",
              "      <td>0.023505</td>\n",
              "      <td>0.135391</td>\n",
              "      <td>0.912126</td>\n",
              "      <td>85.540001</td>\n",
              "      <td>0.003991</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>2002-09-05</td>\n",
              "      <td>85.599998</td>\n",
              "      <td>85.650002</td>\n",
              "      <td>85.190002</td>\n",
              "      <td>85.540001</td>\n",
              "      <td>47.942638</td>\n",
              "      <td>0.017606</td>\n",
              "      <td>-0.203112</td>\n",
              "      <td>0.564337</td>\n",
              "      <td>84.879997</td>\n",
              "      <td>-0.007716</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>2002-09-06</td>\n",
              "      <td>85.089996</td>\n",
              "      <td>85.250000</td>\n",
              "      <td>84.839996</td>\n",
              "      <td>84.879997</td>\n",
              "      <td>47.572742</td>\n",
              "      <td>0.009791</td>\n",
              "      <td>-0.710926</td>\n",
              "      <td>0.216542</td>\n",
              "      <td>84.760002</td>\n",
              "      <td>-0.001414</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>2002-09-09</td>\n",
              "      <td>84.940002</td>\n",
              "      <td>85.150002</td>\n",
              "      <td>84.750000</td>\n",
              "      <td>84.760002</td>\n",
              "      <td>47.505463</td>\n",
              "      <td>0.027002</td>\n",
              "      <td>-0.609368</td>\n",
              "      <td>-0.216563</td>\n",
              "      <td>85.059998</td>\n",
              "      <td>0.003539</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>2002-09-10</td>\n",
              "      <td>84.709999</td>\n",
              "      <td>85.209999</td>\n",
              "      <td>84.660004</td>\n",
              "      <td>85.059998</td>\n",
              "      <td>47.673588</td>\n",
              "      <td>0.006507</td>\n",
              "      <td>1.184878</td>\n",
              "      <td>-0.144378</td>\n",
              "      <td>84.750000</td>\n",
              "      <td>-0.003644</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-da65fc91-b31b-4fdd-a140-6ddf9a3aeb70')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-da65fc91-b31b-4fdd-a140-6ddf9a3aeb70 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-da65fc91-b31b-4fdd-a140-6ddf9a3aeb70');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 395
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def to_dataset(low_risk, high_risk):\n",
        "#   return np.vstack((low_risk['Daily Return'], low_risk['MA'], low_risk['Volume'], high_risk['Daily Return'], high_risk['MA'], high_risk['Volume'],high_risk['ROE Binary']))\n",
        "\n",
        "# dataset = to_dataset(low_risk, high_risk).T\n",
        "# print(dataset.shape, dataset)"
      ],
      "metadata": {
        "id": "10wATCSHPIYZ"
      },
      "execution_count": 396,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pd.concat([low_risk, high_risk], join='outer', axis=1)[['Date'],['Daily Return'],['MA'],['Volume'],['ROE Binary']]\n",
        "# pd.concat([low_risk.add_prefix('l_'), high_risk.add_prefix('h_')], join='outer', axis=1)[['Date','ROE Binary']]\n",
        "ml_master_dataset = pd.concat([low_risk.add_prefix('l_'), high_risk.add_prefix('h_')], join='outer', axis=1)[['l_Date','l_Daily Return','l_MA','l_Volume','h_Daily Return','h_MA','h_Volume','h_ROE Binary']]\n",
        "ml_master_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "49t7Zc8bKsUQ",
        "outputId": "ce2047bf-dc32-4155-a7af-3b5f04d380bf"
      },
      "execution_count": 397,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          l_Date  l_Daily Return      l_MA  l_Volume  h_Daily Return  \\\n",
              "25    2002-09-04        0.135391  0.912126  0.023505        0.479605   \n",
              "26    2002-09-05       -0.203112  0.564337  0.017606        0.149555   \n",
              "27    2002-09-06       -0.710926  0.216542  0.009791        0.128926   \n",
              "28    2002-09-09       -0.609368 -0.216563  0.027002        0.804501   \n",
              "29    2002-09-10        1.184878 -0.144378  0.006507        0.288793   \n",
              "...          ...             ...       ...       ...             ...   \n",
              "5142  2022-12-30       -0.270837  0.826825  1.532486        0.923099   \n",
              "5143  2023-01-03       -1.286460  0.419968  2.070246       -1.830743   \n",
              "5144  2023-01-04       -0.236973  0.419968  2.371829        0.299117   \n",
              "5145  2023-01-05        1.455712  0.715269  0.966325       -1.206745   \n",
              "5146  2023-01-06        4.096301  0.826824  2.070063        2.820901   \n",
              "\n",
              "          h_MA  h_Volume  h_ROE Binary  \n",
              "25    0.276737  0.550024             0  \n",
              "26    0.229367  0.723874             1  \n",
              "27    0.522307  0.415721             1  \n",
              "28    0.929931  0.365951             1  \n",
              "29    1.338801  0.445799             0  \n",
              "...        ...       ...           ...  \n",
              "5142 -1.149319  0.903889             1  \n",
              "5143 -1.654172  0.805676             1  \n",
              "5144 -1.185467  0.924976             0  \n",
              "5145 -1.374945  0.828493             1  \n",
              "5146 -2.212630  1.119878             1  \n",
              "\n",
              "[5122 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b0f7390e-0504-4a29-9848-80280c12d7be\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>l_Date</th>\n",
              "      <th>l_Daily Return</th>\n",
              "      <th>l_MA</th>\n",
              "      <th>l_Volume</th>\n",
              "      <th>h_Daily Return</th>\n",
              "      <th>h_MA</th>\n",
              "      <th>h_Volume</th>\n",
              "      <th>h_ROE Binary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>2002-09-04</td>\n",
              "      <td>0.135391</td>\n",
              "      <td>0.912126</td>\n",
              "      <td>0.023505</td>\n",
              "      <td>0.479605</td>\n",
              "      <td>0.276737</td>\n",
              "      <td>0.550024</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>2002-09-05</td>\n",
              "      <td>-0.203112</td>\n",
              "      <td>0.564337</td>\n",
              "      <td>0.017606</td>\n",
              "      <td>0.149555</td>\n",
              "      <td>0.229367</td>\n",
              "      <td>0.723874</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>2002-09-06</td>\n",
              "      <td>-0.710926</td>\n",
              "      <td>0.216542</td>\n",
              "      <td>0.009791</td>\n",
              "      <td>0.128926</td>\n",
              "      <td>0.522307</td>\n",
              "      <td>0.415721</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>2002-09-09</td>\n",
              "      <td>-0.609368</td>\n",
              "      <td>-0.216563</td>\n",
              "      <td>0.027002</td>\n",
              "      <td>0.804501</td>\n",
              "      <td>0.929931</td>\n",
              "      <td>0.365951</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>2002-09-10</td>\n",
              "      <td>1.184878</td>\n",
              "      <td>-0.144378</td>\n",
              "      <td>0.006507</td>\n",
              "      <td>0.288793</td>\n",
              "      <td>1.338801</td>\n",
              "      <td>0.445799</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5142</th>\n",
              "      <td>2022-12-30</td>\n",
              "      <td>-0.270837</td>\n",
              "      <td>0.826825</td>\n",
              "      <td>1.532486</td>\n",
              "      <td>0.923099</td>\n",
              "      <td>-1.149319</td>\n",
              "      <td>0.903889</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5143</th>\n",
              "      <td>2023-01-03</td>\n",
              "      <td>-1.286460</td>\n",
              "      <td>0.419968</td>\n",
              "      <td>2.070246</td>\n",
              "      <td>-1.830743</td>\n",
              "      <td>-1.654172</td>\n",
              "      <td>0.805676</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5144</th>\n",
              "      <td>2023-01-04</td>\n",
              "      <td>-0.236973</td>\n",
              "      <td>0.419968</td>\n",
              "      <td>2.371829</td>\n",
              "      <td>0.299117</td>\n",
              "      <td>-1.185467</td>\n",
              "      <td>0.924976</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5145</th>\n",
              "      <td>2023-01-05</td>\n",
              "      <td>1.455712</td>\n",
              "      <td>0.715269</td>\n",
              "      <td>0.966325</td>\n",
              "      <td>-1.206745</td>\n",
              "      <td>-1.374945</td>\n",
              "      <td>0.828493</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5146</th>\n",
              "      <td>2023-01-06</td>\n",
              "      <td>4.096301</td>\n",
              "      <td>0.826824</td>\n",
              "      <td>2.070063</td>\n",
              "      <td>2.820901</td>\n",
              "      <td>-2.212630</td>\n",
              "      <td>1.119878</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5122 rows √ó 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b0f7390e-0504-4a29-9848-80280c12d7be')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b0f7390e-0504-4a29-9848-80280c12d7be button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b0f7390e-0504-4a29-9848-80280c12d7be');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 397
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# X_tensor = torch.from_numpy(ml_master_dataset[:,:-1])\n",
        "# Y_tensor = torch.from_numpy(ml_master_dataset[:,-1])"
      ],
      "metadata": {
        "id": "nrXWLxZw6GJk"
      },
      "execution_count": 398,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build graph"
      ],
      "metadata": {
        "id": "_zmM8pF0Nxl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-1\n",
        "n_epochs = 500\n",
        "torch.manual_seed(42)\n",
        "lambda1 = 1e-3 #0.5\n",
        "lambda2 = 1e-3 #0.5\n",
        "\n",
        "loss_fn = nn.BCELoss()\n",
        "\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "qjzCkZxbRPke"
      },
      "execution_count": 399,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folds=10\n",
        "splits=KFold(n_splits=folds,shuffle=True,random_state=42)"
      ],
      "metadata": {
        "id": "yjgtH2co3IPg"
      },
      "execution_count": 400,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#no cross-validation\n",
        "\n",
        "def train_and_get_a_b(dataset):\n",
        "\n",
        "  a = torch.randn((6), requires_grad=True, dtype=torch.double)\n",
        "  b = torch.randn((6), requires_grad=True, dtype=torch.double)\n",
        "  # print(a, a.size(), b, b.size())\n",
        "\n",
        "  optimizer = optim.SGD([a, b], lr=lr)\n",
        "\n",
        "  X_tensor = torch.from_numpy(dataset[:,:-1])\n",
        "  Y_tensor = torch.from_numpy(dataset[:,-1])\n",
        "  # print(X_tensor, Y_tensor)\n",
        "    \n",
        "    \n",
        "  for epoch in range(n_epochs):\n",
        "    \n",
        "      yhat = torch.exp(torch.matmul(X_tensor, a)) / (torch.exp (torch.matmul(X_tensor, a)) + torch.exp(torch.matmul(X_tensor, b)))\n",
        "\n",
        "      loss = loss_fn(yhat, Y_tensor)\n",
        "      loss.backward()   \n",
        "\n",
        "      if epoch % 10 == 0:\n",
        "        print(f\"Epoch: {epoch}. Loss: {loss}\")\n",
        "\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "  return a,b"
      ],
      "metadata": {
        "id": "LUEtsz9ZRRFN"
      },
      "execution_count": 401,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Dataset for MV Portfolio"
      ],
      "metadata": {
        "id": "8R2xXpWs2tus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mv_master_dataset = pd.concat([low_risk.add_prefix('l_'), high_risk.add_prefix('h_')], join='outer', axis=1)[['l_Date','l_Close','h_Close']]\n",
        "mv_master_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "aqbtJ5LC2E7x",
        "outputId": "694d5cab-120b-42ac-df6b-3df12c828bac"
      },
      "execution_count": 402,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          l_Date    l_Close     h_Close\n",
              "25    2002-09-04  85.199997   89.540001\n",
              "26    2002-09-05  85.540001   88.779999\n",
              "27    2002-09-06  84.879997   90.000000\n",
              "28    2002-09-09  84.760002   90.660004\n",
              "29    2002-09-10  85.059998   91.699997\n",
              "...          ...        ...         ...\n",
              "5142  2022-12-30  95.779999  382.429993\n",
              "5143  2023-01-03  96.529999  380.820007\n",
              "5144  2023-01-04  97.269997  383.760010\n",
              "5145  2023-01-05  97.129997  379.380005\n",
              "5146  2023-01-06  98.379997  388.079987\n",
              "\n",
              "[5122 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e91b457d-c897-4399-ac9c-f59f472bc322\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>l_Date</th>\n",
              "      <th>l_Close</th>\n",
              "      <th>h_Close</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>2002-09-04</td>\n",
              "      <td>85.199997</td>\n",
              "      <td>89.540001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>2002-09-05</td>\n",
              "      <td>85.540001</td>\n",
              "      <td>88.779999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>2002-09-06</td>\n",
              "      <td>84.879997</td>\n",
              "      <td>90.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>2002-09-09</td>\n",
              "      <td>84.760002</td>\n",
              "      <td>90.660004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>2002-09-10</td>\n",
              "      <td>85.059998</td>\n",
              "      <td>91.699997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5142</th>\n",
              "      <td>2022-12-30</td>\n",
              "      <td>95.779999</td>\n",
              "      <td>382.429993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5143</th>\n",
              "      <td>2023-01-03</td>\n",
              "      <td>96.529999</td>\n",
              "      <td>380.820007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5144</th>\n",
              "      <td>2023-01-04</td>\n",
              "      <td>97.269997</td>\n",
              "      <td>383.760010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5145</th>\n",
              "      <td>2023-01-05</td>\n",
              "      <td>97.129997</td>\n",
              "      <td>379.380005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5146</th>\n",
              "      <td>2023-01-06</td>\n",
              "      <td>98.379997</td>\n",
              "      <td>388.079987</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5122 rows √ó 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e91b457d-c897-4399-ac9c-f59f472bc322')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e91b457d-c897-4399-ac9c-f59f472bc322 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e91b457d-c897-4399-ac9c-f59f472bc322');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 402
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mv_dataset_for_date(date):\n",
        "  startdate = pd.to_datetime(date) - delta_50weeks\n",
        "  enddate = pd.to_datetime(date)\n",
        "  mask = (pd.to_datetime(mv_master_dataset['l_Date']) > startdate) & (pd.to_datetime(mv_master_dataset['l_Date']) <= enddate)\n",
        "  subset = mv_master_dataset.loc[mask]\n",
        "  dataset = subset[['l_Close','h_Close']]\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "Ekt1011K23kV"
      },
      "execution_count": 432,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_annual_sample_return_and_covariance(data):\n",
        "    daily_return = data.pct_change()\n",
        "    annual_return = daily_return.mean() * trading_days_in_year\n",
        "    daily_covariance = daily_return.cov()\n",
        "    annual_covariance = daily_covariance * trading_days_in_year\n",
        "    return annual_return, annual_covariance"
      ],
      "metadata": {
        "id": "-ay84qKK3kSO"
      },
      "execution_count": 404,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sample_return_and_covariance(data):\n",
        "    daily_return = data.pct_change().mean()\n",
        "    daily_covariance = data.pct_change().cov()\n",
        "    return daily_return, daily_covariance"
      ],
      "metadata": {
        "id": "iBzfeFvH32g4"
      },
      "execution_count": 405,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MinimizeRiskConstr(MeanReturns, CovarReturns, PortfolioSize, R):\n",
        "    \n",
        "    def  f(x,CovarReturns):\n",
        "         \n",
        "        func = np.matmul(np.matmul(x,CovarReturns ), x.T)\n",
        "        return func\n",
        "\n",
        "    def constraintEq(x):\n",
        "        AEq=np.ones(x.shape)\n",
        "        bEq=1\n",
        "        EqconstraintVal = np.matmul(AEq,x.T)-bEq \n",
        "        return EqconstraintVal\n",
        "    \n",
        "    def constraintIneq(x, MeanReturns, R):\n",
        "        AIneq = np.array(MeanReturns)\n",
        "        bIneq = R\n",
        "        IneqconstraintVal = np.matmul(AIneq,x.T) - bIneq\n",
        "        return IneqconstraintVal\n",
        "    \n",
        "\n",
        "    xinit=np.repeat(0.1, PortfolioSize)\n",
        "    cons = ({'type': 'eq', 'fun':constraintEq},\n",
        "            {'type':'ineq', 'fun':constraintIneq, 'args':(MeanReturns,R) })\n",
        "    lb = 0\n",
        "    ub = 1\n",
        "    bnds = tuple([(lb,ub) for x in xinit])\n",
        "\n",
        "    opt = optimize.minimize (f, args = (CovarReturns), method ='trust-constr',  \\\n",
        "                        x0 = xinit,   bounds = bnds, constraints = cons, tol = TERMINATION)\n",
        "    \n",
        "    return  opt"
      ],
      "metadata": {
        "id": "mw2-EUMt4Otu"
      },
      "execution_count": 406,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mv_backtest_data(date):\n",
        "  startdate = pd.to_datetime(date)\n",
        "  enddate = pd.to_datetime(date) + delta_1week\n",
        "    \n",
        "  low_risk_mask = (pd.to_datetime(low_risk['Date']) > startdate) & (pd.to_datetime(low_risk['Date']) <= enddate)\n",
        "  high_risk_mask = (pd.to_datetime(high_risk['Date']) > startdate) & (pd.to_datetime(high_risk['Date']) <= enddate)\n",
        "  low_risk_backtest_data = low_risk.loc[low_risk_mask]\n",
        "  high_risk_backtest_data = high_risk.loc[high_risk_mask]\n",
        "\n",
        "  return low_risk_backtest_data, high_risk_backtest_data"
      ],
      "metadata": {
        "id": "-WvoS-8a6YvW"
      },
      "execution_count": 407,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_backtest_return(backtest_data):\n",
        "  first_open = backtest_data.iloc[0]['Open']\n",
        "  last_close = backtest_data.iloc[-1]['Close']\n",
        "  return (last_close - first_open)/first_open"
      ],
      "metadata": {
        "id": "jGK6T6AW7Yg4"
      },
      "execution_count": 408,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mv_backtest_return(low_risk_backtest_data, high_risk_backtest_data, weight):\n",
        "  low_return = calculate_backtest_return(low_risk_backtest_data)\n",
        "  high_return = calculate_backtest_return(high_risk_backtest_data)\n",
        "  return low_return * (1-weight) + high_return * weight"
      ],
      "metadata": {
        "id": "CwpNskex6YvX"
      },
      "execution_count": 434,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mv_backtest_return(date, weight):\n",
        "  low_risk_backtest_data, high_risk_backtest_data = get_mv_backtest_data(date)\n",
        "  return calculate_mv_backtest_return(low_risk_backtest_data, high_risk_backtest_data, weight)"
      ],
      "metadata": {
        "id": "otburto36YvX"
      },
      "execution_count": 435,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Backtesting"
      ],
      "metadata": {
        "id": "lYyH6QC07N4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from datetime import date\n",
        "from datetime import timedelta"
      ],
      "metadata": {
        "id": "pt5JBhCLyjYr"
      },
      "execution_count": 411,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_date = date(2003,9,21)\n",
        "last_date = date(2023,1,1)"
      ],
      "metadata": {
        "id": "Bbj-yDR504Fv"
      },
      "execution_count": 412,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "delta_50weeks = timedelta(weeks=50)\n",
        "delta_1week = timedelta(weeks=1)"
      ],
      "metadata": {
        "id": "4v4UTD431hJ4"
      },
      "execution_count": 413,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "daterange = pd.date_range(first_date, last_date, freq='1W')\n",
        "daterange"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKt0kekQ21Ts",
        "outputId": "fbaa106d-2747-4ad9-e63d-6ab5e27e2353"
      },
      "execution_count": 414,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatetimeIndex(['2003-09-21', '2003-09-28', '2003-10-05', '2003-10-12',\n",
              "               '2003-10-19', '2003-10-26', '2003-11-02', '2003-11-09',\n",
              "               '2003-11-16', '2003-11-23',\n",
              "               ...\n",
              "               '2022-10-30', '2022-11-06', '2022-11-13', '2022-11-20',\n",
              "               '2022-11-27', '2022-12-04', '2022-12-11', '2022-12-18',\n",
              "               '2022-12-25', '2023-01-01'],\n",
              "              dtype='datetime64[ns]', length=1007, freq='W-SUN')"
            ]
          },
          "metadata": {},
          "execution_count": 414
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ml_dataset_for_date(date):\n",
        "  startdate = pd.to_datetime(date) - delta_50weeks\n",
        "  enddate = pd.to_datetime(date)\n",
        "  mask = (pd.to_datetime(ml_master_dataset['l_Date']) > startdate) & (pd.to_datetime(ml_master_dataset['l_Date']) <= enddate)\n",
        "  subset = ml_master_dataset.loc[mask]\n",
        "  # print(subset)\n",
        "  dataset = subset[['l_Daily Return','l_MA','l_Volume','h_Daily Return','h_MA','h_Volume','h_ROE Binary']].to_numpy()\n",
        "  # print(dataset)\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "EV3MvASZ7kOM"
      },
      "execution_count": 415,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First date"
      ],
      "metadata": {
        "id": "8Po5CtXBzyxS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = get_ml_dataset_for_date(first_date)\n",
        "dataset[:-1]"
      ],
      "metadata": {
        "id": "fra6gOlGT1-v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "162d8bfc-cb6e-483f-acf0-ebe57ba35b2b"
      },
      "execution_count": 416,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.10155789,  1.89647755,  0.02441736, ..., -0.97480731,\n",
              "         0.57250387,  1.        ],\n",
              "       [ 0.40625526,  1.88335059,  0.04092873, ..., -0.59959326,\n",
              "         0.8560541 ,  0.        ],\n",
              "       [ 0.16925525,  1.88991473,  0.02873525, ..., -0.83893149,\n",
              "         0.86063301,  1.        ],\n",
              "       ...,\n",
              "       [ 0.20311239,  0.702162  ,  0.02435655, ...,  0.53227856,\n",
              "         0.40786757,  1.        ],\n",
              "       [ 1.11718079,  1.03027115,  0.05385199, ...,  0.41510118,\n",
              "         0.3432117 ,  1.        ],\n",
              "       [-0.06772444,  1.33868853,  0.04019894, ...,  0.66191837,\n",
              "         0.32553542,  0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 416
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a,b = train_and_get_a_b(dataset[:-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmWm27_faerS",
        "outputId": "e03ef6fd-9482-41b9-bb22-e3ef58524f20"
      },
      "execution_count": 417,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0. Loss: 1.2250968985965647\n",
            "Epoch: 10. Loss: 1.0422664556524388\n",
            "Epoch: 20. Loss: 0.9103199651473237\n",
            "Epoch: 30. Loss: 0.8211627365206585\n",
            "Epoch: 40. Loss: 0.7621501754153689\n",
            "Epoch: 50. Loss: 0.7225548587061368\n",
            "Epoch: 60. Loss: 0.6954737927837602\n",
            "Epoch: 70. Loss: 0.6767014150071979\n",
            "Epoch: 80. Loss: 0.6635699476207669\n",
            "Epoch: 90. Loss: 0.6543132706623938\n",
            "Epoch: 100. Loss: 0.6477360885029361\n",
            "Epoch: 110. Loss: 0.6430221585728905\n",
            "Epoch: 120. Loss: 0.6396116532648428\n",
            "Epoch: 130. Loss: 0.6371191459597229\n",
            "Epoch: 140. Loss: 0.635278073957452\n",
            "Epoch: 150. Loss: 0.6339030752274836\n",
            "Epoch: 160. Loss: 0.6328644511700781\n",
            "Epoch: 170. Loss: 0.6320708338895114\n",
            "Epoch: 180. Loss: 0.6314573915018703\n",
            "Epoch: 190. Loss: 0.6309777707686528\n",
            "Epoch: 200. Loss: 0.6305985665087154\n",
            "Epoch: 210. Loss: 0.6302955051416969\n",
            "Epoch: 220. Loss: 0.6300507962110538\n",
            "Epoch: 230. Loss: 0.6298512837528443\n",
            "Epoch: 240. Loss: 0.6296871483410416\n",
            "Epoch: 250. Loss: 0.6295509903413822\n",
            "Epoch: 260. Loss: 0.6294371785102312\n",
            "Epoch: 270. Loss: 0.6293413842983838\n",
            "Epoch: 280. Loss: 0.6292602468248716\n",
            "Epoch: 290. Loss: 0.6291911302889445\n",
            "Epoch: 300. Loss: 0.6291319471250912\n",
            "Epoch: 310. Loss: 0.6290810281684981\n",
            "Epoch: 320. Loss: 0.6290370266219606\n",
            "Epoch: 330. Loss: 0.6289988464656815\n",
            "Epoch: 340. Loss: 0.6289655886479608\n",
            "Epoch: 350. Loss: 0.6289365102917657\n",
            "Epoch: 360. Loss: 0.6289109934924184\n",
            "Epoch: 370. Loss: 0.628888521232558\n",
            "Epoch: 380. Loss: 0.6288686586180415\n",
            "Epoch: 390. Loss: 0.6288510381231767\n",
            "Epoch: 400. Loss: 0.628835347881967\n",
            "Epoch: 410. Loss: 0.6288213223134048\n",
            "Epoch: 420. Loss: 0.6288087345510793\n",
            "Epoch: 430. Loss: 0.628797390280147\n",
            "Epoch: 440. Loss: 0.6287871226819385\n",
            "Epoch: 450. Loss: 0.6287777882581012\n",
            "Epoch: 460. Loss: 0.6287692633592182\n",
            "Epoch: 470. Loss: 0.628761441282425\n",
            "Epoch: 480. Loss: 0.628754229832236\n",
            "Epoch: 490. Loss: 0.6287475492612717\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  y_test = torch.exp(torch.matmul(torch.from_numpy(dataset[-1][:-1]), a)) / (torch.exp (torch.matmul(torch.from_numpy(dataset[-1][:-1]), a)) + torch.exp(torch.matmul(torch.from_numpy(dataset[-1][:-1]), b)))\n",
        "  print(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMsYhNrSeoDv",
        "outputId": "f7c45bc3-f076-4d91-e162-fc1ed65b7bd1"
      },
      "execution_count": 418,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.6920, dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_ml_portfolio_weights(x, k):\n",
        "  return 0 if x < k else 1"
      ],
      "metadata": {
        "id": "MsH_YCVPyOer"
      },
      "execution_count": 419,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight = calculate_ml_portfolio_weights(y_test.numpy(), 0.5)\n",
        "weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqJY6MNe079u",
        "outputId": "dfd0635e-80a3-44d5-ccff-f3644265a5b6"
      },
      "execution_count": 420,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 420
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_backtest_data(date, weight):\n",
        "  startdate = pd.to_datetime(date)\n",
        "  enddate = pd.to_datetime(date) + delta_1week\n",
        "\n",
        "  investment = low_risk if weight == 0 else high_risk\n",
        "    \n",
        "  backtest_mask = (pd.to_datetime(investment['Date']) > startdate) & (pd.to_datetime(investment['Date']) <= enddate)\n",
        "  backtest_data = investment.loc[backtest_mask]\n",
        "\n",
        "  return backtest_data"
      ],
      "metadata": {
        "id": "CkX6whwN4KX0"
      },
      "execution_count": 421,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_backtest_return(backtest_data):\n",
        "  first_open = backtest_data.iloc[0]['Open']\n",
        "  last_close = backtest_data.iloc[-1]['Close']\n",
        "  return (last_close - first_open)/first_open"
      ],
      "metadata": {
        "id": "7grWmruL4LNu"
      },
      "execution_count": 422,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_backtest_return(date, weight):\n",
        "  backtest_data = get_backtest_data(date, weight)\n",
        "  return calculate_backtest_return(backtest_data)"
      ],
      "metadata": {
        "id": "zGPtDN524CA0"
      },
      "execution_count": 423,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "backtest_data = get_backtest_data(first_date, weight)\n",
        "backtest_data.iloc[-1]['Close']\n",
        "backtest_data.iloc[0]['Open']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWzbQZb01qHF",
        "outputId": "b26e776b-6149-4ea5-a20d-333716adc9e6"
      },
      "execution_count": 424,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "102.849998"
            ]
          },
          "metadata": {},
          "execution_count": 424
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_backtest_return(first_date, weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxw0Vpct44Bq",
        "outputId": "003c9f0b-0ed1-4e66-8e73-54cad5f091b4"
      },
      "execution_count": 425,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.028196412799152443"
            ]
          },
          "metadata": {},
          "execution_count": 425
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Back test for all range"
      ],
      "metadata": {
        "id": "d5v5C2j_0IQw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Portfolio"
      ],
      "metadata": {
        "id": "xgECBqg00_LI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = pd.DataFrame(columns=['a','b','prob'])\n",
        "\n",
        "for date in daterange:\n",
        "  print(date)\n",
        "  dataset = get_ml_dataset_for_date(date)\n",
        "  a,b = train_and_get_a_b(dataset[:-1])  \n",
        "  with torch.no_grad():\n",
        "    y_test = torch.exp(torch.matmul(torch.from_numpy(dataset[-1][:-1]), a)) / (torch.exp (torch.matmul(torch.from_numpy(dataset[-1][:-1]), a)) + torch.exp(torch.matmul(torch.from_numpy(dataset[-1][:-1]), b)))\n",
        "    print(y_test)\n",
        "    parameters.loc[date] = [a,b,y_test.numpy()]\n",
        "\n",
        "parameters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "m8zIjtzG4xOv",
        "outputId": "cdae6557-96b7-4922-f211-eb557cab9f54"
      },
      "execution_count": 426,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch: 430. Loss: 0.579132914685977\n",
            "Epoch: 440. Loss: 0.5791325550558514\n",
            "Epoch: 450. Loss: 0.5791322520808101\n",
            "Epoch: 460. Loss: 0.5791319968279854\n",
            "Epoch: 470. Loss: 0.5791317817747692\n",
            "Epoch: 480. Loss: 0.5791316005857305\n",
            "Epoch: 490. Loss: 0.5791314479249337\n",
            "tensor(0.9898, dtype=torch.float64)\n",
            "2021-03-07 00:00:00\n",
            "Epoch: 0. Loss: 1.6926048258833029\n",
            "Epoch: 10. Loss: 1.371197322306517\n",
            "Epoch: 20. Loss: 1.0853757657759322\n",
            "Epoch: 30. Loss: 0.8623561998168796\n",
            "Epoch: 40. Loss: 0.7184527421792531\n",
            "Epoch: 50. Loss: 0.6469919017243319\n",
            "Epoch: 60. Loss: 0.6177306557360324\n",
            "Epoch: 70. Loss: 0.6054786742592019\n",
            "Epoch: 80. Loss: 0.5991356739487206\n",
            "Epoch: 90. Loss: 0.595038311464021\n",
            "Epoch: 100. Loss: 0.5920230840235082\n",
            "Epoch: 110. Loss: 0.589665925157147\n",
            "Epoch: 120. Loss: 0.5877718507898121\n",
            "Epoch: 130. Loss: 0.5862286032547389\n",
            "Epoch: 140. Loss: 0.5849609049906295\n",
            "Epoch: 150. Loss: 0.5839138457646948\n",
            "Epoch: 160. Loss: 0.5830455657630869\n",
            "Epoch: 170. Loss: 0.5823233510664408\n",
            "Epoch: 180. Loss: 0.5817212231954357\n",
            "Epoch: 190. Loss: 0.58121831040322\n",
            "Epoch: 200. Loss: 0.5807976889970308\n",
            "Epoch: 210. Loss: 0.580445533279057\n",
            "Epoch: 220. Loss: 0.5801504786262562\n",
            "Epoch: 230. Loss: 0.5799031362247498\n",
            "Epoch: 240. Loss: 0.579695717914074\n",
            "Epoch: 250. Loss: 0.5795217422917924\n",
            "Epoch: 260. Loss: 0.5793758016888855\n",
            "Epoch: 270. Loss: 0.5792533754040501\n",
            "Epoch: 280. Loss: 0.5791506785885133\n",
            "Epoch: 290. Loss: 0.5790645389765336\n",
            "Epoch: 300. Loss: 0.5789922956392071\n",
            "Epoch: 310. Loss: 0.5789317153555947\n",
            "Epoch: 320. Loss: 0.5788809232188579\n",
            "Epoch: 330. Loss: 0.5788383448444437\n",
            "Epoch: 340. Loss: 0.5788026581035978\n",
            "Epoch: 350. Loss: 0.5787727527244028\n",
            "Epoch: 360. Loss: 0.5787476964227902\n",
            "Epoch: 370. Loss: 0.5787267064743472\n",
            "Epoch: 380. Loss: 0.5787091258330784\n",
            "Epoch: 390. Loss: 0.5786944030588548\n",
            "Epoch: 400. Loss: 0.5786820754405991\n",
            "Epoch: 410. Loss: 0.5786717548042104\n",
            "Epoch: 420. Loss: 0.5786631155778502\n",
            "Epoch: 430. Loss: 0.5786558847562977\n",
            "Epoch: 440. Loss: 0.5786498334634428\n",
            "Epoch: 450. Loss: 0.5786447698598522\n",
            "Epoch: 460. Loss: 0.5786405331824042\n",
            "Epoch: 470. Loss: 0.5786369887366178\n",
            "Epoch: 480. Loss: 0.5786340236905628\n",
            "Epoch: 490. Loss: 0.578631543543049\n",
            "tensor(0.9005, dtype=torch.float64)\n",
            "2021-03-14 00:00:00\n",
            "Epoch: 0. Loss: 1.2364966109513722\n",
            "Epoch: 10. Loss: 1.0537793563139302\n",
            "Epoch: 20. Loss: 0.9269195497541152\n",
            "Epoch: 30. Loss: 0.8403195828468581\n",
            "Epoch: 40. Loss: 0.7774374337878648\n",
            "Epoch: 50. Loss: 0.7295420850805452\n",
            "Epoch: 60. Loss: 0.6923445962548608\n",
            "Epoch: 70. Loss: 0.6634410502085006\n",
            "Epoch: 80. Loss: 0.6412057751283674\n",
            "Epoch: 90. Loss: 0.6243361654006676\n",
            "Epoch: 100. Loss: 0.6117047042402063\n",
            "Epoch: 110. Loss: 0.6023394577641855\n",
            "Epoch: 120. Loss: 0.5954348688958494\n",
            "Epoch: 130. Loss: 0.5903523252602924\n",
            "Epoch: 140. Loss: 0.586604148034587\n",
            "Epoch: 150. Loss: 0.5838278877826879\n",
            "Epoch: 160. Loss: 0.5817588336095055\n",
            "Epoch: 170. Loss: 0.5802055248613962\n",
            "Epoch: 180. Loss: 0.5790300302402291\n",
            "Epoch: 190. Loss: 0.5781329766395679\n",
            "Epoch: 200. Loss: 0.5774425711197236\n",
            "Epoch: 210. Loss: 0.5769067076803099\n",
            "Epoch: 220. Loss: 0.5764873494702979\n",
            "Epoch: 230. Loss: 0.576156546991403\n",
            "Epoch: 240. Loss: 0.5758936167019608\n",
            "Epoch: 250. Loss: 0.5756831375782987\n",
            "Epoch: 260. Loss: 0.5755135232846338\n",
            "Epoch: 270. Loss: 0.5753759998771087\n",
            "Epoch: 280. Loss: 0.5752638700502072\n",
            "Epoch: 290. Loss: 0.5751719806507763\n",
            "Epoch: 300. Loss: 0.5750963350467021\n",
            "Epoch: 310. Loss: 0.5750338092325897\n",
            "Epoch: 320. Loss: 0.5749819426077105\n",
            "Epoch: 330. Loss: 0.5749387827878233\n",
            "Epoch: 340. Loss: 0.5749027697269822\n",
            "Epoch: 350. Loss: 0.5748726485950909\n",
            "Epoch: 360. Loss: 0.5748474038098254\n",
            "Epoch: 370. Loss: 0.5748262087221646\n",
            "Epoch: 380. Loss: 0.5748083869557348\n",
            "Epoch: 390. Loss: 0.5747933824773296\n",
            "Epoch: 400. Loss: 0.574780736252248\n",
            "Epoch: 410. Loss: 0.5747700678999056\n",
            "Epoch: 420. Loss: 0.5747610611734771\n",
            "Epoch: 430. Loss: 0.5747534523853488\n",
            "Epoch: 440. Loss: 0.5747470211186528\n",
            "Epoch: 450. Loss: 0.5747415827260856\n",
            "Epoch: 460. Loss: 0.5747369822363477\n",
            "Epoch: 470. Loss: 0.5747330893771786\n",
            "Epoch: 480. Loss: 0.5747297944902994\n",
            "Epoch: 490. Loss: 0.5747270051635154\n",
            "tensor(0.7063, dtype=torch.float64)\n",
            "2021-03-21 00:00:00\n",
            "Epoch: 0. Loss: 1.0330661720674934\n",
            "Epoch: 10. Loss: 0.9356132211920681\n",
            "Epoch: 20. Loss: 0.8797052772278066\n",
            "Epoch: 30. Loss: 0.8381359705126459\n",
            "Epoch: 40. Loss: 0.8043271727293698\n",
            "Epoch: 50. Loss: 0.7753130280288623\n",
            "Epoch: 60. Loss: 0.7496636763066348\n",
            "Epoch: 70. Loss: 0.7267247298862581\n",
            "Epoch: 80. Loss: 0.7061867835479122\n",
            "Epoch: 90. Loss: 0.687874456308512\n",
            "Epoch: 100. Loss: 0.6716536305202829\n",
            "Epoch: 110. Loss: 0.657394274580344\n",
            "Epoch: 120. Loss: 0.6449581072666603\n",
            "Epoch: 130. Loss: 0.6341969388188288\n",
            "Epoch: 140. Loss: 0.6249553459381488\n",
            "Epoch: 150. Loss: 0.6170748934036677\n",
            "Epoch: 160. Loss: 0.6103986374373221\n",
            "Epoch: 170. Loss: 0.6047752653897857\n",
            "Epoch: 180. Loss: 0.600062502999451\n",
            "Epoch: 190. Loss: 0.5961295947409427\n",
            "Epoch: 200. Loss: 0.592858808265779\n",
            "Epoch: 210. Loss: 0.590146034465891\n",
            "Epoch: 220. Loss: 0.5879006376911875\n",
            "Epoch: 230. Loss: 0.5860447501334126\n",
            "Epoch: 240. Loss: 0.584512205508522\n",
            "Epoch: 250. Loss: 0.583247282158562\n",
            "Epoch: 260. Loss: 0.5822033880255799\n",
            "Epoch: 270. Loss: 0.5813417803537654\n",
            "Epoch: 280. Loss: 0.5806303780389693\n",
            "Epoch: 290. Loss: 0.5800426971343641\n",
            "Epoch: 300. Loss: 0.5795569204597022\n",
            "Epoch: 310. Loss: 0.5791550995093343\n",
            "Epoch: 320. Loss: 0.5788224793747087\n",
            "Epoch: 330. Loss: 0.5785469336671516\n",
            "Epoch: 340. Loss: 0.578318495169834\n",
            "Epoch: 350. Loss: 0.5781289681978926\n",
            "Epoch: 360. Loss: 0.5779716097242988\n",
            "Epoch: 370. Loss: 0.5778408677890388\n",
            "Epoch: 380. Loss: 0.5777321672738444\n",
            "Epoch: 390. Loss: 0.5776417346353476\n",
            "Epoch: 400. Loss: 0.5775664545646289\n",
            "Epoch: 410. Loss: 0.5775037527473112\n",
            "Epoch: 420. Loss: 0.577451499930396\n",
            "Epoch: 430. Loss: 0.5774079333700143\n",
            "Epoch: 440. Loss: 0.5773715924554245\n",
            "Epoch: 450. Loss: 0.577341265898632\n",
            "Epoch: 460. Loss: 0.5773159483653771\n",
            "Epoch: 470. Loss: 0.5772948048197987\n",
            "Epoch: 480. Loss: 0.5772771411775269\n",
            "Epoch: 490. Loss: 0.5772623801236985\n",
            "tensor(0.8426, dtype=torch.float64)\n",
            "2021-03-28 00:00:00\n",
            "Epoch: 0. Loss: 1.7551308648255313\n",
            "Epoch: 10. Loss: 1.0273298854362907\n",
            "Epoch: 20. Loss: 0.8863533409357603\n",
            "Epoch: 30. Loss: 0.841513364401068\n",
            "Epoch: 40. Loss: 0.8103550216586181\n",
            "Epoch: 50. Loss: 0.7829396451031696\n",
            "Epoch: 60. Loss: 0.7580591694092639\n",
            "Epoch: 70. Loss: 0.7354333850840488\n",
            "Epoch: 80. Loss: 0.7149268709732709\n",
            "Epoch: 90. Loss: 0.6964451934804642\n",
            "Epoch: 100. Loss: 0.6799037946892581\n",
            "Epoch: 110. Loss: 0.6652129806358992\n",
            "Epoch: 120. Loss: 0.6522712715627708\n",
            "Epoch: 130. Loss: 0.6409638893210606\n",
            "Epoch: 140. Loss: 0.6311643048706402\n",
            "Epoch: 150. Loss: 0.622737523727209\n",
            "Epoch: 160. Loss: 0.6155442208735995\n",
            "Epoch: 170. Loss: 0.60944506341458\n",
            "Epoch: 180. Loss: 0.6043047175898341\n",
            "Epoch: 190. Loss: 0.5999951883349325\n",
            "Epoch: 200. Loss: 0.5963982927294403\n",
            "Epoch: 210. Loss: 0.5934072127119548\n",
            "Epoch: 220. Loss: 0.5909271909933071\n",
            "Epoch: 230. Loss: 0.5888755137059867\n",
            "Epoch: 240. Loss: 0.5871809602580675\n",
            "Epoch: 250. Loss: 0.5857829016645691\n",
            "Epoch: 260. Loss: 0.584630205554215\n",
            "Epoch: 270. Loss: 0.583680071683905\n",
            "Epoch: 280. Loss: 0.5828968857792322\n",
            "Epoch: 290. Loss: 0.5822511476469924\n",
            "Epoch: 300. Loss: 0.5817185042849865\n",
            "Epoch: 310. Loss: 0.5812789004617974\n",
            "Epoch: 320. Loss: 0.5809158470947117\n",
            "Epoch: 330. Loss: 0.5806158004103653\n",
            "Epoch: 340. Loss: 0.580367641007339\n",
            "Epoch: 350. Loss: 0.5801622404210552\n",
            "Epoch: 360. Loss: 0.5799921027435406\n",
            "Epoch: 370. Loss: 0.5798510696446127\n",
            "Epoch: 380. Loss: 0.5797340783516446\n",
            "Epoch: 390. Loss: 0.5796369635017787\n",
            "Epoch: 400. Loss: 0.5795562951226446\n",
            "Epoch: 410. Loss: 0.579489246238573\n",
            "Epoch: 420. Loss: 0.5794334846995108\n",
            "Epoch: 430. Loss: 0.5793870847784287\n",
            "Epoch: 440. Loss: 0.579348454885192\n",
            "Epoch: 450. Loss: 0.5793162784139981\n",
            "Epoch: 460. Loss: 0.5792894652941798\n",
            "Epoch: 470. Loss: 0.5792671122675276\n",
            "Epoch: 480. Loss: 0.5792484702853236\n",
            "Epoch: 490. Loss: 0.5792329177193195\n",
            "tensor(0.8115, dtype=torch.float64)\n",
            "2021-04-04 00:00:00\n",
            "Epoch: 0. Loss: 0.8946526750183403\n",
            "Epoch: 10. Loss: 0.6996022751184977\n",
            "Epoch: 20. Loss: 0.6587820377381551\n",
            "Epoch: 30. Loss: 0.6371511087827905\n",
            "Epoch: 40. Loss: 0.6216225516973422\n",
            "Epoch: 50. Loss: 0.6098868340149539\n",
            "Epoch: 60. Loss: 0.6009875988354386\n",
            "Epoch: 70. Loss: 0.5942531933979189\n",
            "Epoch: 80. Loss: 0.5891531849529533\n",
            "Epoch: 90. Loss: 0.5852713133623593\n",
            "Epoch: 100. Loss: 0.5822896523777948\n",
            "Epoch: 110. Loss: 0.5799713650989399\n",
            "Epoch: 120. Loss: 0.5781432312895555\n",
            "Epoch: 130. Loss: 0.5766800113991073\n",
            "Epoch: 140. Loss: 0.5754916377729943\n",
            "Epoch: 150. Loss: 0.5745133259675668\n",
            "Epoch: 160. Loss: 0.5736982402880765\n",
            "Epoch: 170. Loss: 0.5730122003045575\n",
            "Epoch: 180. Loss: 0.5724299305232387\n",
            "Epoch: 190. Loss: 0.5719324356438723\n",
            "Epoch: 200. Loss: 0.5715051772360716\n",
            "Epoch: 210. Loss: 0.5711368116964018\n",
            "Epoch: 220. Loss: 0.5708183169116329\n",
            "Epoch: 230. Loss: 0.5705423861183133\n",
            "Epoch: 240. Loss: 0.5703030046040118\n",
            "Epoch: 250. Loss: 0.5700951512672293\n",
            "Epoch: 260. Loss: 0.5699145854579206\n",
            "Epoch: 270. Loss: 0.5697576922147451\n",
            "Epoch: 280. Loss: 0.569621367696424\n",
            "Epoch: 290. Loss: 0.5695029325041091\n",
            "Epoch: 300. Loss: 0.5694000645815044\n",
            "Epoch: 310. Loss: 0.5693107460675845\n",
            "Epoch: 320. Loss: 0.5692332202824751\n",
            "Epoch: 330. Loss: 0.5691659562373791\n",
            "Epoch: 340. Loss: 0.5691076188695845\n",
            "Epoch: 350. Loss: 0.5690570437457768\n",
            "Epoch: 360. Loss: 0.5690132153402081\n",
            "Epoch: 370. Loss: 0.5689752482386108\n",
            "Epoch: 380. Loss: 0.5689423707840978\n",
            "Epoch: 390. Loss: 0.5689139107943599\n",
            "Epoch: 400. Loss: 0.5688892830579336\n",
            "Epoch: 410. Loss: 0.5688679783728798\n",
            "Epoch: 420. Loss: 0.5688495539315785\n",
            "Epoch: 430. Loss: 0.5688336248855601\n",
            "Epoch: 440. Loss: 0.5688198569476413\n",
            "Epoch: 450. Loss: 0.5688079599072872\n",
            "Epoch: 460. Loss: 0.5687976819504467\n",
            "Epoch: 470. Loss: 0.5687888046880439\n",
            "Epoch: 480. Loss: 0.5687811388084233\n",
            "Epoch: 490. Loss: 0.5687745202787613\n",
            "tensor(0.8480, dtype=torch.float64)\n",
            "2021-04-11 00:00:00\n",
            "Epoch: 0. Loss: 1.3279087214927803\n",
            "Epoch: 10. Loss: 1.0823555843316728\n",
            "Epoch: 20. Loss: 0.907054076928114\n",
            "Epoch: 30. Loss: 0.7989116655108598\n",
            "Epoch: 40. Loss: 0.7387520575486956\n",
            "Epoch: 50. Loss: 0.7038213344870042\n",
            "Epoch: 60. Loss: 0.6795927586411705\n",
            "Epoch: 70. Loss: 0.6603250972314068\n",
            "Epoch: 80. Loss: 0.6442435591081045\n",
            "Epoch: 90. Loss: 0.6307123287205243\n",
            "Epoch: 100. Loss: 0.619364317094483\n",
            "Epoch: 110. Loss: 0.6098966151186905\n",
            "Epoch: 120. Loss: 0.6020325468289124\n",
            "Epoch: 130. Loss: 0.5955198345211878\n",
            "Epoch: 140. Loss: 0.5901339579581992\n",
            "Epoch: 150. Loss: 0.5856800232504782\n",
            "Epoch: 160. Loss: 0.5819923669163596\n",
            "Epoch: 170. Loss: 0.5789324803847445\n",
            "Epoch: 180. Loss: 0.576385999620174\n",
            "Epoch: 190. Loss: 0.5742593524022231\n",
            "Epoch: 200. Loss: 0.5724764553696391\n",
            "Epoch: 210. Loss: 0.5709756839056686\n",
            "Epoch: 220. Loss: 0.5697072174156684\n",
            "Epoch: 230. Loss: 0.568630785706532\n",
            "Epoch: 240. Loss: 0.5677137981502731\n",
            "Epoch: 250. Loss: 0.5669298154621548\n",
            "Epoch: 260. Loss: 0.5662573158731253\n",
            "Epoch: 270. Loss: 0.5656787073386285\n",
            "Epoch: 280. Loss: 0.5651795413660825\n",
            "Epoch: 290. Loss: 0.5647478897177831\n",
            "Epoch: 300. Loss: 0.5643738513122927\n",
            "Epoch: 310. Loss: 0.5640491623973679\n",
            "Epoch: 320. Loss: 0.5637668881753132\n",
            "Epoch: 330. Loss: 0.5635211784202805\n",
            "Epoch: 340. Loss: 0.5633070732470041\n",
            "Epoch: 350. Loss: 0.5631203481392888\n",
            "Epoch: 360. Loss: 0.5629573897143006\n",
            "Epoch: 370. Loss: 0.5628150955787862\n",
            "Epoch: 380. Loss: 0.562690793113254\n",
            "Epoch: 390. Loss: 0.5625821731769782\n",
            "Epoch: 400. Loss: 0.5624872356260039\n",
            "Epoch: 410. Loss: 0.5624042442325158\n",
            "Epoch: 420. Loss: 0.562331689131136\n",
            "Epoch: 430. Loss: 0.5622682553313874\n",
            "Epoch: 440. Loss: 0.5622127961537449\n",
            "Epoch: 450. Loss: 0.5621643106914009\n",
            "Epoch: 460. Loss: 0.5621219245882365\n",
            "Epoch: 470. Loss: 0.5620848735687509\n",
            "Epoch: 480. Loss: 0.5620524892680512\n",
            "Epoch: 490. Loss: 0.5620241869972263\n",
            "tensor(0.8253, dtype=torch.float64)\n",
            "2021-04-18 00:00:00\n",
            "Epoch: 0. Loss: 1.2555819718419154\n",
            "Epoch: 10. Loss: 0.9385851333896189\n",
            "Epoch: 20. Loss: 0.7251921031578173\n",
            "Epoch: 30. Loss: 0.6123743079364339\n",
            "Epoch: 40. Loss: 0.5721979280636419\n",
            "Epoch: 50. Loss: 0.5606084539722684\n",
            "Epoch: 60. Loss: 0.5562862655799705\n",
            "Epoch: 70. Loss: 0.5540347969132069\n",
            "Epoch: 80. Loss: 0.5525928484272548\n",
            "Epoch: 90. Loss: 0.5515580708878315\n",
            "Epoch: 100. Loss: 0.550770209991463\n",
            "Epoch: 110. Loss: 0.55015235620494\n",
            "Epoch: 120. Loss: 0.5496605441493705\n",
            "Epoch: 130. Loss: 0.5492658559923397\n",
            "Epoch: 140. Loss: 0.5489474686816747\n",
            "Epoch: 150. Loss: 0.548689623479704\n",
            "Epoch: 160. Loss: 0.5484800909780764\n",
            "Epoch: 170. Loss: 0.5483092611786894\n",
            "Epoch: 180. Loss: 0.5481695324525836\n",
            "Epoch: 190. Loss: 0.5480548678970816\n",
            "Epoch: 200. Loss: 0.5479604594943124\n",
            "Epoch: 210. Loss: 0.5478824685730989\n",
            "Epoch: 220. Loss: 0.5478178232143448\n",
            "Epoch: 230. Loss: 0.5477640592968073\n",
            "Epoch: 240. Loss: 0.5477191954186212\n",
            "Epoch: 250. Loss: 0.5476816342793851\n",
            "Epoch: 260. Loss: 0.5476500847987509\n",
            "Epoch: 270. Loss: 0.5476235005182629\n",
            "Epoch: 280. Loss: 0.5476010308083529\n",
            "Epoch: 290. Loss: 0.5475819821581259\n",
            "Epoch: 300. Loss: 0.5475657874140156\n",
            "Epoch: 310. Loss: 0.547551981292732\n",
            "Epoch: 320. Loss: 0.5475401808530143\n",
            "Epoch: 330. Loss: 0.5475300698917529\n",
            "Epoch: 340. Loss: 0.5475213864501944\n",
            "Epoch: 350. Loss: 0.5475139127885755\n",
            "Epoch: 360. Loss: 0.5475074673230017\n",
            "Epoch: 370. Loss: 0.5475018981247926\n",
            "Epoch: 380. Loss: 0.5474970776661741\n",
            "Epoch: 390. Loss: 0.5474928985620322\n",
            "Epoch: 400. Loss: 0.5474892701092966\n",
            "Epoch: 410. Loss: 0.5474861154664061\n",
            "Epoch: 420. Loss: 0.5474833693475863\n",
            "Epoch: 430. Loss: 0.547480976132162\n",
            "Epoch: 440. Loss: 0.5474788883093059\n",
            "Epoch: 450. Loss: 0.5474770651945992\n",
            "Epoch: 460. Loss: 0.5474754718674514\n",
            "Epoch: 470. Loss: 0.5474740782884895\n",
            "Epoch: 480. Loss: 0.5474728585640357\n",
            "Epoch: 490. Loss: 0.5474717903311569\n",
            "tensor(0.8704, dtype=torch.float64)\n",
            "2021-04-25 00:00:00\n",
            "Epoch: 0. Loss: 0.9413599846976617\n",
            "Epoch: 10. Loss: 0.8482380747792642\n",
            "Epoch: 20. Loss: 0.7772951209646706\n",
            "Epoch: 30. Loss: 0.7250243689436776\n",
            "Epoch: 40. Loss: 0.6873554774464322\n",
            "Epoch: 50. Loss: 0.6598100343517572\n",
            "Epoch: 60. Loss: 0.6388253701007532\n",
            "Epoch: 70. Loss: 0.6222398334128433\n",
            "Epoch: 80. Loss: 0.6088641743424358\n",
            "Epoch: 90. Loss: 0.5980038078724933\n",
            "Epoch: 100. Loss: 0.5891912490809548\n",
            "Epoch: 110. Loss: 0.5820678362585741\n",
            "Epoch: 120. Loss: 0.5763372811511654\n",
            "Epoch: 130. Loss: 0.5717484255916769\n",
            "Epoch: 140. Loss: 0.568088309353734\n",
            "Epoch: 150. Loss: 0.5651781439572683\n",
            "Epoch: 160. Loss: 0.5628697037013631\n",
            "Epoch: 170. Loss: 0.5610415669073272\n",
            "Epoch: 180. Loss: 0.5595952855969799\n",
            "Epoch: 190. Loss: 0.5584516980099505\n",
            "Epoch: 200. Loss: 0.5575475585304642\n",
            "Epoch: 210. Loss: 0.5568325809392075\n",
            "Epoch: 220. Loss: 0.5562669228814949\n",
            "Epoch: 230. Loss: 0.555819094055435\n",
            "Epoch: 240. Loss: 0.5554642458078879\n",
            "Epoch: 250. Loss: 0.5551827897440683\n",
            "Epoch: 260. Loss: 0.5549592920087424\n",
            "Epoch: 270. Loss: 0.5547815938902995\n",
            "Epoch: 280. Loss: 0.5546401155801493\n",
            "Epoch: 290. Loss: 0.5545273066528212\n",
            "Epoch: 300. Loss: 0.5544372132412447\n",
            "Epoch: 310. Loss: 0.5543651375653478\n",
            "Epoch: 320. Loss: 0.5543073703024777\n",
            "Epoch: 330. Loss: 0.5542609802823504\n",
            "Epoch: 340. Loss: 0.5542236492321717\n",
            "Epoch: 350. Loss: 0.554193541897961\n",
            "Epoch: 360. Loss: 0.5541692039356411\n",
            "Epoch: 370. Loss: 0.554149481599849\n",
            "Epoch: 380. Loss: 0.5541334585454768\n",
            "Epoch: 390. Loss: 0.5541204060679578\n",
            "Epoch: 400. Loss: 0.5541097439012949\n",
            "Epoch: 410. Loss: 0.5541010093142739\n",
            "Epoch: 420. Loss: 0.5540938327321647\n",
            "Epoch: 430. Loss: 0.5540879184926136\n",
            "Epoch: 440. Loss: 0.5540830296432815\n",
            "Epoch: 450. Loss: 0.5540789759230323\n",
            "Epoch: 460. Loss: 0.5540756042521522\n",
            "Epoch: 470. Loss: 0.554072791201186\n",
            "Epoch: 480. Loss: 0.5540704370210856\n",
            "Epoch: 490. Loss: 0.5540684609061813\n",
            "tensor(0.7936, dtype=torch.float64)\n",
            "2021-05-02 00:00:00\n",
            "Epoch: 0. Loss: 0.9784626230441391\n",
            "Epoch: 10. Loss: 0.8046296063540953\n",
            "Epoch: 20. Loss: 0.6892192805972642\n",
            "Epoch: 30. Loss: 0.6307106280113015\n",
            "Epoch: 40. Loss: 0.6084886906270611\n",
            "Epoch: 50. Loss: 0.5996862516807533\n",
            "Epoch: 60. Loss: 0.5943434816303365\n",
            "Epoch: 70. Loss: 0.5900561867035343\n",
            "Epoch: 80. Loss: 0.5863344078279937\n",
            "Epoch: 90. Loss: 0.5830407350026405\n",
            "Epoch: 100. Loss: 0.5801059202335785\n",
            "Epoch: 110. Loss: 0.5774805915183452\n",
            "Epoch: 120. Loss: 0.5751253640622054\n",
            "Epoch: 130. Loss: 0.5730076729222745\n",
            "Epoch: 140. Loss: 0.5711001374301901\n",
            "Epoch: 150. Loss: 0.5693794688058169\n",
            "Epoch: 160. Loss: 0.5678256672882573\n",
            "Epoch: 170. Loss: 0.5664214105819478\n",
            "Epoch: 180. Loss: 0.5651515799147729\n",
            "Epoch: 190. Loss: 0.5640028884546664\n",
            "Epoch: 200. Loss: 0.5629635871231528\n",
            "Epoch: 210. Loss: 0.5620232295750803\n",
            "Epoch: 220. Loss: 0.5611724828241911\n",
            "Epoch: 230. Loss: 0.5604029733826034\n",
            "Epoch: 240. Loss: 0.5597071612510975\n",
            "Epoch: 250. Loss: 0.5590782359108403\n",
            "Epoch: 260. Loss: 0.5585100298100506\n",
            "Epoch: 270. Loss: 0.5579969458420645\n",
            "Epoch: 280. Loss: 0.5575338960680135\n",
            "Epoch: 290. Loss: 0.5571162495149594\n",
            "Epoch: 300. Loss: 0.5567397873267036\n",
            "Epoch: 310. Loss: 0.5564006638937405\n",
            "Epoch: 320. Loss: 0.5560953728651602\n",
            "Epoch: 330. Loss: 0.5558207171659774\n",
            "Epoch: 340. Loss: 0.5555737823207216\n",
            "Epoch: 350. Loss: 0.5553519125270959\n",
            "Epoch: 360. Loss: 0.5551526890387393\n",
            "Epoch: 370. Loss: 0.5549739105086765\n",
            "Epoch: 380. Loss: 0.5548135750188885\n",
            "Epoch: 390. Loss: 0.5546698635798601\n",
            "Epoch: 400. Loss: 0.5545411249296603\n",
            "Epoch: 410. Loss: 0.5544258614974363\n",
            "Epoch: 420. Loss: 0.5543227164231\n",
            "Epoch: 430. Loss: 0.5542304615451954\n",
            "Epoch: 440. Loss: 0.5541479862838595\n",
            "Epoch: 450. Loss: 0.5540742873566802\n",
            "Epoch: 460. Loss: 0.5540084592730679\n",
            "Epoch: 470. Loss: 0.5539496855583316\n",
            "Epoch: 480. Loss: 0.5538972306626195\n",
            "Epoch: 490. Loss: 0.5538504325127366\n",
            "tensor(0.8253, dtype=torch.float64)\n",
            "2021-05-09 00:00:00\n",
            "Epoch: 0. Loss: 1.5304580140734991\n",
            "Epoch: 10. Loss: 0.9201416510006851\n",
            "Epoch: 20. Loss: 0.7307982027276416\n",
            "Epoch: 30. Loss: 0.6583467877338944\n",
            "Epoch: 40. Loss: 0.6225035912035334\n",
            "Epoch: 50. Loss: 0.6011907156141283\n",
            "Epoch: 60. Loss: 0.5866957896696282\n",
            "Epoch: 70. Loss: 0.5762652247416862\n",
            "Epoch: 80. Loss: 0.5686800009022954\n",
            "Epoch: 90. Loss: 0.5631806486735315\n",
            "Epoch: 100. Loss: 0.5592093010297301\n",
            "Epoch: 110. Loss: 0.5563456648299382\n",
            "Epoch: 120. Loss: 0.554277558360803\n",
            "Epoch: 130. Loss: 0.5527773121394569\n",
            "Epoch: 140. Loss: 0.5516813106259337\n",
            "Epoch: 150. Loss: 0.5508731659214253\n",
            "Epoch: 160. Loss: 0.5502706052277623\n",
            "Epoch: 170. Loss: 0.549815648644683\n",
            "Epoch: 180. Loss: 0.5494674511417188\n",
            "Epoch: 190. Loss: 0.5491971834485674\n",
            "Epoch: 200. Loss: 0.5489844174077526\n",
            "Epoch: 210. Loss: 0.5488145948526449\n",
            "Epoch: 220. Loss: 0.5486772642604507\n",
            "Epoch: 230. Loss: 0.5485648555577265\n",
            "Epoch: 240. Loss: 0.548471829460162\n",
            "Epoch: 250. Loss: 0.5483940863378914\n",
            "Epoch: 260. Loss: 0.5483285544985177\n",
            "Epoch: 270. Loss: 0.5482729024213324\n",
            "Epoch: 280. Loss: 0.5482253366789477\n",
            "Epoch: 290. Loss: 0.5481844592017133\n",
            "Epoch: 300. Loss: 0.5481491657576799\n",
            "Epoch: 310. Loss: 0.5481185731686626\n",
            "Epoch: 320. Loss: 0.5480919666582454\n",
            "Epoch: 330. Loss: 0.5480687613852554\n",
            "Epoch: 340. Loss: 0.5480484740396752\n",
            "Epoch: 350. Loss: 0.5480307016306305\n",
            "Epoch: 360. Loss: 0.5480151054583974\n",
            "Epoch: 370. Loss: 0.5480013988576329\n",
            "Epoch: 380. Loss: 0.5479893377113706\n",
            "Epoch: 390. Loss: 0.5479787130221695\n",
            "Epoch: 400. Loss: 0.5479693450273212\n",
            "Epoch: 410. Loss: 0.5479610784859965\n",
            "Epoch: 420. Loss: 0.54795377886594\n",
            "Epoch: 430. Loss: 0.5479473292283777\n",
            "Epoch: 440. Loss: 0.5479416276608077\n",
            "Epoch: 450. Loss: 0.5479365851442609\n",
            "Epoch: 460. Loss: 0.5479321237685796\n",
            "Epoch: 470. Loss: 0.5479281752291082\n",
            "Epoch: 480. Loss: 0.5479246795529757\n",
            "Epoch: 490. Loss: 0.5479215840142264\n",
            "tensor(0.7784, dtype=torch.float64)\n",
            "2021-05-16 00:00:00\n",
            "Epoch: 0. Loss: 2.114650328909861\n",
            "Epoch: 10. Loss: 0.9579294641782631\n",
            "Epoch: 20. Loss: 0.7675250069744011\n",
            "Epoch: 30. Loss: 0.7091533170997923\n",
            "Epoch: 40. Loss: 0.6727533492987204\n",
            "Epoch: 50. Loss: 0.6460496623741613\n",
            "Epoch: 60. Loss: 0.6254927306268101\n",
            "Epoch: 70. Loss: 0.6095282900428568\n",
            "Epoch: 80. Loss: 0.5972214939998854\n",
            "Epoch: 90. Loss: 0.5878500169664646\n",
            "Epoch: 100. Loss: 0.5808018340664164\n",
            "Epoch: 110. Loss: 0.5755562394728215\n",
            "Epoch: 120. Loss: 0.5716810349944117\n",
            "Epoch: 130. Loss: 0.5688289908108346\n",
            "Epoch: 140. Loss: 0.5667301255243058\n",
            "Epoch: 150. Loss: 0.5651804515830217\n",
            "Epoch: 160. Loss: 0.564029268623408\n",
            "Epoch: 170. Loss: 0.5631669972014879\n",
            "Epoch: 180. Loss: 0.5625147426226141\n",
            "Epoch: 190. Loss: 0.5620159791698256\n",
            "Epoch: 200. Loss: 0.5616302422397333\n",
            "Epoch: 210. Loss: 0.5613284967294632\n",
            "Epoch: 220. Loss: 0.5610898071335947\n",
            "Epoch: 230. Loss: 0.560898974636494\n",
            "Epoch: 240. Loss: 0.5607448727706079\n",
            "Epoch: 250. Loss: 0.5606192789404852\n",
            "Epoch: 260. Loss: 0.5605160541924843\n",
            "Epoch: 270. Loss: 0.560430566123172\n",
            "Epoch: 280. Loss: 0.5603592811367847\n",
            "Epoch: 290. Loss: 0.5602994746858202\n",
            "Epoch: 300. Loss: 0.5602490239006576\n",
            "Epoch: 310. Loss: 0.560206257984076\n",
            "Epoch: 320. Loss: 0.5601698493258174\n",
            "Epoch: 330. Loss: 0.5601387335107182\n",
            "Epoch: 340. Loss: 0.5601120499829755\n",
            "Epoch: 350. Loss: 0.5600890975995702\n",
            "Epoch: 360. Loss: 0.5600693010104446\n",
            "Epoch: 370. Loss: 0.5600521849835214\n",
            "Epoch: 380. Loss: 0.5600373546141864\n",
            "Epoch: 390. Loss: 0.5600244799339211\n",
            "Epoch: 400. Loss: 0.5600132838380183\n",
            "Epoch: 410. Loss: 0.5600035325399978\n",
            "Epoch: 420. Loss: 0.5599950279661946\n",
            "Epoch: 430. Loss: 0.5599876016525165\n",
            "Epoch: 440. Loss: 0.55998110981345\n",
            "Epoch: 450. Loss: 0.5599754293327215\n",
            "Epoch: 460. Loss: 0.5599704544837464\n",
            "Epoch: 470. Loss: 0.5599660942318395\n",
            "Epoch: 480. Loss: 0.5599622700031491\n",
            "Epoch: 490. Loss: 0.559958913830301\n",
            "tensor(0.7874, dtype=torch.float64)\n",
            "2021-05-23 00:00:00\n",
            "Epoch: 0. Loss: 1.7603529379634086\n",
            "Epoch: 10. Loss: 1.5235842603148269\n",
            "Epoch: 20. Loss: 1.3068962013179304\n",
            "Epoch: 30. Loss: 1.1182282053530423\n",
            "Epoch: 40. Loss: 0.9656810790894864\n",
            "Epoch: 50. Loss: 0.853145389229729\n",
            "Epoch: 60. Loss: 0.7776372704681201\n",
            "Epoch: 70. Loss: 0.7293790269810865\n",
            "Epoch: 80. Loss: 0.6973106028865694\n",
            "Epoch: 90. Loss: 0.6740927293502471\n",
            "Epoch: 100. Loss: 0.6560134040327972\n",
            "Epoch: 110. Loss: 0.6413433689543522\n",
            "Epoch: 120. Loss: 0.629222681275844\n",
            "Epoch: 130. Loss: 0.6191418036457246\n",
            "Epoch: 140. Loss: 0.6107373649164026\n",
            "Epoch: 150. Loss: 0.6037195860958315\n",
            "Epoch: 160. Loss: 0.5978476736304703\n",
            "Epoch: 170. Loss: 0.592920472754772\n",
            "Epoch: 180. Loss: 0.5887711016441831\n",
            "Epoch: 190. Loss: 0.585262324102774\n",
            "Epoch: 200. Loss: 0.5822820767128585\n",
            "Epoch: 210. Loss: 0.5797392580999748\n",
            "Epoch: 220. Loss: 0.5775599526971648\n",
            "Epoch: 230. Loss: 0.5756841793139015\n",
            "Epoch: 240. Loss: 0.5740631734225724\n",
            "Epoch: 250. Loss: 0.5726571619654006\n",
            "Epoch: 260. Loss: 0.5714335668112375\n",
            "Epoch: 270. Loss: 0.5703655677578185\n",
            "Epoch: 280. Loss: 0.5694309599782946\n",
            "Epoch: 290. Loss: 0.5686112488038517\n",
            "Epoch: 300. Loss: 0.567890933782675\n",
            "Epoch: 310. Loss: 0.5672569426005694\n",
            "Epoch: 320. Loss: 0.5666981830757536\n",
            "Epoch: 330. Loss: 0.5662051878794429\n",
            "Epoch: 340. Loss: 0.565769831927609\n",
            "Epoch: 350. Loss: 0.5653851066685542\n",
            "Epoch: 360. Loss: 0.5650449389110533\n",
            "Epoch: 370. Loss: 0.5647440445493074\n",
            "Epoch: 380. Loss: 0.5644778096774146\n",
            "Epoch: 390. Loss: 0.5642421932609617\n",
            "Epoch: 400. Loss: 0.564033646840661\n",
            "Epoch: 410. Loss: 0.5638490477593471\n",
            "Epoch: 420. Loss: 0.5636856431910737\n",
            "Epoch: 430. Loss: 0.5635410028591403\n",
            "Epoch: 440. Loss: 0.5634129787982529\n",
            "Epoch: 450. Loss: 0.5632996708760325\n",
            "Epoch: 460. Loss: 0.5631993970654392\n",
            "Epoch: 470. Loss: 0.5631106676717531\n",
            "Epoch: 480. Loss: 0.5630321628805909\n",
            "Epoch: 490. Loss: 0.5629627131187298\n",
            "tensor(0.7074, dtype=torch.float64)\n",
            "2021-05-30 00:00:00\n",
            "Epoch: 0. Loss: 1.7390533096744316\n",
            "Epoch: 10. Loss: 0.7077019577018867\n",
            "Epoch: 20. Loss: 0.6119176812711544\n",
            "Epoch: 30. Loss: 0.590312608079111\n",
            "Epoch: 40. Loss: 0.5799175521282368\n",
            "Epoch: 50. Loss: 0.572982911665182\n",
            "Epoch: 60. Loss: 0.5677474780010527\n",
            "Epoch: 70. Loss: 0.5636481540695275\n",
            "Epoch: 80. Loss: 0.5604067848528853\n",
            "Epoch: 90. Loss: 0.5578358042652041\n",
            "Epoch: 100. Loss: 0.5557929282270724\n",
            "Epoch: 110. Loss: 0.554166808735155\n",
            "Epoch: 120. Loss: 0.5528697767358152\n",
            "Epoch: 130. Loss: 0.5518327974783069\n",
            "Epoch: 140. Loss: 0.5510015497644898\n",
            "Epoch: 150. Loss: 0.5503333060462138\n",
            "Epoch: 160. Loss: 0.5497944514387991\n",
            "Epoch: 170. Loss: 0.5493585212648888\n",
            "Epoch: 180. Loss: 0.5490046564029082\n",
            "Epoch: 190. Loss: 0.5487163918059554\n",
            "Epoch: 200. Loss: 0.5484807084244778\n",
            "Epoch: 210. Loss: 0.5482872920935896\n",
            "Epoch: 220. Loss: 0.5481279543775293\n",
            "Epoch: 230. Loss: 0.5479961798401681\n",
            "Epoch: 240. Loss: 0.5478867718811714\n",
            "Epoch: 250. Loss: 0.5477955753868503\n",
            "Epoch: 260. Loss: 0.5477192592583434\n",
            "Epoch: 270. Loss: 0.5476551456455866\n",
            "Epoch: 280. Loss: 0.5476010756484624\n",
            "Epoch: 290. Loss: 0.5475553035247177\n",
            "Epoch: 300. Loss: 0.5475164132115053\n",
            "Epoch: 310. Loss: 0.547483252337721\n",
            "Epoch: 320. Loss: 0.547454879967076\n",
            "Epoch: 330. Loss: 0.5474305251366496\n",
            "Epoch: 340. Loss: 0.5474095538963907\n",
            "Epoch: 350. Loss: 0.5473914430533773\n",
            "Epoch: 360. Loss: 0.5473757592127201\n",
            "Epoch: 370. Loss: 0.5473621420096358\n",
            "Epoch: 380. Loss: 0.5473502906635495\n",
            "Epoch: 390. Loss: 0.5473399531699095\n",
            "Epoch: 400. Loss: 0.5473309175901306\n",
            "Epoch: 410. Loss: 0.5473230050135892\n",
            "Epoch: 420. Loss: 0.547316063854716\n",
            "Epoch: 430. Loss: 0.5473099652183285\n",
            "Epoch: 440. Loss: 0.5473045991215176\n",
            "Epoch: 450. Loss: 0.5472998714039131\n",
            "Epoch: 460. Loss: 0.5472957011924956\n",
            "Epoch: 470. Loss: 0.5472920188142715\n",
            "Epoch: 480. Loss: 0.5472887640716141\n",
            "Epoch: 490. Loss: 0.5472858848121027\n",
            "tensor(0.6966, dtype=torch.float64)\n",
            "2021-06-06 00:00:00\n",
            "Epoch: 0. Loss: 1.104079667911524\n",
            "Epoch: 10. Loss: 0.9600071337786988\n",
            "Epoch: 20. Loss: 0.8649902102168885\n",
            "Epoch: 30. Loss: 0.7983940173018311\n",
            "Epoch: 40. Loss: 0.7472333920203913\n",
            "Epoch: 50. Loss: 0.7058014392109621\n",
            "Epoch: 60. Loss: 0.6718757627823447\n",
            "Epoch: 70. Loss: 0.6444012162019062\n",
            "Epoch: 80. Loss: 0.6225807096075161\n",
            "Epoch: 90. Loss: 0.6056110417955973\n",
            "Epoch: 100. Loss: 0.5926591187990564\n",
            "Epoch: 110. Loss: 0.5829137280151976\n",
            "Epoch: 120. Loss: 0.5756428463241232\n",
            "Epoch: 130. Loss: 0.5702293317652553\n",
            "Epoch: 140. Loss: 0.5661810466463904\n",
            "Epoch: 150. Loss: 0.5631222188286455\n",
            "Epoch: 160. Loss: 0.5607751442529534\n",
            "Epoch: 170. Loss: 0.5589393740163904\n",
            "Epoch: 180. Loss: 0.5574725351445967\n",
            "Epoch: 190. Loss: 0.5562745122181555\n",
            "Epoch: 200. Loss: 0.555275251680751\n",
            "Epoch: 210. Loss: 0.5544257601400465\n",
            "Epoch: 220. Loss: 0.5536916499213741\n",
            "Epoch: 230. Loss: 0.5530486005170813\n",
            "Epoch: 240. Loss: 0.5524792090840916\n",
            "Epoch: 250. Loss: 0.5519708242215091\n",
            "Epoch: 260. Loss: 0.5515140648144548\n",
            "Epoch: 270. Loss: 0.5511018110908007\n",
            "Epoch: 280. Loss: 0.5507285188121176\n",
            "Epoch: 290. Loss: 0.5503897534849109\n",
            "Epoch: 300. Loss: 0.5500818738664265\n",
            "Epoch: 310. Loss: 0.5498018165291645\n",
            "Epoch: 320. Loss: 0.5495469487147997\n",
            "Epoch: 330. Loss: 0.5493149672766506\n",
            "Epoch: 340. Loss: 0.5491038287004837\n",
            "Epoch: 350. Loss: 0.5489117000717826\n",
            "Epoch: 360. Loss: 0.5487369241607034\n",
            "Epoch: 370. Loss: 0.5485779940292358\n",
            "Epoch: 380. Loss: 0.548433534073317\n",
            "Epoch: 390. Loss: 0.5483022854300569\n",
            "Epoch: 400. Loss: 0.5481830943656741\n",
            "Epoch: 410. Loss: 0.5480749027207636\n",
            "Epoch: 420. Loss: 0.5479767397989483\n",
            "Epoch: 430. Loss: 0.5478877152920294\n",
            "Epoch: 440. Loss: 0.5478070129728064\n",
            "Epoch: 450. Loss: 0.5477338849783119\n",
            "Epoch: 460. Loss: 0.5476676465665594\n",
            "Epoch: 470. Loss: 0.5476076712693635\n",
            "Epoch: 480. Loss: 0.5475533863892984\n",
            "Epoch: 490. Loss: 0.5475042688051526\n",
            "tensor(0.8260, dtype=torch.float64)\n",
            "2021-06-13 00:00:00\n",
            "Epoch: 0. Loss: 2.103407540968126\n",
            "Epoch: 10. Loss: 0.8312756500145134\n",
            "Epoch: 20. Loss: 0.6563649550199189\n",
            "Epoch: 30. Loss: 0.6172999977870475\n",
            "Epoch: 40. Loss: 0.5978591429362458\n",
            "Epoch: 50. Loss: 0.5851985265589924\n",
            "Epoch: 60. Loss: 0.5758361318104122\n",
            "Epoch: 70. Loss: 0.5685510343556921\n",
            "Epoch: 80. Loss: 0.5627942913891543\n",
            "Epoch: 90. Loss: 0.5582307212993354\n",
            "Epoch: 100. Loss: 0.5546135906185182\n",
            "Epoch: 110. Loss: 0.5517487189017111\n",
            "Epoch: 120. Loss: 0.5494809085791688\n",
            "Epoch: 130. Loss: 0.5476861227947136\n",
            "Epoch: 140. Loss: 0.546265573522813\n",
            "Epoch: 150. Loss: 0.545140857212337\n",
            "Epoch: 160. Loss: 0.544249916190574\n",
            "Epoch: 170. Loss: 0.5435437196694928\n",
            "Epoch: 180. Loss: 0.542983569646953\n",
            "Epoch: 190. Loss: 0.5425389367385762\n",
            "Epoch: 200. Loss: 0.5421857353966123\n",
            "Epoch: 210. Loss: 0.5419049570746639\n",
            "Epoch: 220. Loss: 0.5416815912263938\n",
            "Epoch: 230. Loss: 0.5415037756178752\n",
            "Epoch: 240. Loss: 0.541362128153424\n",
            "Epoch: 250. Loss: 0.5412492217565134\n",
            "Epoch: 260. Loss: 0.5411591716865212\n",
            "Epoch: 270. Loss: 0.541087311087928\n",
            "Epoch: 280. Loss: 0.5410299357313992\n",
            "Epoch: 290. Loss: 0.5409841030128907\n",
            "Epoch: 300. Loss: 0.5409474735180235\n",
            "Epoch: 310. Loss: 0.5409181860036553\n",
            "Epoch: 320. Loss: 0.5408947586399282\n",
            "Epoch: 330. Loss: 0.5408760109114847\n",
            "Epoch: 340. Loss: 0.5408610017904266\n",
            "Epoch: 350. Loss: 0.54084898074083\n",
            "Epoch: 360. Loss: 0.5408393488541523\n",
            "Epoch: 370. Loss: 0.5408316279926876\n",
            "Epoch: 380. Loss: 0.5408254362702211\n",
            "Epoch: 390. Loss: 0.5408204685530448\n",
            "Epoch: 400. Loss: 0.5408164809421577\n",
            "Epoch: 410. Loss: 0.5408132784155623\n",
            "Epoch: 420. Loss: 0.540810704981111\n",
            "Epoch: 430. Loss: 0.540808635825486\n",
            "Epoch: 440. Loss: 0.5408069710514802\n",
            "Epoch: 450. Loss: 0.5408056306799355\n",
            "Epoch: 460. Loss: 0.5408045506592639\n",
            "Epoch: 470. Loss: 0.5408036796781961\n",
            "Epoch: 480. Loss: 0.540802976619173\n",
            "Epoch: 490. Loss: 0.5408024085229476\n",
            "tensor(0.6093, dtype=torch.float64)\n",
            "2021-06-20 00:00:00\n",
            "Epoch: 0. Loss: 0.8926178734168608\n",
            "Epoch: 10. Loss: 0.7485815287746704\n",
            "Epoch: 20. Loss: 0.6680270337067276\n",
            "Epoch: 30. Loss: 0.6269675052811631\n",
            "Epoch: 40. Loss: 0.6049798582500496\n",
            "Epoch: 50. Loss: 0.5915383844099443\n",
            "Epoch: 60. Loss: 0.5824485404292177\n",
            "Epoch: 70. Loss: 0.5760186700907303\n",
            "Epoch: 80. Loss: 0.5713956435174492\n",
            "Epoch: 90. Loss: 0.5680440414956115\n",
            "Epoch: 100. Loss: 0.5655924421621388\n",
            "Epoch: 110. Loss: 0.5637770694456997\n",
            "Epoch: 120. Loss: 0.5624111816439008\n",
            "Epoch: 130. Loss: 0.5613636587031232\n",
            "Epoch: 140. Loss: 0.5605430491676109\n",
            "Epoch: 150. Loss: 0.5598857996290898\n",
            "Epoch: 160. Loss: 0.5593477657205318\n",
            "Epoch: 170. Loss: 0.5588982099484402\n",
            "Epoch: 180. Loss: 0.5585156184943906\n",
            "Epoch: 190. Loss: 0.558184813800993\n",
            "Epoch: 200. Loss: 0.5578949736639198\n",
            "Epoch: 210. Loss: 0.557638276936121\n",
            "Epoch: 220. Loss: 0.5574089790429302\n",
            "Epoch: 230. Loss: 0.5572027809197991\n",
            "Epoch: 240. Loss: 0.5570163977269877\n",
            "Epoch: 250. Loss: 0.5568472634186313\n",
            "Epoch: 260. Loss: 0.5566933276866977\n",
            "Epoch: 270. Loss: 0.5565529157628893\n",
            "Epoch: 280. Loss: 0.5564246310566283\n",
            "Epoch: 290. Loss: 0.5563072870477651\n",
            "Epoch: 300. Loss: 0.5561998592154358\n",
            "Epoch: 310. Loss: 0.5561014507384263\n",
            "Epoch: 320. Loss: 0.5560112677028147\n",
            "Epoch: 330. Loss: 0.5559286009082333\n",
            "Epoch: 340. Loss: 0.5558528122836969\n",
            "Epoch: 350. Loss: 0.5557833245486785\n",
            "Epoch: 360. Loss: 0.55571961318029\n",
            "Epoch: 370. Loss: 0.5556612000373878\n",
            "Epoch: 380. Loss: 0.5556076481906629\n",
            "Epoch: 390. Loss: 0.5555585576436761\n",
            "Epoch: 400. Loss: 0.555513561723267\n",
            "Epoch: 410. Loss: 0.5554723239822746\n",
            "Epoch: 420. Loss: 0.5554345355022147\n",
            "Epoch: 430. Loss: 0.5553999125146752\n",
            "Epoch: 440. Loss: 0.5553681942819765\n",
            "Epoch: 450. Loss: 0.5553391411929555\n",
            "Epoch: 460. Loss: 0.555312533040602\n",
            "Epoch: 470. Loss: 0.5552881674560087\n",
            "Epoch: 480. Loss: 0.5552658584786834\n",
            "Epoch: 490. Loss: 0.5552454352473121\n",
            "tensor(0.9165, dtype=torch.float64)\n",
            "2021-06-27 00:00:00\n",
            "Epoch: 0. Loss: 0.6983718994098931\n",
            "Epoch: 10. Loss: 0.6035175094006361\n",
            "Epoch: 20. Loss: 0.5894694057804132\n",
            "Epoch: 30. Loss: 0.5836891889643515\n",
            "Epoch: 40. Loss: 0.5798202133937005\n",
            "Epoch: 50. Loss: 0.5767141931283678\n",
            "Epoch: 60. Loss: 0.5740691307987233\n",
            "Epoch: 70. Loss: 0.5717610370295196\n",
            "Epoch: 80. Loss: 0.5697164181944261\n",
            "Epoch: 90. Loss: 0.5678840993941957\n",
            "Epoch: 100. Loss: 0.5662265871900536\n",
            "Epoch: 110. Loss: 0.5647158959552263\n",
            "Epoch: 120. Loss: 0.563330864315956\n",
            "Epoch: 130. Loss: 0.5620552602218316\n",
            "Epoch: 140. Loss: 0.5608764179836359\n",
            "Epoch: 150. Loss: 0.5597842586361007\n",
            "Epoch: 160. Loss: 0.5587705885647647\n",
            "Epoch: 170. Loss: 0.5578285990174082\n",
            "Epoch: 180. Loss: 0.556952509654549\n",
            "Epoch: 190. Loss: 0.5561373147908263\n",
            "Epoch: 200. Loss: 0.555378602528299\n",
            "Epoch: 210. Loss: 0.5546724254641193\n",
            "Epoch: 220. Loss: 0.554015207809803\n",
            "Epoch: 230. Loss: 0.5534036781850737\n",
            "Epoch: 240. Loss: 0.5528348205095134\n",
            "Epoch: 250. Loss: 0.5523058376594985\n",
            "Epoch: 260. Loss: 0.5518141241447143\n",
            "Epoch: 270. Loss: 0.5513572451764727\n",
            "Epoch: 280. Loss: 0.5509329202854113\n",
            "Epoch: 290. Loss: 0.5505390101965488\n",
            "Epoch: 300. Loss: 0.5501735060547197\n",
            "Epoch: 310. Loss: 0.549834520362485\n",
            "Epoch: 320. Loss: 0.5495202791805318\n",
            "Epoch: 330. Loss: 0.5492291152718977\n",
            "Epoch: 340. Loss: 0.5489594619632381\n",
            "Epoch: 350. Loss: 0.5487098475608305\n",
            "Epoch: 360. Loss: 0.5484788902044324\n",
            "Epoch: 370. Loss: 0.5482652930742866\n",
            "Epoch: 380. Loss: 0.5480678398895253\n",
            "Epoch: 390. Loss: 0.5478853906527094\n",
            "Epoch: 400. Loss: 0.5477168776072101\n",
            "Epoch: 410. Loss: 0.5475613013828609\n",
            "Epoch: 420. Loss: 0.5474177273117296\n",
            "Epoch: 430. Loss: 0.5472852819005987\n",
            "Epoch: 440. Loss: 0.5471631494502252\n",
            "Epoch: 450. Loss: 0.5470505688140261\n",
            "Epoch: 460. Loss: 0.5469468302906836\n",
            "Epoch: 470. Loss: 0.54685127264649\n",
            "Epoch: 480. Loss: 0.5467632802641625\n",
            "Epoch: 490. Loss: 0.5466822804154332\n",
            "tensor(0.6518, dtype=torch.float64)\n",
            "2021-07-04 00:00:00\n",
            "Epoch: 0. Loss: 1.2113986672163566\n",
            "Epoch: 10. Loss: 0.9417191104918664\n",
            "Epoch: 20. Loss: 0.7978299863098172\n",
            "Epoch: 30. Loss: 0.7003030431625408\n",
            "Epoch: 40. Loss: 0.6403719106648561\n",
            "Epoch: 50. Loss: 0.6095345110372936\n",
            "Epoch: 60. Loss: 0.5940781444827777\n",
            "Epoch: 70. Loss: 0.5848603539603592\n",
            "Epoch: 80. Loss: 0.5782847073788938\n",
            "Epoch: 90. Loss: 0.5731380451861599\n",
            "Epoch: 100. Loss: 0.5689568249056943\n",
            "Epoch: 110. Loss: 0.5655097052951226\n",
            "Epoch: 120. Loss: 0.562648290535627\n",
            "Epoch: 130. Loss: 0.5602623635472409\n",
            "Epoch: 140. Loss: 0.5582647864192246\n",
            "Epoch: 150. Loss: 0.5565851559481801\n",
            "Epoch: 160. Loss: 0.5551662482125445\n",
            "Epoch: 170. Loss: 0.5539615160632795\n",
            "Epoch: 180. Loss: 0.5529331183165973\n",
            "Epoch: 190. Loss: 0.5520503076059728\n",
            "Epoch: 200. Loss: 0.5512881021159236\n",
            "Epoch: 210. Loss: 0.550626194258225\n",
            "Epoch: 220. Loss: 0.5500480585499345\n",
            "Epoch: 230. Loss: 0.5495402258491797\n",
            "Epoch: 240. Loss: 0.5490916952480632\n",
            "Epoch: 250. Loss: 0.5486934589283058\n",
            "Epoch: 260. Loss: 0.5483381190933921\n",
            "Epoch: 270. Loss: 0.5480195795653415\n",
            "Epoch: 280. Loss: 0.547732797693576\n",
            "Epoch: 290. Loss: 0.5474735848462348\n",
            "Epoch: 300. Loss: 0.547238445959213\n",
            "Epoch: 310. Loss: 0.5470244504453438\n",
            "Epoch: 320. Loss: 0.5468291282642671\n",
            "Epoch: 330. Loss: 0.5466503861723343\n",
            "Epoch: 340. Loss: 0.5464864401577959\n",
            "Epoch: 350. Loss: 0.5463357608606823\n",
            "Epoch: 360. Loss: 0.5461970294145927\n",
            "Epoch: 370. Loss: 0.5460691016587768\n",
            "Epoch: 380. Loss: 0.5459509790780138\n",
            "Epoch: 390. Loss: 0.54584178515496\n",
            "Epoch: 400. Loss: 0.5457407460811761\n",
            "Epoch: 410. Loss: 0.5456471749820913\n",
            "Epoch: 420. Loss: 0.5455604589783031\n",
            "Epoch: 430. Loss: 0.5454800485392731\n",
            "Epoch: 440. Loss: 0.5454054486924441\n",
            "Epoch: 450. Loss: 0.5453362117364366\n",
            "Epoch: 460. Loss: 0.5452719311756039\n",
            "Epoch: 470. Loss: 0.5452122366482289\n",
            "Epoch: 480. Loss: 0.5451567896647945\n",
            "Epoch: 490. Loss: 0.5451052800081913\n",
            "tensor(0.8014, dtype=torch.float64)\n",
            "2021-07-11 00:00:00\n",
            "Epoch: 0. Loss: 1.8863981607701092\n",
            "Epoch: 10. Loss: 0.6143420137213218\n",
            "Epoch: 20. Loss: 0.5777608468295331\n",
            "Epoch: 30. Loss: 0.5701811058501017\n",
            "Epoch: 40. Loss: 0.5661491386323745\n",
            "Epoch: 50. Loss: 0.5633395848497583\n",
            "Epoch: 60. Loss: 0.5611537022174414\n",
            "Epoch: 70. Loss: 0.5593584433089663\n",
            "Epoch: 80. Loss: 0.5578401184706042\n",
            "Epoch: 90. Loss: 0.5565317421879796\n",
            "Epoch: 100. Loss: 0.55538830514243\n",
            "Epoch: 110. Loss: 0.5543772996334724\n",
            "Epoch: 120. Loss: 0.5534744027810085\n",
            "Epoch: 130. Loss: 0.5526610848300166\n",
            "Epoch: 140. Loss: 0.5519230666105749\n",
            "Epoch: 150. Loss: 0.5512492357753369\n",
            "Epoch: 160. Loss: 0.5506308555738735\n",
            "Epoch: 170. Loss: 0.5500609787623711\n",
            "Epoch: 180. Loss: 0.5495340114949322\n",
            "Epoch: 190. Loss: 0.5490453884648765\n",
            "Epoch: 200. Loss: 0.5485913308178226\n",
            "Epoch: 210. Loss: 0.5481686655932092\n",
            "Epoch: 220. Loss: 0.5477746908236917\n",
            "Epoch: 230. Loss: 0.5474070744686562\n",
            "Epoch: 240. Loss: 0.5470637784047842\n",
            "Epoch: 250. Loss: 0.5467430009788544\n",
            "Epoch: 260. Loss: 0.5464431333278907\n",
            "Epoch: 270. Loss: 0.5461627259316003\n",
            "Epoch: 280. Loss: 0.5459004627920537\n",
            "Epoch: 290. Loss: 0.5456551413200618\n",
            "Epoch: 300. Loss: 0.5454256565105949\n",
            "Epoch: 310. Loss: 0.5452109883586814\n",
            "Epoch: 320. Loss: 0.545010191738076\n",
            "Epoch: 330. Loss: 0.5448223881639029\n",
            "Epoch: 340. Loss: 0.544646759006762\n",
            "Epoch: 350. Loss: 0.5444825398335903\n",
            "Epoch: 360. Loss: 0.5443290156302616\n",
            "Epoch: 370. Loss: 0.544185516720005\n",
            "Epoch: 380. Loss: 0.5440514152357407\n",
            "Epoch: 390. Loss: 0.543926122037363\n",
            "Epoch: 400. Loss: 0.5438090839897757\n",
            "Epoch: 410. Loss: 0.5436997815362101\n",
            "Epoch: 420. Loss: 0.5435977265156069\n",
            "Epoch: 430. Loss: 0.5435024601837424\n",
            "Epoch: 440. Loss: 0.5434135514061728\n",
            "Epoch: 450. Loss: 0.5433305949975599\n",
            "Epoch: 460. Loss: 0.5432532101870055\n",
            "Epoch: 470. Loss: 0.543181039192969\n",
            "Epoch: 480. Loss: 0.543113745894462\n",
            "Epoch: 490. Loss: 0.5430510145876636\n",
            "tensor(0.7804, dtype=torch.float64)\n",
            "2021-07-18 00:00:00\n",
            "Epoch: 0. Loss: 1.063485681217728\n",
            "Epoch: 10. Loss: 0.8691892409508891\n",
            "Epoch: 20. Loss: 0.7382618807826814\n",
            "Epoch: 30. Loss: 0.6582941679129874\n",
            "Epoch: 40. Loss: 0.6132184169449532\n",
            "Epoch: 50. Loss: 0.589512592998694\n",
            "Epoch: 60. Loss: 0.5765798433444423\n",
            "Epoch: 70. Loss: 0.5687001272601017\n",
            "Epoch: 80. Loss: 0.5633992196319995\n",
            "Epoch: 90. Loss: 0.5596199959697761\n",
            "Epoch: 100. Loss: 0.5568476619328145\n",
            "Epoch: 110. Loss: 0.5547840348086399\n",
            "Epoch: 120. Loss: 0.5532323696870157\n",
            "Epoch: 130. Loss: 0.5520538061582216\n",
            "Epoch: 140. Loss: 0.5511477907474636\n",
            "Epoch: 150. Loss: 0.5504410811870534\n",
            "Epoch: 160. Loss: 0.5498804175440789\n",
            "Epoch: 170. Loss: 0.5494272120546771\n",
            "Epoch: 180. Loss: 0.5490536014578729\n",
            "Epoch: 190. Loss: 0.5487395107826049\n",
            "Epoch: 200. Loss: 0.5484704863305969\n",
            "Epoch: 210. Loss: 0.548236111045195\n",
            "Epoch: 220. Loss: 0.5480288554262196\n",
            "Epoch: 230. Loss: 0.5478432500512548\n",
            "Epoch: 240. Loss: 0.5476752929791744\n",
            "Epoch: 250. Loss: 0.5475220271830199\n",
            "Epoch: 260. Loss: 0.5473812402079442\n",
            "Epoch: 270. Loss: 0.547251251211368\n",
            "Epoch: 280. Loss: 0.5471307602095886\n",
            "Epoch: 290. Loss: 0.5470187414617935\n",
            "Epoch: 300. Loss: 0.5469143680903628\n",
            "Epoch: 310. Loss: 0.5468169587635475\n",
            "Epoch: 320. Loss: 0.5467259399378037\n",
            "Epoch: 330. Loss: 0.5466408190621125\n",
            "Epoch: 340. Loss: 0.5465611655000746\n",
            "Epoch: 350. Loss: 0.5464865968842554\n",
            "Epoch: 360. Loss: 0.5464167692946629\n",
            "Epoch: 370. Loss: 0.5463513701309869\n",
            "Epoch: 380. Loss: 0.5462901128846039\n",
            "Epoch: 390. Loss: 0.5462327332529262\n",
            "Epoch: 400. Loss: 0.5461789862048346\n",
            "Epoch: 410. Loss: 0.546128643722595\n",
            "Epoch: 420. Loss: 0.5460814930274613\n",
            "Epoch: 430. Loss: 0.546037335153538\n",
            "Epoch: 440. Loss: 0.5459959837746684\n",
            "Epoch: 450. Loss: 0.5459572642172849\n",
            "Epoch: 460. Loss: 0.545921012611888\n",
            "Epoch: 470. Loss: 0.5458870751496508\n",
            "Epoch: 480. Loss: 0.5458553074203376\n",
            "Epoch: 490. Loss: 0.5458255738145122\n",
            "tensor(0.7266, dtype=torch.float64)\n",
            "2021-07-25 00:00:00\n",
            "Epoch: 0. Loss: 1.708072300264083\n",
            "Epoch: 10. Loss: 1.0006483942252058\n",
            "Epoch: 20. Loss: 0.8025839649916894\n",
            "Epoch: 30. Loss: 0.7045271876444398\n",
            "Epoch: 40. Loss: 0.6487793342196959\n",
            "Epoch: 50. Loss: 0.6200764226932549\n",
            "Epoch: 60. Loss: 0.6046129311950288\n",
            "Epoch: 70. Loss: 0.5944540157886351\n",
            "Epoch: 80. Loss: 0.5866829324601023\n",
            "Epoch: 90. Loss: 0.580330712978859\n",
            "Epoch: 100. Loss: 0.575020326497971\n",
            "Epoch: 110. Loss: 0.5705517201418668\n",
            "Epoch: 120. Loss: 0.5667858525878908\n",
            "Epoch: 130. Loss: 0.5636123588830687\n",
            "Epoch: 140. Loss: 0.5609395146715005\n",
            "Epoch: 150. Loss: 0.5586900481834897\n",
            "Epoch: 160. Loss: 0.556798593652615\n",
            "Epoch: 170. Loss: 0.5552097575941117\n",
            "Epoch: 180. Loss: 0.5538765426301152\n",
            "Epoch: 190. Loss: 0.5527590446137005\n",
            "Epoch: 200. Loss: 0.5518233727895281\n",
            "Epoch: 210. Loss: 0.5510407513106035\n",
            "Epoch: 220. Loss: 0.5503867666346279\n",
            "Epoch: 230. Loss: 0.5498407319713957\n",
            "Epoch: 240. Loss: 0.549385146327131\n",
            "Epoch: 250. Loss: 0.5490052310881103\n",
            "Epoch: 260. Loss: 0.5486885312679617\n",
            "Epoch: 270. Loss: 0.5484245715934579\n",
            "Epoch: 280. Loss: 0.5482045597430658\n",
            "Epoch: 290. Loss: 0.5480211305247462\n",
            "Epoch: 300. Loss: 0.5478681257963103\n",
            "Epoch: 310. Loss: 0.5477404056548267\n",
            "Epoch: 320. Loss: 0.5476336869641624\n",
            "Epoch: 330. Loss: 0.5475444057249739\n",
            "Epoch: 340. Loss: 0.5474696001633764\n",
            "Epoch: 350. Loss: 0.547406811747517\n",
            "Epoch: 360. Loss: 0.5473540016474431\n",
            "Epoch: 370. Loss: 0.5473094804379655\n",
            "Epoch: 380. Loss: 0.5472718491080691\n",
            "Epoch: 390. Loss: 0.5472399496836181\n",
            "Epoch: 400. Loss: 0.5472128239921683\n",
            "Epoch: 410. Loss: 0.5471896792993456\n",
            "Epoch: 420. Loss: 0.5471698597256922\n",
            "Epoch: 430. Loss: 0.5471528225118055\n",
            "Epoch: 440. Loss: 0.5471381183390892\n",
            "Epoch: 450. Loss: 0.5471253750349059\n",
            "Epoch: 460. Loss: 0.5471142840959357\n",
            "Epoch: 470. Loss: 0.5471045895537383\n",
            "Epoch: 480. Loss: 0.5470960787835832\n",
            "Epoch: 490. Loss: 0.5470885749230828\n",
            "tensor(0.8445, dtype=torch.float64)\n",
            "2021-08-01 00:00:00\n",
            "Epoch: 0. Loss: 1.2457578636532465\n",
            "Epoch: 10. Loss: 1.1496667536692802\n",
            "Epoch: 20. Loss: 1.0695910560060657\n",
            "Epoch: 30. Loss: 1.0017503028259764\n",
            "Epoch: 40. Loss: 0.9432109901097742\n",
            "Epoch: 50. Loss: 0.8914528618178379\n",
            "Epoch: 60. Loss: 0.8447607896696347\n",
            "Epoch: 70. Loss: 0.8022385469442487\n",
            "Epoch: 80. Loss: 0.763529486169844\n",
            "Epoch: 90. Loss: 0.7285472032446004\n",
            "Epoch: 100. Loss: 0.6973011470888358\n",
            "Epoch: 110. Loss: 0.6697981455464068\n",
            "Epoch: 120. Loss: 0.6459905607037958\n",
            "Epoch: 130. Loss: 0.6257527421149907\n",
            "Epoch: 140. Loss: 0.6088761883178552\n",
            "Epoch: 150. Loss: 0.5950774953234101\n",
            "Epoch: 160. Loss: 0.5840143533487068\n",
            "Epoch: 170. Loss: 0.5753063967241623\n",
            "Epoch: 180. Loss: 0.5685596053142575\n",
            "Epoch: 190. Loss: 0.5633925556862508\n",
            "Epoch: 200. Loss: 0.5594600064189071\n",
            "Epoch: 210. Loss: 0.5564683835486475\n",
            "Epoch: 220. Loss: 0.554180927012905\n",
            "Epoch: 230. Loss: 0.5524144253152703\n",
            "Epoch: 240. Loss: 0.5510313124909597\n",
            "Epoch: 250. Loss: 0.5499304115376871\n",
            "Epoch: 260. Loss: 0.549038285158941\n",
            "Epoch: 270. Loss: 0.5483020203011646\n",
            "Epoch: 280. Loss: 0.5476835928126964\n",
            "Epoch: 290. Loss: 0.5471556430252943\n",
            "Epoch: 300. Loss: 0.5466983916549728\n",
            "Epoch: 310. Loss: 0.5462974269306768\n",
            "Epoch: 320. Loss: 0.5459421347600715\n",
            "Epoch: 330. Loss: 0.5456245930552864\n",
            "Epoch: 340. Loss: 0.5453387961937158\n",
            "Epoch: 350. Loss: 0.5450801119845983\n",
            "Epoch: 360. Loss: 0.5448449013240633\n",
            "Epoch: 370. Loss: 0.5446302512277588\n",
            "Epoch: 380. Loss: 0.5444337867085622\n",
            "Epoch: 390. Loss: 0.5442535374556786\n",
            "Epoch: 400. Loss: 0.5440878426377684\n",
            "Epoch: 410. Loss: 0.543935282288175\n",
            "Epoch: 420. Loss: 0.5437946272920645\n",
            "Epoch: 430. Loss: 0.5436648024569277\n",
            "Epoch: 440. Loss: 0.5435448588453607\n",
            "Epoch: 450. Loss: 0.5434339527182246\n",
            "Epoch: 460. Loss: 0.5433313292413592\n",
            "Epoch: 470. Loss: 0.5432363096637611\n",
            "Epoch: 480. Loss: 0.5431482810579189\n",
            "Epoch: 490. Loss: 0.5430666879777899\n",
            "tensor(0.8210, dtype=torch.float64)\n",
            "2021-08-08 00:00:00\n",
            "Epoch: 0. Loss: 2.024837172357503\n",
            "Epoch: 10. Loss: 1.4547592558339197\n",
            "Epoch: 20. Loss: 1.174936563104959\n",
            "Epoch: 30. Loss: 1.0063487526022201\n",
            "Epoch: 40. Loss: 0.8826895402093545\n",
            "Epoch: 50. Loss: 0.7943901840525561\n",
            "Epoch: 60. Loss: 0.7381220826923527\n",
            "Epoch: 70. Loss: 0.7041878526357486\n",
            "Epoch: 80. Loss: 0.6822487561679476\n",
            "Epoch: 90. Loss: 0.6660727889948664\n",
            "Epoch: 100. Loss: 0.6527624381096714\n",
            "Epoch: 110. Loss: 0.6411028221579481\n",
            "Epoch: 120. Loss: 0.6305999078260943\n",
            "Epoch: 130. Loss: 0.6210470966658713\n",
            "Epoch: 140. Loss: 0.6123468494556719\n",
            "Epoch: 150. Loss: 0.604440443870833\n",
            "Epoch: 160. Loss: 0.5972812608391803\n",
            "Epoch: 170. Loss: 0.5908251356321254\n",
            "Epoch: 180. Loss: 0.5850272898393615\n",
            "Epoch: 190. Loss: 0.5798417654160137\n",
            "Epoch: 200. Loss: 0.5752217847156963\n",
            "Epoch: 210. Loss: 0.5711204217915667\n",
            "Epoch: 220. Loss: 0.5674913363003813\n",
            "Epoch: 230. Loss: 0.5642894622136774\n",
            "Epoch: 240. Loss: 0.5614716010452281\n",
            "Epoch: 250. Loss: 0.5589968963929586\n",
            "Epoch: 260. Loss: 0.5568271822433661\n",
            "Epoch: 270. Loss: 0.554927208046238\n",
            "Epoch: 280. Loss: 0.5532647509584142\n",
            "Epoch: 290. Loss: 0.551810630451198\n",
            "Epoch: 300. Loss: 0.550538642990066\n",
            "Epoch: 310. Loss: 0.5494254350950333\n",
            "Epoch: 320. Loss: 0.5484503322266224\n",
            "Epoch: 330. Loss: 0.5475951390933259\n",
            "Epoch: 340. Loss: 0.5468439245813149\n",
            "Epoch: 350. Loss: 0.5461828019252287\n",
            "Epoch: 360. Loss: 0.5455997122320067\n",
            "Epoch: 370. Loss: 0.5450842172055065\n",
            "Epoch: 380. Loss: 0.5446273049863246\n",
            "Epoch: 390. Loss: 0.5442212114473676\n",
            "Epoch: 400. Loss: 0.5438592580596082\n",
            "Epoch: 410. Loss: 0.5435357065285601\n",
            "Epoch: 420. Loss: 0.5432456297534448\n",
            "Epoch: 430. Loss: 0.542984798228218\n",
            "Epoch: 440. Loss: 0.5427495807395684\n",
            "Epoch: 450. Loss: 0.542536858080219\n",
            "Epoch: 460. Loss: 0.5423439484515243\n",
            "Epoch: 470. Loss: 0.5421685432493135\n",
            "Epoch: 480. Loss: 0.5420086519893025\n",
            "Epoch: 490. Loss: 0.5418625552165486\n",
            "tensor(0.7962, dtype=torch.float64)\n",
            "2021-08-15 00:00:00\n",
            "Epoch: 0. Loss: 3.952281119642989\n",
            "Epoch: 10. Loss: 0.9433516322982316\n",
            "Epoch: 20. Loss: 0.702199393655113\n",
            "Epoch: 30. Loss: 0.6615758891743846\n",
            "Epoch: 40. Loss: 0.6394533453651186\n",
            "Epoch: 50. Loss: 0.6228987925534917\n",
            "Epoch: 60. Loss: 0.6095145049211239\n",
            "Epoch: 70. Loss: 0.5984669318207295\n",
            "Epoch: 80. Loss: 0.5893093961439992\n",
            "Epoch: 90. Loss: 0.5817232197829489\n",
            "Epoch: 100. Loss: 0.5754471223817983\n",
            "Epoch: 110. Loss: 0.5702576359670092\n",
            "Epoch: 120. Loss: 0.5659633523717673\n",
            "Epoch: 130. Loss: 0.5624021566498424\n",
            "Epoch: 140. Loss: 0.5594385832942809\n",
            "Epoch: 150. Loss: 0.5569608205845249\n",
            "Epoch: 160. Loss: 0.5548775157972728\n",
            "Epoch: 170. Loss: 0.5531146219134457\n",
            "Epoch: 180. Loss: 0.5516124681838008\n",
            "Epoch: 190. Loss: 0.5503231575329373\n",
            "Epoch: 200. Loss: 0.5492083293993987\n",
            "Epoch: 210. Loss: 0.5482372835503233\n",
            "Epoch: 220. Loss: 0.5473854356262026\n",
            "Epoch: 230. Loss: 0.5466330635470781\n",
            "Epoch: 240. Loss: 0.5459643007716724\n",
            "Epoch: 250. Loss: 0.545366334197986\n",
            "Epoch: 260. Loss: 0.5448287687985597\n",
            "Epoch: 270. Loss: 0.5443431263587617\n",
            "Epoch: 280. Loss: 0.5439024510324848\n",
            "Epoch: 290. Loss: 0.5435009993706255\n",
            "Epoch: 300. Loss: 0.5431339968025906\n",
            "Epoch: 310. Loss: 0.542797446204242\n",
            "Epoch: 320. Loss: 0.5424879771960092\n",
            "Epoch: 330. Loss: 0.5422027272514156\n",
            "Epoch: 340. Loss: 0.5419392476423751\n",
            "Epoch: 350. Loss: 0.5416954287866408\n",
            "Epoch: 360. Loss: 0.5414694407708093\n",
            "Epoch: 370. Loss: 0.5412596857652402\n",
            "Epoch: 380. Loss: 0.5410647597803322\n",
            "Epoch: 390. Loss: 0.5408834217819201\n",
            "Epoch: 400. Loss: 0.5407145686233897\n",
            "Epoch: 410. Loss: 0.5405572145922103\n",
            "Epoch: 420. Loss: 0.5404104746315377\n",
            "Epoch: 430. Loss: 0.5402735505009958\n",
            "Epoch: 440. Loss: 0.5401457192983302\n",
            "Epoch: 450. Loss: 0.5400263238859214\n",
            "Epoch: 460. Loss: 0.5399147648612562\n",
            "Epoch: 470. Loss: 0.539810493784619\n",
            "Epoch: 480. Loss: 0.5397130074352826\n",
            "Epoch: 490. Loss: 0.5396218429130014\n",
            "tensor(0.8068, dtype=torch.float64)\n",
            "2021-08-22 00:00:00\n",
            "Epoch: 0. Loss: 6.346429451230293\n",
            "Epoch: 10. Loss: 1.438248091230998\n",
            "Epoch: 20. Loss: 0.633005306525739\n",
            "Epoch: 30. Loss: 0.5933777114165917\n",
            "Epoch: 40. Loss: 0.5769509277621749\n",
            "Epoch: 50. Loss: 0.5664150236186302\n",
            "Epoch: 60. Loss: 0.5590109985528144\n",
            "Epoch: 70. Loss: 0.5535551614339809\n",
            "Epoch: 80. Loss: 0.5494306759876532\n",
            "Epoch: 90. Loss: 0.5462727072749984\n",
            "Epoch: 100. Loss: 0.5438384171552529\n",
            "Epoch: 110. Loss: 0.541952922047485\n",
            "Epoch: 120. Loss: 0.540485271898147\n",
            "Epoch: 130. Loss: 0.53933599670942\n",
            "Epoch: 140. Loss: 0.5384293418985054\n",
            "Epoch: 150. Loss: 0.537707724268668\n",
            "Epoch: 160. Loss: 0.5371275048768022\n",
            "Epoch: 170. Loss: 0.5366556999241106\n",
            "Epoch: 180. Loss: 0.5362674218845437\n",
            "Epoch: 190. Loss: 0.5359439030009345\n",
            "Epoch: 200. Loss: 0.5356709816253107\n",
            "Epoch: 210. Loss: 0.5354379521035656\n",
            "Epoch: 220. Loss: 0.5352366966111048\n",
            "Epoch: 230. Loss: 0.5350610332207973\n",
            "Epoch: 240. Loss: 0.5349062282773647\n",
            "Epoch: 250. Loss: 0.5347686327041354\n",
            "Epoch: 260. Loss: 0.5346454112502035\n",
            "Epoch: 270. Loss: 0.5345343411266276\n",
            "Epoch: 280. Loss: 0.5344336622750703\n",
            "Epoch: 290. Loss: 0.5343419659632943\n",
            "Epoch: 300. Loss: 0.5342581117847041\n",
            "Epoch: 310. Loss: 0.5341811656890395\n",
            "Epoch: 320. Loss: 0.5341103535811801\n",
            "Epoch: 330. Loss: 0.5340450264483628\n",
            "Epoch: 340. Loss: 0.5339846340327585\n",
            "Epoch: 350. Loss: 0.5339287048483693\n",
            "Epoch: 360. Loss: 0.533876830918679\n",
            "Epoch: 370. Loss: 0.533828656037148\n",
            "Epoch: 380. Loss: 0.5337838666660505\n",
            "Epoch: 390. Loss: 0.5337421848197449\n",
            "Epoch: 400. Loss: 0.5337033624480673\n",
            "Epoch: 410. Loss: 0.5336671769603177\n",
            "Epoch: 420. Loss: 0.5336334276221497\n",
            "Epoch: 430. Loss: 0.5336019326253497\n",
            "Epoch: 440. Loss: 0.5335725266804258\n",
            "Epoch: 450. Loss: 0.5335450590188582\n",
            "Epoch: 460. Loss: 0.5335193917192129\n",
            "Epoch: 470. Loss: 0.5334953982916788\n",
            "Epoch: 480. Loss: 0.5334729624707576\n",
            "Epoch: 490. Loss: 0.533451977177205\n",
            "tensor(0.8236, dtype=torch.float64)\n",
            "2021-08-29 00:00:00\n",
            "Epoch: 0. Loss: 2.581353253326043\n",
            "Epoch: 10. Loss: 0.7527980427091965\n",
            "Epoch: 20. Loss: 0.6405887915521654\n",
            "Epoch: 30. Loss: 0.6072435918069304\n",
            "Epoch: 40. Loss: 0.5889530805495073\n",
            "Epoch: 50. Loss: 0.5775372101256556\n",
            "Epoch: 60. Loss: 0.5694983913070484\n",
            "Epoch: 70. Loss: 0.5633089543255717\n",
            "Epoch: 80. Loss: 0.5583018331038818\n",
            "Epoch: 90. Loss: 0.554151870283309\n",
            "Epoch: 100. Loss: 0.5506711160201122\n",
            "Epoch: 110. Loss: 0.5477326688105415\n",
            "Epoch: 120. Loss: 0.5452417100960729\n",
            "Epoch: 130. Loss: 0.5431234652013248\n",
            "Epoch: 140. Loss: 0.5413173943370649\n",
            "Epoch: 150. Loss: 0.5397738432747271\n",
            "Epoch: 160. Loss: 0.538451793227678\n",
            "Epoch: 170. Loss: 0.5373171944417614\n",
            "Epoch: 180. Loss: 0.5363416691158406\n",
            "Epoch: 190. Loss: 0.5355014793786256\n",
            "Epoch: 200. Loss: 0.5347766986655256\n",
            "Epoch: 210. Loss: 0.5341505435876386\n",
            "Epoch: 220. Loss: 0.5336088335680691\n",
            "Epoch: 230. Loss: 0.5331395523765713\n",
            "Epoch: 240. Loss: 0.5327324909523156\n",
            "Epoch: 250. Loss: 0.5323789551349043\n",
            "Epoch: 260. Loss: 0.5320715253445473\n",
            "Epoch: 270. Loss: 0.5318038579925786\n",
            "Epoch: 280. Loss: 0.5315705205730937\n",
            "Epoch: 290. Loss: 0.5313668540878079\n",
            "Epoch: 300. Loss: 0.5311888577819146\n",
            "Epoch: 310. Loss: 0.5310330921986456\n",
            "Epoch: 320. Loss: 0.5308965973603617\n",
            "Epoch: 330. Loss: 0.53077682350714\n",
            "Epoch: 340. Loss: 0.5306715723112838\n",
            "Epoch: 350. Loss: 0.5305789468697553\n",
            "Epoch: 360. Loss: 0.5304973090804335\n",
            "Epoch: 370. Loss: 0.5304252432506905\n",
            "Epoch: 380. Loss: 0.5303615249818879\n",
            "Epoch: 390. Loss: 0.5303050945315345\n",
            "Epoch: 400. Loss: 0.5302550339839034\n",
            "Epoch: 410. Loss: 0.5302105476659811\n",
            "Epoch: 420. Loss: 0.5301709453333004\n",
            "Epoch: 430. Loss: 0.530135627723111\n",
            "Epoch: 440. Loss: 0.530104074133229\n",
            "Epoch: 450. Loss: 0.5300758317359877\n",
            "Epoch: 460. Loss: 0.5300505063797148\n",
            "Epoch: 470. Loss: 0.5300277546664838\n",
            "Epoch: 480. Loss: 0.53000727712564\n",
            "Epoch: 490. Loss: 0.5299888123287168\n",
            "tensor(0.8801, dtype=torch.float64)\n",
            "2021-09-05 00:00:00\n",
            "Epoch: 0. Loss: 0.9521730995401412\n",
            "Epoch: 10. Loss: 0.794248665465483\n",
            "Epoch: 20. Loss: 0.7008608372948087\n",
            "Epoch: 30. Loss: 0.6342713600006575\n",
            "Epoch: 40. Loss: 0.5906550134481547\n",
            "Epoch: 50. Loss: 0.5655206948593726\n",
            "Epoch: 60. Loss: 0.5524130611513254\n",
            "Epoch: 70. Loss: 0.5457260965089624\n",
            "Epoch: 80. Loss: 0.5420543001353995\n",
            "Epoch: 90. Loss: 0.5397378473757388\n",
            "Epoch: 100. Loss: 0.5380413904455869\n",
            "Epoch: 110. Loss: 0.5366471320410166\n",
            "Epoch: 120. Loss: 0.5354171333814369\n",
            "Epoch: 130. Loss: 0.5342903584827101\n",
            "Epoch: 140. Loss: 0.5332388511374618\n",
            "Epoch: 150. Loss: 0.5322490315119567\n",
            "Epoch: 160. Loss: 0.5313136418680199\n",
            "Epoch: 170. Loss: 0.5304282443945544\n",
            "Epoch: 180. Loss: 0.5295896859527102\n",
            "Epoch: 190. Loss: 0.5287954210726\n",
            "Epoch: 200. Loss: 0.5280432122653882\n",
            "Epoch: 210. Loss: 0.52733099703891\n",
            "Epoch: 220. Loss: 0.5266568287525938\n",
            "Epoch: 230. Loss: 0.5260188501832694\n",
            "Epoch: 240. Loss: 0.5254152815392396\n",
            "Epoch: 250. Loss: 0.5248444148026027\n",
            "Epoch: 260. Loss: 0.52430461078836\n",
            "Epoch: 270. Loss: 0.5237942973159622\n",
            "Epoch: 280. Loss: 0.5233119677840097\n",
            "Epoch: 290. Loss: 0.5228561798390049\n",
            "Epoch: 300. Loss: 0.5224255540086834\n",
            "Epoch: 310. Loss: 0.5220187722514534\n",
            "Epoch: 320. Loss: 0.5216345764101059\n",
            "Epoch: 330. Loss: 0.5212717665744246\n",
            "Epoch: 340. Loss: 0.5209291993644783\n",
            "Epoch: 350. Loss: 0.5206057861491412\n",
            "Epoch: 360. Loss: 0.5203004912150263\n",
            "Epoch: 370. Loss: 0.5200123299006135\n",
            "Epoch: 380. Loss: 0.519740366709427\n",
            "Epoch: 390. Loss: 0.5194837134149377\n",
            "Epoch: 400. Loss: 0.5192415271685874\n",
            "Epoch: 410. Loss: 0.5190130086210273\n",
            "Epoch: 420. Loss: 0.5187974000653908\n",
            "Epoch: 430. Loss: 0.5185939836101962\n",
            "Epoch: 440. Loss: 0.5184020793883373\n",
            "Epoch: 450. Loss: 0.5182210438075509\n",
            "Epoch: 460. Loss: 0.5180502678467809\n",
            "Epoch: 470. Loss: 0.5178891754019765\n",
            "Epoch: 480. Loss: 0.5177372216840574\n",
            "Epoch: 490. Loss: 0.5175938916710798\n",
            "tensor(0.8168, dtype=torch.float64)\n",
            "2021-09-12 00:00:00\n",
            "Epoch: 0. Loss: 1.301738238205332\n",
            "Epoch: 10. Loss: 0.7232007158570416\n",
            "Epoch: 20. Loss: 0.6188793785962654\n",
            "Epoch: 30. Loss: 0.5826755465838331\n",
            "Epoch: 40. Loss: 0.5685761672317293\n",
            "Epoch: 50. Loss: 0.5622509177542974\n",
            "Epoch: 60. Loss: 0.5585677549344858\n",
            "Epoch: 70. Loss: 0.5558362831642564\n",
            "Epoch: 80. Loss: 0.5535207740149352\n",
            "Epoch: 90. Loss: 0.5514424333597686\n",
            "Epoch: 100. Loss: 0.549532482191886\n",
            "Epoch: 110. Loss: 0.5477579901746972\n",
            "Epoch: 120. Loss: 0.5460992019368\n",
            "Epoch: 130. Loss: 0.5445422222922729\n",
            "Epoch: 140. Loss: 0.5430763890920574\n",
            "Epoch: 150. Loss: 0.541693148466587\n",
            "Epoch: 160. Loss: 0.5403854603291182\n",
            "Epoch: 170. Loss: 0.5391474242183354\n",
            "Epoch: 180. Loss: 0.5379740177590147\n",
            "Epoch: 190. Loss: 0.5368609042091415\n",
            "Epoch: 200. Loss: 0.535804287570943\n",
            "Epoch: 210. Loss: 0.5348008024024556\n",
            "Epoch: 220. Loss: 0.5338474296019693\n",
            "Epoch: 230. Loss: 0.5329414318383725\n",
            "Epoch: 240. Loss: 0.5320803039018736\n",
            "Epoch: 250. Loss: 0.5312617344020106\n",
            "Epoch: 260. Loss: 0.5304835760989562\n",
            "Epoch: 270. Loss: 0.5297438228039631\n",
            "Epoch: 280. Loss: 0.5290405912791595\n",
            "Epoch: 290. Loss: 0.5283721069434536\n",
            "Epoch: 300. Loss: 0.5277366924782555\n",
            "Epoch: 310. Loss: 0.5271327586450807\n",
            "Epoch: 320. Loss: 0.5265587967932333\n",
            "Epoch: 330. Loss: 0.5260133726619596\n",
            "Epoch: 340. Loss: 0.5254951211772723\n",
            "Epoch: 350. Loss: 0.525002742016314\n",
            "Epoch: 360. Loss: 0.5245349957671892\n",
            "Epoch: 370. Loss: 0.5240907005539103\n",
            "Epoch: 380. Loss: 0.5236687290276566\n",
            "Epoch: 390. Loss: 0.523268005649433\n",
            "Epoch: 400. Loss: 0.5228875042072519\n",
            "Epoch: 410. Loss: 0.5225262455246196\n",
            "Epoch: 420. Loss: 0.5221832953273954\n",
            "Epoch: 430. Loss: 0.5218577622438871\n",
            "Epoch: 440. Loss: 0.5215487959189105\n",
            "Epoch: 450. Loss: 0.5212555852269729\n",
            "Epoch: 460. Loss: 0.5209773565730919\n",
            "Epoch: 470. Loss: 0.5207133722722715\n",
            "Epoch: 480. Loss: 0.5204629290005693\n",
            "Epoch: 490. Loss: 0.5202253563121121\n",
            "tensor(0.7824, dtype=torch.float64)\n",
            "2021-09-19 00:00:00\n",
            "Epoch: 0. Loss: 0.7084901981248966\n",
            "Epoch: 10. Loss: 0.6157332460877526\n",
            "Epoch: 20. Loss: 0.5867276129761958\n",
            "Epoch: 30. Loss: 0.5715576905499165\n",
            "Epoch: 40. Loss: 0.560397068397616\n",
            "Epoch: 50. Loss: 0.5516863696038032\n",
            "Epoch: 60. Loss: 0.5448485216385683\n",
            "Epoch: 70. Loss: 0.5394937627236269\n",
            "Epoch: 80. Loss: 0.5353139938367881\n",
            "Epoch: 90. Loss: 0.5320576425625547\n",
            "Epoch: 100. Loss: 0.5295200112426465\n",
            "Epoch: 110. Loss: 0.5275368876804726\n",
            "Epoch: 120. Loss: 0.5259785922883761\n",
            "Epoch: 130. Loss: 0.5247441554334242\n",
            "Epoch: 140. Loss: 0.5237558831741553\n",
            "Epoch: 150. Loss: 0.5229545542924847\n",
            "Epoch: 160. Loss: 0.5222953594692071\n",
            "Epoch: 170. Loss: 0.5217445818059248\n",
            "Epoch: 180. Loss: 0.521276951365656\n",
            "Epoch: 190. Loss: 0.5208735769897633\n",
            "Epoch: 200. Loss: 0.520520352989667\n",
            "Epoch: 210. Loss: 0.5202067455185017\n",
            "Epoch: 220. Loss: 0.5199248762850591\n",
            "Epoch: 230. Loss: 0.5196688355552548\n",
            "Epoch: 240. Loss: 0.5194341698950026\n",
            "Epoch: 250. Loss: 0.5192175018793007\n",
            "Epoch: 260. Loss: 0.5190162487557577\n",
            "Epoch: 270. Loss: 0.5188284148905519\n",
            "Epoch: 280. Loss: 0.5186524389792312\n",
            "Epoch: 290. Loss: 0.5184870817576338\n",
            "Epoch: 300. Loss: 0.5183313435741563\n",
            "Epoch: 310. Loss: 0.5181844039251524\n",
            "Epoch: 320. Loss: 0.5180455771116345\n",
            "Epoch: 330. Loss: 0.5179142797096666\n",
            "Epoch: 340. Loss: 0.5177900066861942\n",
            "Epoch: 350. Loss: 0.5176723138350349\n",
            "Epoch: 360. Loss: 0.5175608048295164\n",
            "Epoch: 370. Loss: 0.5174551216456634\n",
            "Epoch: 380. Loss: 0.5173549374456247\n",
            "Epoch: 390. Loss: 0.5172599512570651\n",
            "Epoch: 400. Loss: 0.5171698839642423\n",
            "Epoch: 410. Loss: 0.5170844752579791\n",
            "Epoch: 420. Loss: 0.5170034812876916\n",
            "Epoch: 430. Loss: 0.5169266728285666\n",
            "Epoch: 440. Loss: 0.5168538338279133\n",
            "Epoch: 450. Loss: 0.5167847602317717\n",
            "Epoch: 460. Loss: 0.5167192590198088\n",
            "Epoch: 470. Loss: 0.5166571473961084\n",
            "Epoch: 480. Loss: 0.516598252097691\n",
            "Epoch: 490. Loss: 0.5165424087929185\n",
            "tensor(0.7710, dtype=torch.float64)\n",
            "2021-09-26 00:00:00\n",
            "Epoch: 0. Loss: 1.2164158166832586\n",
            "Epoch: 10. Loss: 0.9706906525869263\n",
            "Epoch: 20. Loss: 0.8384722516113292\n",
            "Epoch: 30. Loss: 0.7401114629768788\n",
            "Epoch: 40. Loss: 0.6669779135329142\n",
            "Epoch: 50. Loss: 0.6180601433081012\n",
            "Epoch: 60. Loss: 0.588820471127841\n",
            "Epoch: 70. Loss: 0.5721869231378333\n",
            "Epoch: 80. Loss: 0.5623667895068607\n",
            "Epoch: 90. Loss: 0.5560274504264091\n",
            "Epoch: 100. Loss: 0.5515237777354867\n",
            "Epoch: 110. Loss: 0.5480646311963838\n",
            "Epoch: 120. Loss: 0.5452552024941052\n",
            "Epoch: 130. Loss: 0.5428853121820107\n",
            "Epoch: 140. Loss: 0.5408342501238722\n",
            "Epoch: 150. Loss: 0.5390271132463946\n",
            "Epoch: 160. Loss: 0.537414004190858\n",
            "Epoch: 170. Loss: 0.535959656658068\n",
            "Epoch: 180. Loss: 0.534637977751369\n",
            "Epoch: 190. Loss: 0.5334289971470013\n",
            "Epoch: 200. Loss: 0.5323170466048144\n",
            "Epoch: 210. Loss: 0.5312896017846743\n",
            "Epoch: 220. Loss: 0.5303365024657938\n",
            "Epoch: 230. Loss: 0.5294494030662767\n",
            "Epoch: 240. Loss: 0.5286213720409022\n",
            "Epoch: 250. Loss: 0.5278465926294941\n",
            "Epoch: 260. Loss: 0.527120135445813\n",
            "Epoch: 270. Loss: 0.5264377835136886\n",
            "Epoch: 280. Loss: 0.5257958963933486\n",
            "Epoch: 290. Loss: 0.5251913038655681\n",
            "Epoch: 300. Loss: 0.5246212221963784\n",
            "Epoch: 310. Loss: 0.5240831877852857\n",
            "Epoch: 320. Loss: 0.5235750042792247\n",
            "Epoch: 330. Loss: 0.5230947001736018\n",
            "Epoch: 340. Loss: 0.5226404946213361\n",
            "Epoch: 350. Loss: 0.522210769697055\n",
            "Epoch: 360. Loss: 0.52180404776222\n",
            "Epoch: 370. Loss: 0.5214189728804766\n",
            "Epoch: 380. Loss: 0.5210542954646067\n",
            "Epoch: 390. Loss: 0.5207088595145913\n",
            "Epoch: 400. Loss: 0.5203815919435031\n",
            "Epoch: 410. Loss: 0.5200714935940305\n",
            "Epoch: 420. Loss: 0.5197776316307483\n",
            "Epoch: 430. Loss: 0.5194991330573725\n",
            "Epoch: 440. Loss: 0.5192351791584058\n",
            "Epoch: 450. Loss: 0.5189850007039615\n",
            "Epoch: 460. Loss: 0.5187478737876615\n",
            "Epoch: 470. Loss: 0.5185231161921137\n",
            "Epoch: 480. Loss: 0.5183100841960956\n",
            "Epoch: 490. Loss: 0.5181081697532455\n",
            "tensor(0.8657, dtype=torch.float64)\n",
            "2021-10-03 00:00:00\n",
            "Epoch: 0. Loss: 3.9398317861534533\n",
            "Epoch: 10. Loss: 0.6559450573245836\n",
            "Epoch: 20. Loss: 0.5889820434596604\n",
            "Epoch: 30. Loss: 0.5747782539065962\n",
            "Epoch: 40. Loss: 0.5638792167331051\n",
            "Epoch: 50. Loss: 0.5547878559944922\n",
            "Epoch: 60. Loss: 0.5471860284861924\n",
            "Epoch: 70. Loss: 0.540835974368847\n",
            "Epoch: 80. Loss: 0.5355339495755937\n",
            "Epoch: 90. Loss: 0.5311060589252348\n",
            "Epoch: 100. Loss: 0.5274053674473322\n",
            "Epoch: 110. Loss: 0.5243087466711437\n",
            "Epoch: 120. Loss: 0.5217136434873161\n",
            "Epoch: 130. Loss: 0.5195349914192439\n",
            "Epoch: 140. Loss: 0.517702403750324\n",
            "Epoch: 150. Loss: 0.5161577115418929\n",
            "Epoch: 160. Loss: 0.5148528564719699\n",
            "Epoch: 170. Loss: 0.5137481174790055\n",
            "Epoch: 180. Loss: 0.5128106352499614\n",
            "Epoch: 190. Loss: 0.5120131937109131\n",
            "Epoch: 200. Loss: 0.511333218475716\n",
            "Epoch: 210. Loss: 0.5107519558207163\n",
            "Epoch: 220. Loss: 0.510253800461111\n",
            "Epoch: 230. Loss: 0.5098257452518221\n",
            "Epoch: 240. Loss: 0.5094569304421439\n",
            "Epoch: 250. Loss: 0.5091382740766445\n",
            "Epoch: 260. Loss: 0.5088621685057209\n",
            "Epoch: 270. Loss: 0.5086222307763879\n",
            "Epoch: 280. Loss: 0.5084130969800423\n",
            "Epoch: 290. Loss: 0.5082302525121903\n",
            "Epoch: 300. Loss: 0.5080698917207812\n",
            "Epoch: 310. Loss: 0.5079288016489347\n",
            "Epoch: 320. Loss: 0.5078042655694405\n",
            "Epoch: 330. Loss: 0.50769398280828\n",
            "Epoch: 340. Loss: 0.5075960020001204\n",
            "Epoch: 350. Loss: 0.5075086654406594\n",
            "Epoch: 360. Loss: 0.5074305626233118\n",
            "Epoch: 370. Loss: 0.5073604913905957\n",
            "Epoch: 380. Loss: 0.5072974254093434\n",
            "Epoch: 390. Loss: 0.5072404869059997\n",
            "Epoch: 400. Loss: 0.5071889237837751\n",
            "Epoch: 410. Loss: 0.5071420903952365\n",
            "Epoch: 420. Loss: 0.5070994313684383\n",
            "Epoch: 430. Loss: 0.5070604679870523\n",
            "Epoch: 440. Loss: 0.5070247867092489\n",
            "Epoch: 450. Loss: 0.506992029479635\n",
            "Epoch: 460. Loss: 0.5069618855460586\n",
            "Epoch: 470. Loss: 0.5069340845407152\n",
            "Epoch: 480. Loss: 0.5069083906244816\n",
            "Epoch: 490. Loss: 0.5068845975262273\n",
            "tensor(0.9257, dtype=torch.float64)\n",
            "2021-10-10 00:00:00\n",
            "Epoch: 0. Loss: 3.6734111688396807\n",
            "Epoch: 10. Loss: 1.3443565406776345\n",
            "Epoch: 20. Loss: 0.8864269943895556\n",
            "Epoch: 30. Loss: 0.7333955462817465\n",
            "Epoch: 40. Loss: 0.6584714718989242\n",
            "Epoch: 50. Loss: 0.6240738935518811\n",
            "Epoch: 60. Loss: 0.6080959164470057\n",
            "Epoch: 70. Loss: 0.5989802018607929\n",
            "Epoch: 80. Loss: 0.5923544088533942\n",
            "Epoch: 90. Loss: 0.5868161961935319\n",
            "Epoch: 100. Loss: 0.5819107871167648\n",
            "Epoch: 110. Loss: 0.5774653890331235\n",
            "Epoch: 120. Loss: 0.5733950193379296\n",
            "Epoch: 130. Loss: 0.5696460608865421\n",
            "Epoch: 140. Loss: 0.5661788449722657\n",
            "Epoch: 150. Loss: 0.562961592990412\n",
            "Epoch: 160. Loss: 0.5599678842407243\n",
            "Epoch: 170. Loss: 0.5571753412000617\n",
            "Epoch: 180. Loss: 0.5545648043407254\n",
            "Epoch: 190. Loss: 0.5521197437912305\n",
            "Epoch: 200. Loss: 0.5498258084062724\n",
            "Epoch: 210. Loss: 0.5476704667933785\n",
            "Epoch: 220. Loss: 0.5456427158963623\n",
            "Epoch: 230. Loss: 0.5437328419567015\n",
            "Epoch: 240. Loss: 0.5419322232805247\n",
            "Epoch: 250. Loss: 0.5402331668840437\n",
            "Epoch: 260. Loss: 0.538628772814014\n",
            "Epoch: 270. Loss: 0.5371128211759907\n",
            "Epoch: 280. Loss: 0.5356796778454543\n",
            "Epoch: 290. Loss: 0.5343242155805756\n",
            "Epoch: 300. Loss: 0.5330417478529588\n",
            "Epoch: 310. Loss: 0.5318279731971419\n",
            "Epoch: 320. Loss: 0.5306789282740572\n",
            "Epoch: 330. Loss: 0.5295909481654669\n",
            "Epoch: 340. Loss: 0.5285606326792933\n",
            "Epoch: 350. Loss: 0.5275848176606965\n",
            "Epoch: 360. Loss: 0.5266605504796953\n",
            "Epoch: 370. Loss: 0.5257850690102308\n",
            "Epoch: 380. Loss: 0.5249557835337736\n",
            "Epoch: 390. Loss: 0.5241702610976493\n",
            "Epoch: 400. Loss: 0.5234262119380586\n",
            "Epoch: 410. Loss: 0.5227214776435203\n",
            "Epoch: 420. Loss: 0.5220540207886694\n",
            "Epoch: 430. Loss: 0.5214219158131373\n",
            "Epoch: 440. Loss: 0.5208233409573075\n",
            "Epoch: 450. Loss: 0.5202565710974504\n",
            "Epoch: 460. Loss: 0.5197199713482499\n",
            "Epoch: 470. Loss: 0.5192119913219385\n",
            "Epoch: 480. Loss: 0.5187311599509156\n",
            "Epoch: 490. Loss: 0.5182760807954647\n",
            "tensor(0.7153, dtype=torch.float64)\n",
            "2021-10-17 00:00:00\n",
            "Epoch: 0. Loss: 0.6321059607731896\n",
            "Epoch: 10. Loss: 0.5945913110114824\n",
            "Epoch: 20. Loss: 0.5820862474186008\n",
            "Epoch: 30. Loss: 0.5734208261454756\n",
            "Epoch: 40. Loss: 0.5663401954615996\n",
            "Epoch: 50. Loss: 0.5602290219951146\n",
            "Epoch: 60. Loss: 0.5548505052503026\n",
            "Epoch: 70. Loss: 0.5500829754731734\n",
            "Epoch: 80. Loss: 0.5458444076300316\n",
            "Epoch: 90. Loss: 0.5420699087005842\n",
            "Epoch: 100. Loss: 0.5387045747750588\n",
            "Epoch: 110. Loss: 0.5357007959385418\n",
            "Epoch: 120. Loss: 0.5330169017311411\n",
            "Epoch: 130. Loss: 0.5306162686045007\n",
            "Epoch: 140. Loss: 0.5284666333719011\n",
            "Epoch: 150. Loss: 0.5265395296156019\n",
            "Epoch: 160. Loss: 0.5248098130617956\n",
            "Epoch: 170. Loss: 0.5232552570306126\n",
            "Epoch: 180. Loss: 0.5218562047955547\n",
            "Epoch: 190. Loss: 0.5205952687068509\n",
            "Epoch: 200. Loss: 0.5194570680328526\n",
            "Epoch: 210. Loss: 0.5184279991243712\n",
            "Epoch: 220. Loss: 0.5174960328379612\n",
            "Epoch: 230. Loss: 0.5166505352152794\n",
            "Epoch: 240. Loss: 0.5158821082438075\n",
            "Epoch: 250. Loss: 0.5151824481570266\n",
            "Epoch: 260. Loss: 0.5145442192073125\n",
            "Epoch: 270. Loss: 0.5139609411975965\n",
            "Epoch: 280. Loss: 0.5134268893188534\n",
            "Epoch: 290. Loss: 0.5129370050349252\n",
            "Epoch: 300. Loss: 0.5124868169037529\n",
            "Epoch: 310. Loss: 0.5120723703395488\n",
            "Epoch: 320. Loss: 0.511690165414389\n",
            "Epoch: 330. Loss: 0.5113371018774548\n",
            "Epoch: 340. Loss: 0.5110104306405485\n",
            "Epoch: 350. Loss: 0.5107077110425488\n",
            "Epoch: 360. Loss: 0.510426773264946\n",
            "Epoch: 370. Loss: 0.5101656853264563\n",
            "Epoch: 380. Loss: 0.5099227241373742\n",
            "Epoch: 390. Loss: 0.5096963501439619\n",
            "Epoch: 400. Loss: 0.5094851851397505\n",
            "Epoch: 410. Loss: 0.5092879928641214\n",
            "Epoch: 420. Loss: 0.5091036620488761\n",
            "Epoch: 430. Loss: 0.5089311916106755\n",
            "Epoch: 440. Loss: 0.5087696777212674\n",
            "Epoch: 450. Loss: 0.5086183025183898\n",
            "Epoch: 460. Loss: 0.5084763242482477\n",
            "Epoch: 470. Loss: 0.5083430686556695\n",
            "Epoch: 480. Loss: 0.5082179214606116\n",
            "Epoch: 490. Loss: 0.5081003217797956\n",
            "tensor(0.7543, dtype=torch.float64)\n",
            "2021-10-24 00:00:00\n",
            "Epoch: 0. Loss: 4.362146477065012\n",
            "Epoch: 10. Loss: 1.1561799641234316\n",
            "Epoch: 20. Loss: 0.8218142721085419\n",
            "Epoch: 30. Loss: 0.7445775383930603\n",
            "Epoch: 40. Loss: 0.7019826119454713\n",
            "Epoch: 50. Loss: 0.6743920537224734\n",
            "Epoch: 60. Loss: 0.6554350405012238\n",
            "Epoch: 70. Loss: 0.6414529075721426\n",
            "Epoch: 80. Loss: 0.6303750857492493\n",
            "Epoch: 90. Loss: 0.6210968215942866\n",
            "Epoch: 100. Loss: 0.6130352373577324\n",
            "Epoch: 110. Loss: 0.6058722962321034\n",
            "Epoch: 120. Loss: 0.5994219595379952\n",
            "Epoch: 130. Loss: 0.5935647975283846\n",
            "Epoch: 140. Loss: 0.5882163951495883\n",
            "Epoch: 150. Loss: 0.5833121132898559\n",
            "Epoch: 160. Loss: 0.5787996281225496\n",
            "Epoch: 170. Loss: 0.5746351321289853\n",
            "Epoch: 180. Loss: 0.5707812531667733\n",
            "Epoch: 190. Loss: 0.5672057871440377\n",
            "Epoch: 200. Loss: 0.5638808293322142\n",
            "Epoch: 210. Loss: 0.5607821160878401\n",
            "Epoch: 220. Loss: 0.5578884917571927\n",
            "Epoch: 230. Loss: 0.5551814612794802\n",
            "Epoch: 240. Loss: 0.5526448088138353\n",
            "Epoch: 250. Loss: 0.5502642710977532\n",
            "Epoch: 260. Loss: 0.5480272577704833\n",
            "Epoch: 270. Loss: 0.5459226124569966\n",
            "Epoch: 280. Loss: 0.5439404092318552\n",
            "Epoch: 290. Loss: 0.5420717796567268\n",
            "Epoch: 300. Loss: 0.5403087660866905\n",
            "Epoch: 310. Loss: 0.5386441974194073\n",
            "Epoch: 320. Loss: 0.5370715839230454\n",
            "Epoch: 330. Loss: 0.5355850282158717\n",
            "Epoch: 340. Loss: 0.534179149874327\n",
            "Epoch: 350. Loss: 0.5328490215117155\n",
            "Epoch: 360. Loss: 0.531590114494039\n",
            "Epoch: 370. Loss: 0.5303982527434247\n",
            "Epoch: 380. Loss: 0.5292695733250611\n",
            "Epoch: 390. Loss: 0.5282004927238245\n",
            "Epoch: 400. Loss: 0.5271876778954359\n",
            "Epoch: 410. Loss: 0.5262280213278913\n",
            "Epoch: 420. Loss: 0.5253186194757014\n",
            "Epoch: 430. Loss: 0.5244567540356005\n",
            "Epoch: 440. Loss: 0.5236398756209177\n",
            "Epoch: 450. Loss: 0.5228655894655175\n",
            "Epoch: 460. Loss: 0.5221316428494416\n",
            "Epoch: 470. Loss: 0.5214359139892245\n",
            "Epoch: 480. Loss: 0.5207764021780105\n",
            "Epoch: 490. Loss: 0.5201512189955696\n",
            "tensor(0.8696, dtype=torch.float64)\n",
            "2021-10-31 00:00:00\n",
            "Epoch: 0. Loss: 1.4475089477027923\n",
            "Epoch: 10. Loss: 0.9294284652710558\n",
            "Epoch: 20. Loss: 0.8105093230965625\n",
            "Epoch: 30. Loss: 0.7504261974099861\n",
            "Epoch: 40. Loss: 0.7102245394117358\n",
            "Epoch: 50. Loss: 0.6813709292135629\n",
            "Epoch: 60. Loss: 0.6593326238266909\n",
            "Epoch: 70. Loss: 0.6413470228855582\n",
            "Epoch: 80. Loss: 0.6259541296209138\n",
            "Epoch: 90. Loss: 0.6124443734988522\n",
            "Epoch: 100. Loss: 0.6004585892634722\n",
            "Epoch: 110. Loss: 0.5897839804360603\n",
            "Epoch: 120. Loss: 0.5802677353824547\n",
            "Epoch: 130. Loss: 0.5717842860853025\n",
            "Epoch: 140. Loss: 0.5642237726553044\n",
            "Epoch: 150. Loss: 0.5574880649976196\n",
            "Epoch: 160. Loss: 0.5514891879046531\n",
            "Epoch: 170. Loss: 0.5461483833210957\n",
            "Epoch: 180. Loss: 0.5413952847253021\n",
            "Epoch: 190. Loss: 0.5371670945515787\n",
            "Epoch: 200. Loss: 0.5334077749387507\n",
            "Epoch: 210. Loss: 0.530067282847026\n",
            "Epoch: 220. Loss: 0.5271008730963895\n",
            "Epoch: 230. Loss: 0.5244684806252103\n",
            "Epoch: 240. Loss: 0.522134183176702\n",
            "Epoch: 250. Loss: 0.5200657390275205\n",
            "Epoch: 260. Loss: 0.5182341908698905\n",
            "Epoch: 270. Loss: 0.5166135257243103\n",
            "Epoch: 280. Loss: 0.5151803809984421\n",
            "Epoch: 290. Loss: 0.5139137878798107\n",
            "Epoch: 300. Loss: 0.5127949446919159\n",
            "Epoch: 310. Loss: 0.5118070143479521\n",
            "Epoch: 320. Loss: 0.5109349414206314\n",
            "Epoch: 330. Loss: 0.5101652855190364\n",
            "Epoch: 340. Loss: 0.5094860685945909\n",
            "Epoch: 350. Loss: 0.5088866344972052\n",
            "Epoch: 360. Loss: 0.5083575195992788\n",
            "Epoch: 370. Loss: 0.5078903336379302\n",
            "Epoch: 380. Loss: 0.5074776501335371\n",
            "Epoch: 390. Loss: 0.5071129058606529\n",
            "Epoch: 400. Loss: 0.5067903089049101\n",
            "Epoch: 410. Loss: 0.5065047548595479\n",
            "Epoch: 420. Loss: 0.5062517507146419\n",
            "Epoch: 430. Loss: 0.5060273459827392\n",
            "Epoch: 440. Loss: 0.5058280705940816\n",
            "Epoch: 450. Loss: 0.5056508790874639\n",
            "Epoch: 460. Loss: 0.5054931006213116\n",
            "Epoch: 470. Loss: 0.505352394334515\n",
            "Epoch: 480. Loss: 0.505226709597705\n",
            "Epoch: 490. Loss: 0.5051142507122093\n",
            "tensor(0.9007, dtype=torch.float64)\n",
            "2021-11-07 00:00:00\n",
            "Epoch: 0. Loss: 5.391773085867359\n",
            "Epoch: 10. Loss: 1.2453436408394707\n",
            "Epoch: 20. Loss: 0.7721409731838145\n",
            "Epoch: 30. Loss: 0.6855043615666835\n",
            "Epoch: 40. Loss: 0.638382485694162\n",
            "Epoch: 50. Loss: 0.6062974716873288\n",
            "Epoch: 60. Loss: 0.5838993419662818\n",
            "Epoch: 70. Loss: 0.5683141925195874\n",
            "Epoch: 80. Loss: 0.5573696560568209\n",
            "Epoch: 90. Loss: 0.5494593089285901\n",
            "Epoch: 100. Loss: 0.5434855784946693\n",
            "Epoch: 110. Loss: 0.5387475928175054\n",
            "Epoch: 120. Loss: 0.5348175532750955\n",
            "Epoch: 130. Loss: 0.5314397654094904\n",
            "Epoch: 140. Loss: 0.5284609545097596\n",
            "Epoch: 150. Loss: 0.5257868838299531\n",
            "Epoch: 160. Loss: 0.5233570205029988\n",
            "Epoch: 170. Loss: 0.5211302825431997\n",
            "Epoch: 180. Loss: 0.5190771533426203\n",
            "Epoch: 190. Loss: 0.5171753134359048\n",
            "Epoch: 200. Loss: 0.5154071752825571\n",
            "Epoch: 210. Loss: 0.5137584423906248\n",
            "Epoch: 220. Loss: 0.5122172252557246\n",
            "Epoch: 230. Loss: 0.5107734676829951\n",
            "Epoch: 240. Loss: 0.5094185531855225\n",
            "Epoch: 250. Loss: 0.5081450214111005\n",
            "Epoch: 260. Loss: 0.5069463557642823\n",
            "Epoch: 270. Loss: 0.5058168197135983\n",
            "Epoch: 280. Loss: 0.5047513279998074\n",
            "Epoch: 290. Loss: 0.503745343796522\n",
            "Epoch: 300. Loss: 0.5027947956885696\n",
            "Epoch: 310. Loss: 0.5018960100683117\n",
            "Epoch: 320. Loss: 0.501045655684822\n",
            "Epoch: 330. Loss: 0.5002406978635434\n",
            "Epoch: 340. Loss: 0.4994783604775263\n",
            "Epoch: 350. Loss: 0.4987560941699256\n",
            "Epoch: 360. Loss: 0.4980715496451886\n",
            "Epoch: 370. Loss: 0.4974225550910963\n",
            "Epoch: 380. Loss: 0.49680709698414044\n",
            "Epoch: 390. Loss: 0.4962233036797281\n",
            "Epoch: 400. Loss: 0.49566943130594\n",
            "Epoch: 410. Loss: 0.49514385157218854\n",
            "Epoch: 420. Loss: 0.4946450411775563\n",
            "Epoch: 430. Loss: 0.49417157256202493\n",
            "Epoch: 440. Loss: 0.4937221057904672\n",
            "Epoch: 450. Loss: 0.4932953813966759\n",
            "Epoch: 460. Loss: 0.4928902140447985\n",
            "Epoch: 470. Loss: 0.4925054868898699\n",
            "Epoch: 480. Loss: 0.49214014653886606\n",
            "Epoch: 490. Loss: 0.49179319852978953\n",
            "tensor(0.9252, dtype=torch.float64)\n",
            "2021-11-14 00:00:00\n",
            "Epoch: 0. Loss: 3.5517467295108274\n",
            "Epoch: 10. Loss: 0.9964238653288189\n",
            "Epoch: 20. Loss: 0.7481281618851755\n",
            "Epoch: 30. Loss: 0.6516600236025203\n",
            "Epoch: 40. Loss: 0.5849433800699909\n",
            "Epoch: 50. Loss: 0.5429006554325843\n",
            "Epoch: 60. Loss: 0.5195577770822383\n",
            "Epoch: 70. Loss: 0.5075220037789292\n",
            "Epoch: 80. Loss: 0.5012800504941622\n",
            "Epoch: 90. Loss: 0.4978104093343667\n",
            "Epoch: 100. Loss: 0.49568697889225505\n",
            "Epoch: 110. Loss: 0.4942620514041293\n",
            "Epoch: 120. Loss: 0.49323308616886385\n",
            "Epoch: 130. Loss: 0.49244955917024863\n",
            "Epoch: 140. Loss: 0.4918305264117846\n",
            "Epoch: 150. Loss: 0.4913288728025538\n",
            "Epoch: 160. Loss: 0.49091509390726207\n",
            "Epoch: 170. Loss: 0.49056950148676326\n",
            "Epoch: 180. Loss: 0.4902782417404752\n",
            "Epoch: 190. Loss: 0.49003113672100745\n",
            "Epoch: 200. Loss: 0.48982044703323985\n",
            "Epoch: 210. Loss: 0.4896401251798512\n",
            "Epoch: 220. Loss: 0.4894853433396536\n",
            "Epoch: 230. Loss: 0.48935218179515866\n",
            "Epoch: 240. Loss: 0.4892374154462162\n",
            "Epoch: 250. Loss: 0.48913836256939314\n",
            "Epoch: 260. Loss: 0.48905277448925866\n",
            "Epoch: 270. Loss: 0.48897875300403654\n",
            "Epoch: 280. Loss: 0.4889146871833932\n",
            "Epoch: 290. Loss: 0.48885920403955374\n",
            "Epoch: 300. Loss: 0.48881112936810045\n",
            "Epoch: 310. Loss: 0.48876945620369466\n",
            "Epoch: 320. Loss: 0.48873331908952616\n",
            "Epoch: 330. Loss: 0.4887019728644987\n",
            "Epoch: 340. Loss: 0.4886747750175935\n",
            "Epoch: 350. Loss: 0.48865117089936644\n",
            "Epoch: 360. Loss: 0.48863068125089354\n",
            "Epoch: 370. Loss: 0.4886128916331589\n",
            "Epoch: 380. Loss: 0.4885974434296846\n",
            "Epoch: 390. Loss: 0.48858402616200586\n",
            "Epoch: 400. Loss: 0.4885723709080918\n",
            "Epoch: 410. Loss: 0.48856224465257947\n",
            "Epoch: 420. Loss: 0.4885534454279024\n",
            "Epoch: 430. Loss: 0.48854579812927174\n",
            "Epoch: 440. Loss: 0.48853915090559086\n",
            "Epoch: 450. Loss: 0.4885333720438758\n",
            "Epoch: 460. Loss: 0.4885283472774398\n",
            "Epoch: 470. Loss: 0.48852397745858084\n",
            "Epoch: 480. Loss: 0.48852017654524194\n",
            "Epoch: 490. Loss: 0.4885168698584333\n",
            "tensor(0.8707, dtype=torch.float64)\n",
            "2021-11-21 00:00:00\n",
            "Epoch: 0. Loss: 7.1986872492161424\n",
            "Epoch: 10. Loss: 1.6907729234495128\n",
            "Epoch: 20. Loss: 0.8684924212781459\n",
            "Epoch: 30. Loss: 0.7585775510038978\n",
            "Epoch: 40. Loss: 0.703446263445838\n",
            "Epoch: 50. Loss: 0.6600840396647043\n",
            "Epoch: 60. Loss: 0.6236619462360274\n",
            "Epoch: 70. Loss: 0.5935134575662909\n",
            "Epoch: 80. Loss: 0.5692605671918094\n",
            "Epoch: 90. Loss: 0.5503177380329195\n",
            "Epoch: 100. Loss: 0.5358889513284523\n",
            "Epoch: 110. Loss: 0.525080824119379\n",
            "Epoch: 120. Loss: 0.5170365259528765\n",
            "Epoch: 130. Loss: 0.51102889994591\n",
            "Epoch: 140. Loss: 0.5064932854145918\n",
            "Epoch: 150. Loss: 0.5030161592855151\n",
            "Epoch: 160. Loss: 0.5003046675744541\n",
            "Epoch: 170. Loss: 0.4981543587727265\n",
            "Epoch: 180. Loss: 0.4964225735951001\n",
            "Epoch: 190. Loss: 0.4950088942872338\n",
            "Epoch: 200. Loss: 0.4938415981232591\n",
            "Epoch: 210. Loss: 0.492868532463201\n",
            "Epoch: 220. Loss: 0.49205103244039455\n",
            "Epoch: 230. Loss: 0.49135986580384255\n",
            "Epoch: 240. Loss: 0.4907725106974246\n",
            "Epoch: 250. Loss: 0.490271308023525\n",
            "Epoch: 260. Loss: 0.48984219015662495\n",
            "Epoch: 270. Loss: 0.48947379261715473\n",
            "Epoch: 280. Loss: 0.4891568229311222\n",
            "Epoch: 290. Loss: 0.4888836043357535\n",
            "Epoch: 300. Loss: 0.4886477399585851\n",
            "Epoch: 310. Loss: 0.4884438612095309\n",
            "Epoch: 320. Loss: 0.48826743594423444\n",
            "Epoch: 330. Loss: 0.48811461973464726\n",
            "Epoch: 340. Loss: 0.4879821387468332\n",
            "Epoch: 350. Loss: 0.48786719618650104\n",
            "Epoch: 360. Loss: 0.4877673966140239\n",
            "Epoch: 370. Loss: 0.48768068403073567\n",
            "Epoch: 380. Loss: 0.487605290743546\n",
            "Epoch: 390. Loss: 0.48753969478724823\n",
            "Epoch: 400. Loss: 0.4874825842303284\n",
            "Epoch: 410. Loss: 0.4874328270819035\n",
            "Epoch: 420. Loss: 0.48738944580239535\n",
            "Epoch: 430. Loss: 0.487351595630933\n",
            "Epoch: 440. Loss: 0.48731854610019787\n",
            "Epoch: 450. Loss: 0.4872896652294926\n",
            "Epoch: 460. Loss: 0.4872644059796215\n",
            "Epoch: 470. Loss: 0.4872422946259296\n",
            "Epoch: 480. Loss: 0.4872229207636822\n",
            "Epoch: 490. Loss: 0.4872059287064932\n",
            "tensor(0.8455, dtype=torch.float64)\n",
            "2021-11-28 00:00:00\n",
            "Epoch: 0. Loss: 3.670453950051072\n",
            "Epoch: 10. Loss: 1.339400561511925\n",
            "Epoch: 20. Loss: 0.953906673370857\n",
            "Epoch: 30. Loss: 0.8031628233402212\n",
            "Epoch: 40. Loss: 0.6926272942071916\n",
            "Epoch: 50. Loss: 0.6115564245528675\n",
            "Epoch: 60. Loss: 0.5598983461216107\n",
            "Epoch: 70. Loss: 0.5315023401631342\n",
            "Epoch: 80. Loss: 0.516547505271495\n",
            "Epoch: 90. Loss: 0.5079719360159435\n",
            "Epoch: 100. Loss: 0.5024539094281708\n",
            "Epoch: 110. Loss: 0.498630877509035\n",
            "Epoch: 120. Loss: 0.49589755049897105\n",
            "Epoch: 130. Loss: 0.49392733445435877\n",
            "Epoch: 140. Loss: 0.4925093879949139\n",
            "Epoch: 150. Loss: 0.4914936213558294\n",
            "Epoch: 160. Loss: 0.4907695191805759\n",
            "Epoch: 170. Loss: 0.49025542293264596\n",
            "Epoch: 180. Loss: 0.4898914366158259\n",
            "Epoch: 190. Loss: 0.4896340596390839\n",
            "Epoch: 200. Loss: 0.48945199776930104\n",
            "Epoch: 210. Loss: 0.48932292252475246\n",
            "Epoch: 220. Loss: 0.4892310109185027\n",
            "Epoch: 230. Loss: 0.48916511209231167\n",
            "Epoch: 240. Loss: 0.48911740261251746\n",
            "Epoch: 250. Loss: 0.489082412802282\n",
            "Epoch: 260. Loss: 0.4890563289459879\n",
            "Epoch: 270. Loss: 0.489036497277094\n",
            "Epoch: 280. Loss: 0.4890210736899499\n",
            "Epoch: 290. Loss: 0.4890087776489887\n",
            "Epoch: 300. Loss: 0.4889987200223434\n",
            "Epoch: 310. Loss: 0.4889902830387799\n",
            "Epoch: 320. Loss: 0.4889830368154096\n",
            "Epoch: 330. Loss: 0.48897668144286005\n",
            "Epoch: 340. Loss: 0.4889710068742898\n",
            "Epoch: 350. Loss: 0.48896586518482105\n",
            "Epoch: 360. Loss: 0.4889611514080174\n",
            "Epoch: 370. Loss: 0.48895679030901157\n",
            "Epoch: 380. Loss: 0.48895272726091826\n",
            "Epoch: 390. Loss: 0.4889489219540758\n",
            "Epoch: 400. Loss: 0.48894534405918094\n",
            "Epoch: 410. Loss: 0.4889419702370709\n",
            "Epoch: 420. Loss: 0.4889387820760819\n",
            "Epoch: 430. Loss: 0.4889357646680436\n",
            "Epoch: 440. Loss: 0.48893290562384567\n",
            "Epoch: 450. Loss: 0.4889301943915163\n",
            "Epoch: 460. Loss: 0.48892762178249516\n",
            "Epoch: 470. Loss: 0.48892517964122073\n",
            "Epoch: 480. Loss: 0.48892286061341833\n",
            "Epoch: 490. Loss: 0.48892065798241224\n",
            "tensor(0.8791, dtype=torch.float64)\n",
            "2021-12-05 00:00:00\n",
            "Epoch: 0. Loss: 4.474117205763623\n",
            "Epoch: 10. Loss: 0.9824789582188435\n",
            "Epoch: 20. Loss: 0.6907796128446466\n",
            "Epoch: 30. Loss: 0.6021933577930868\n",
            "Epoch: 40. Loss: 0.5577970329439699\n",
            "Epoch: 50. Loss: 0.5377777330924295\n",
            "Epoch: 60. Loss: 0.5282564674407002\n",
            "Epoch: 70. Loss: 0.5229602303079497\n",
            "Epoch: 80. Loss: 0.5196136437258775\n",
            "Epoch: 90. Loss: 0.5173319765140889\n",
            "Epoch: 100. Loss: 0.5157069077438152\n",
            "Epoch: 110. Loss: 0.5145166913749033\n",
            "Epoch: 120. Loss: 0.5136261190383226\n",
            "Epoch: 130. Loss: 0.5129466992414744\n",
            "Epoch: 140. Loss: 0.5124181683200958\n",
            "Epoch: 150. Loss: 0.5119986083411582\n",
            "Epoch: 160. Loss: 0.5116585396644305\n",
            "Epoch: 170. Loss: 0.5113771057059568\n",
            "Epoch: 180. Loss: 0.5111394912624326\n",
            "Epoch: 190. Loss: 0.510935131859456\n",
            "Epoch: 200. Loss: 0.5107564578113163\n",
            "Epoch: 210. Loss: 0.5105980100692761\n",
            "Epoch: 220. Loss: 0.5104558178059523\n",
            "Epoch: 230. Loss: 0.5103269609152238\n",
            "Epoch: 240. Loss: 0.5102092630393483\n",
            "Epoch: 250. Loss: 0.5101010764778063\n",
            "Epoch: 260. Loss: 0.5100011315635278\n",
            "Epoch: 270. Loss: 0.509908431134331\n",
            "Epoch: 280. Loss: 0.5098221764717434\n",
            "Epoch: 290. Loss: 0.5097417151617408\n",
            "Epoch: 300. Loss: 0.5096665042166212\n",
            "Epoch: 310. Loss: 0.5095960838243369\n",
            "Epoch: 320. Loss: 0.5095300585091042\n",
            "Epoch: 330. Loss: 0.5094680834741506\n",
            "Epoch: 340. Loss: 0.5094098545824444\n",
            "Epoch: 350. Loss: 0.5093551009053391\n",
            "Epoch: 360. Loss: 0.5093035790966014\n",
            "Epoch: 370. Loss: 0.5092550690752994\n",
            "Epoch: 380. Loss: 0.5092093706569496\n",
            "Epoch: 390. Loss: 0.5091663008799288\n",
            "Epoch: 400. Loss: 0.5091256918485271\n",
            "Epoch: 410. Loss: 0.5090873889655401\n",
            "Epoch: 420. Loss: 0.5090512494631044\n",
            "Epoch: 430. Loss: 0.5090171411654876\n",
            "Epoch: 440. Loss: 0.5089849414351028\n",
            "Epoch: 450. Loss: 0.5089545362654468\n",
            "Epoch: 460. Loss: 0.5089258194935214\n",
            "Epoch: 470. Loss: 0.5088986921106919\n",
            "Epoch: 480. Loss: 0.5088730616556015\n",
            "Epoch: 490. Loss: 0.5088488416761965\n",
            "tensor(0.8119, dtype=torch.float64)\n",
            "2021-12-12 00:00:00\n",
            "Epoch: 0. Loss: 2.3087373277256873\n",
            "Epoch: 10. Loss: 1.0464013131071876\n",
            "Epoch: 20. Loss: 0.8451558744985682\n",
            "Epoch: 30. Loss: 0.7579428281906958\n",
            "Epoch: 40. Loss: 0.707056146094252\n",
            "Epoch: 50. Loss: 0.6743891781037376\n",
            "Epoch: 60. Loss: 0.6515745565612205\n",
            "Epoch: 70. Loss: 0.6343780475423806\n",
            "Epoch: 80. Loss: 0.6207534039746978\n",
            "Epoch: 90. Loss: 0.6096564593751527\n",
            "Epoch: 100. Loss: 0.6004730555315506\n",
            "Epoch: 110. Loss: 0.5927851519105519\n",
            "Epoch: 120. Loss: 0.5862802311302799\n",
            "Epoch: 130. Loss: 0.5807147118807408\n",
            "Epoch: 140. Loss: 0.5758965339251005\n",
            "Epoch: 150. Loss: 0.5716744978166144\n",
            "Epoch: 160. Loss: 0.5679302705778979\n",
            "Epoch: 170. Loss: 0.5645718221040656\n",
            "Epoch: 180. Loss: 0.5615279390283243\n",
            "Epoch: 190. Loss: 0.5587436877550193\n",
            "Epoch: 200. Loss: 0.5561767300262601\n",
            "Epoch: 210. Loss: 0.5537943847238338\n",
            "Epoch: 220. Loss: 0.5515713218897621\n",
            "Epoch: 230. Loss: 0.5494877770096823\n",
            "Epoch: 240. Loss: 0.5475281833368935\n",
            "Epoch: 250. Loss: 0.5456801337558086\n",
            "Epoch: 260. Loss: 0.543933598473929\n",
            "Epoch: 270. Loss: 0.5422803388895503\n",
            "Epoch: 280. Loss: 0.5407134704009486\n",
            "Epoch: 290. Loss: 0.5392271373832717\n",
            "Epoch: 300. Loss: 0.5378162720833204\n",
            "Epoch: 310. Loss: 0.5364764159622541\n",
            "Epoch: 320. Loss: 0.5352035873106402\n",
            "Epoch: 330. Loss: 0.53399418303585\n",
            "Epoch: 340. Loss: 0.5328449056233995\n",
            "Epoch: 350. Loss: 0.5317527086123404\n",
            "Epoch: 360. Loss: 0.5307147556745336\n",
            "Epoch: 370. Loss: 0.5297283896885517\n",
            "Epoch: 380. Loss: 0.5287911091611205\n",
            "Epoch: 390. Loss: 0.5279005500576051\n",
            "Epoch: 400. Loss: 0.5270544716230713\n",
            "Epoch: 410. Loss: 0.5262507451561037\n",
            "Epoch: 420. Loss: 0.525487344975639\n",
            "Epoch: 430. Loss: 0.5247623410239863\n",
            "Epoch: 440. Loss: 0.5240738926972028\n",
            "Epoch: 450. Loss: 0.5234202436019595\n",
            "Epoch: 460. Loss: 0.5227997170168835\n",
            "Epoch: 470. Loss: 0.5222107118940376\n",
            "Epoch: 480. Loss: 0.5216516992784904\n",
            "Epoch: 490. Loss: 0.5211212190550355\n",
            "tensor(0.6177, dtype=torch.float64)\n",
            "2021-12-19 00:00:00\n",
            "Epoch: 0. Loss: 1.3831316405133163\n",
            "Epoch: 10. Loss: 0.5637469586750015\n",
            "Epoch: 20. Loss: 0.5483986188289807\n",
            "Epoch: 30. Loss: 0.5433328944130061\n",
            "Epoch: 40. Loss: 0.540630809051188\n",
            "Epoch: 50. Loss: 0.5387472199881125\n",
            "Epoch: 60. Loss: 0.537265347845219\n",
            "Epoch: 70. Loss: 0.5360345715755822\n",
            "Epoch: 80. Loss: 0.5349802743909265\n",
            "Epoch: 90. Loss: 0.5340570076152246\n",
            "Epoch: 100. Loss: 0.5332342974632233\n",
            "Epoch: 110. Loss: 0.5324908685403902\n",
            "Epoch: 120. Loss: 0.5318115674341999\n",
            "Epoch: 130. Loss: 0.5311854438519596\n",
            "Epoch: 140. Loss: 0.5306044680068747\n",
            "Epoch: 150. Loss: 0.5300626507869063\n",
            "Epoch: 160. Loss: 0.5295554350389919\n",
            "Epoch: 170. Loss: 0.5290792735112286\n",
            "Epoch: 180. Loss: 0.5286313361681876\n",
            "Epoch: 190. Loss: 0.5282093072299164\n",
            "Epoch: 200. Loss: 0.5278112443442904\n",
            "Epoch: 210. Loss: 0.52743548070282\n",
            "Epoch: 220. Loss: 0.5270805567863178\n",
            "Epoch: 230. Loss: 0.526745172531342\n",
            "Epoch: 240. Loss: 0.5264281535654396\n",
            "Epoch: 250. Loss: 0.5261284271403179\n",
            "Epoch: 260. Loss: 0.5258450047607572\n",
            "Epoch: 270. Loss: 0.5255769694496035\n",
            "Epoch: 280. Loss: 0.5253234662365313\n",
            "Epoch: 290. Loss: 0.5250836949018902\n",
            "Epoch: 300. Loss: 0.5248569043105484\n",
            "Epoch: 310. Loss: 0.524642387878155\n",
            "Epoch: 320. Loss: 0.5244394798540685\n",
            "Epoch: 330. Loss: 0.5242475522021306\n",
            "Epoch: 340. Loss: 0.5240660119267871\n",
            "Epoch: 350. Loss: 0.5238942987375228\n",
            "Epoch: 360. Loss: 0.5237318829758202\n",
            "Epoch: 370. Loss: 0.5235782637503974\n",
            "Epoch: 380. Loss: 0.5234329672414251\n",
            "Epoch: 390. Loss: 0.5232955451448246\n",
            "Epoch: 400. Loss: 0.5231655732350705\n",
            "Epoch: 410. Loss: 0.5230426500300974\n",
            "Epoch: 420. Loss: 0.5229263955456102\n",
            "Epoch: 430. Loss: 0.522816450128791\n",
            "Epoch: 440. Loss: 0.5227124733633455\n",
            "Epoch: 450. Loss: 0.5226141430392998\n",
            "Epoch: 460. Loss: 0.5225211541820516\n",
            "Epoch: 470. Loss: 0.522433218136022\n",
            "Epoch: 480. Loss: 0.5223500616989017\n",
            "Epoch: 490. Loss: 0.5222714263030016\n",
            "tensor(0.7049, dtype=torch.float64)\n",
            "2021-12-26 00:00:00\n",
            "Epoch: 0. Loss: 1.0849889304208962\n",
            "Epoch: 10. Loss: 0.7179055099037708\n",
            "Epoch: 20. Loss: 0.5999434319250244\n",
            "Epoch: 30. Loss: 0.5669068780903139\n",
            "Epoch: 40. Loss: 0.5521205329610257\n",
            "Epoch: 50. Loss: 0.5444436005902467\n",
            "Epoch: 60. Loss: 0.5397877942596415\n",
            "Epoch: 70. Loss: 0.536513979408459\n",
            "Epoch: 80. Loss: 0.5339776895676543\n",
            "Epoch: 90. Loss: 0.5319111024874315\n",
            "Epoch: 100. Loss: 0.530187924348169\n",
            "Epoch: 110. Loss: 0.5287371516135723\n",
            "Epoch: 120. Loss: 0.5275113261472661\n",
            "Epoch: 130. Loss: 0.5264745508899482\n",
            "Epoch: 140. Loss: 0.5255977380751686\n",
            "Epoch: 150. Loss: 0.5248565458401955\n",
            "Epoch: 160. Loss: 0.5242303363945345\n",
            "Epoch: 170. Loss: 0.5237015388740176\n",
            "Epoch: 180. Loss: 0.5232551873579163\n",
            "Epoch: 190. Loss: 0.5228785480884599\n",
            "Epoch: 200. Loss: 0.5225608029492452\n",
            "Epoch: 210. Loss: 0.5222927755822939\n",
            "Epoch: 220. Loss: 0.5220666934033594\n",
            "Epoch: 230. Loss: 0.521875981157246\n",
            "Epoch: 240. Loss: 0.5217150824846521\n",
            "Epoch: 250. Loss: 0.5215793063163578\n",
            "Epoch: 260. Loss: 0.5214646951364798\n",
            "Epoch: 270. Loss: 0.5213679123756724\n",
            "Epoch: 280. Loss: 0.5212861464303337\n",
            "Epoch: 290. Loss: 0.5212170290503694\n",
            "Epoch: 300. Loss: 0.5211585660853527\n",
            "Epoch: 310. Loss: 0.5211090788176481\n",
            "Epoch: 320. Loss: 0.5210671543347621\n",
            "Epoch: 330. Loss: 0.521031603598054\n",
            "Epoch: 340. Loss: 0.521001426049337\n",
            "Epoch: 350. Loss: 0.52097577976062\n",
            "Epoch: 360. Loss: 0.5209539562761096\n",
            "Epoch: 370. Loss: 0.5209353594209302\n",
            "Epoch: 380. Loss: 0.5209194874595073\n",
            "Epoch: 390. Loss: 0.5209059180799525\n",
            "Epoch: 400. Loss: 0.5208942957608299\n",
            "Epoch: 410. Loss: 0.5208843211450546\n",
            "Epoch: 420. Loss: 0.5208757421038821\n",
            "Epoch: 430. Loss: 0.5208683462234127\n",
            "Epoch: 440. Loss: 0.5208619544879599\n",
            "Epoch: 450. Loss: 0.5208564159701339\n",
            "Epoch: 460. Loss: 0.5208516033674881\n",
            "Epoch: 470. Loss: 0.5208474092509197\n",
            "Epoch: 480. Loss: 0.5208437429113801\n",
            "Epoch: 490. Loss: 0.5208405277094754\n",
            "tensor(0.7312, dtype=torch.float64)\n",
            "2022-01-02 00:00:00\n",
            "Epoch: 0. Loss: 5.699609621810447\n",
            "Epoch: 10. Loss: 1.2158196412621676\n",
            "Epoch: 20. Loss: 0.7916801291155183\n",
            "Epoch: 30. Loss: 0.6865054143426775\n",
            "Epoch: 40. Loss: 0.620453043446453\n",
            "Epoch: 50. Loss: 0.5793427736574623\n",
            "Epoch: 60. Loss: 0.555965910207976\n",
            "Epoch: 70. Loss: 0.5436937402503136\n",
            "Epoch: 80. Loss: 0.537350631918502\n",
            "Epoch: 90. Loss: 0.5338263336231057\n",
            "Epoch: 100. Loss: 0.5315851662562352\n",
            "Epoch: 110. Loss: 0.5299508839477726\n",
            "Epoch: 120. Loss: 0.5286393390436482\n",
            "Epoch: 130. Loss: 0.5275294716837883\n",
            "Epoch: 140. Loss: 0.5265653695294488\n",
            "Epoch: 150. Loss: 0.5257171425254771\n",
            "Epoch: 160. Loss: 0.5249657892963001\n",
            "Epoch: 170. Loss: 0.5242973840541827\n",
            "Epoch: 180. Loss: 0.5237007945315096\n",
            "Epoch: 190. Loss: 0.5231667246990258\n",
            "Epoch: 200. Loss: 0.5226872577752892\n",
            "Epoch: 210. Loss: 0.522255594542474\n",
            "Epoch: 220. Loss: 0.5218658743602801\n",
            "Epoch: 230. Loss: 0.5215130368204709\n",
            "Epoch: 240. Loss: 0.521192707706043\n",
            "Epoch: 250. Loss: 0.5209011022875443\n",
            "Epoch: 260. Loss: 0.5206349424367629\n",
            "Epoch: 270. Loss: 0.5203913853574914\n",
            "Epoch: 280. Loss: 0.5201679622924158\n",
            "Epoch: 290. Loss: 0.519962525852995\n",
            "Epoch: 300. Loss: 0.5197732048074727\n",
            "Epoch: 310. Loss: 0.519598365310129\n",
            "Epoch: 320. Loss: 0.5194365776822641\n",
            "Epoch: 330. Loss: 0.5192865879686026\n",
            "Epoch: 340. Loss: 0.5191472935939283\n",
            "Epoch: 350. Loss: 0.5190177225347503\n",
            "Epoch: 360. Loss: 0.5188970155003892\n",
            "Epoch: 370. Loss: 0.5187844106878247\n",
            "Epoch: 380. Loss: 0.5186792307357929\n",
            "Epoch: 390. Loss: 0.5185808715568124\n",
            "Epoch: 400. Loss: 0.5184887927719153\n",
            "Epoch: 410. Loss: 0.5184025095126708\n",
            "Epoch: 420. Loss: 0.5183215853893763\n",
            "Epoch: 430. Loss: 0.5182456264537558\n",
            "Epoch: 440. Loss: 0.5181742760097692\n",
            "Epoch: 450. Loss: 0.5181072101477702\n",
            "Epoch: 460. Loss: 0.5180441338957443\n",
            "Epoch: 470. Loss: 0.5179847778971419\n",
            "Epoch: 480. Loss: 0.5179288955383012\n",
            "Epoch: 490. Loss: 0.5178762604599266\n",
            "tensor(0.7149, dtype=torch.float64)\n",
            "2022-01-09 00:00:00\n",
            "Epoch: 0. Loss: 2.1731622027194772\n",
            "Epoch: 10. Loss: 0.7047513450068841\n",
            "Epoch: 20. Loss: 0.6151039773069139\n",
            "Epoch: 30. Loss: 0.5725467468597839\n",
            "Epoch: 40. Loss: 0.5492236407961474\n",
            "Epoch: 50. Loss: 0.5370822342637178\n",
            "Epoch: 60. Loss: 0.5306768549368243\n",
            "Epoch: 70. Loss: 0.5271067470072799\n",
            "Epoch: 80. Loss: 0.5249907496673462\n",
            "Epoch: 90. Loss: 0.5236715864239966\n",
            "Epoch: 100. Loss: 0.5228171266235343\n",
            "Epoch: 110. Loss: 0.522246455767957\n",
            "Epoch: 120. Loss: 0.5218545545891964\n",
            "Epoch: 130. Loss: 0.521577671478228\n",
            "Epoch: 140. Loss: 0.521375977253657\n",
            "Epoch: 150. Loss: 0.5212241328314883\n",
            "Epoch: 160. Loss: 0.5211058238033992\n",
            "Epoch: 170. Loss: 0.5210104543458511\n",
            "Epoch: 180. Loss: 0.5209310920861229\n",
            "Epoch: 190. Loss: 0.5208631687772963\n",
            "Epoch: 200. Loss: 0.5208036499820414\n",
            "Epoch: 210. Loss: 0.5207505007083306\n",
            "Epoch: 220. Loss: 0.5207023398042966\n",
            "Epoch: 230. Loss: 0.5206582156046119\n",
            "Epoch: 240. Loss: 0.5206174598533415\n",
            "Epoch: 250. Loss: 0.5205795923457849\n",
            "Epoch: 260. Loss: 0.520544258526962\n",
            "Epoch: 270. Loss: 0.5205111885541361\n",
            "Epoch: 280. Loss: 0.5204801703655838\n",
            "Epoch: 290. Loss: 0.5204510319048538\n",
            "Epoch: 300. Loss: 0.5204236293394969\n",
            "Epoch: 310. Loss: 0.5203978392111408\n",
            "Epoch: 320. Loss: 0.5203735531685355\n",
            "Epoch: 330. Loss: 0.5203506744012629\n",
            "Epoch: 340. Loss: 0.5203291151961\n",
            "Epoch: 350. Loss: 0.5203087952369442\n",
            "Epoch: 360. Loss: 0.5202896403993403\n",
            "Epoch: 370. Loss: 0.5202715818758812\n",
            "Epoch: 380. Loss: 0.5202545555245992\n",
            "Epoch: 390. Loss: 0.5202385013691144\n",
            "Epoch: 400. Loss: 0.5202233632033584\n",
            "Epoch: 410. Loss: 0.5202090882695054\n",
            "Epoch: 420. Loss: 0.520195626988159\n",
            "Epoch: 430. Loss: 0.5201829327266972\n",
            "Epoch: 440. Loss: 0.5201709615962233\n",
            "Epoch: 450. Loss: 0.5201596722705727\n",
            "Epoch: 460. Loss: 0.5201490258228245\n",
            "Epoch: 470. Loss: 0.5201389855761055\n",
            "Epoch: 480. Loss: 0.5201295169663726\n",
            "Epoch: 490. Loss: 0.5201205874154674\n",
            "tensor(0.8275, dtype=torch.float64)\n",
            "2022-01-16 00:00:00\n",
            "Epoch: 0. Loss: 0.9088148230418076\n",
            "Epoch: 10. Loss: 0.7370346087423525\n",
            "Epoch: 20. Loss: 0.6682256625681543\n",
            "Epoch: 30. Loss: 0.6279835756678319\n",
            "Epoch: 40. Loss: 0.602602978388519\n",
            "Epoch: 50. Loss: 0.5853636438988707\n",
            "Epoch: 60. Loss: 0.5730836928090707\n",
            "Epoch: 70. Loss: 0.564098654560387\n",
            "Epoch: 80. Loss: 0.5573978258722408\n",
            "Epoch: 90. Loss: 0.5523072473390753\n",
            "Epoch: 100. Loss: 0.5483601556356739\n",
            "Epoch: 110. Loss: 0.5452297767161917\n",
            "Epoch: 120. Loss: 0.5426869898136354\n",
            "Epoch: 130. Loss: 0.5405712127593312\n",
            "Epoch: 140. Loss: 0.538769884976611\n",
            "Epoch: 150. Loss: 0.5372039897025681\n",
            "Epoch: 160. Loss: 0.5358178836766144\n",
            "Epoch: 170. Loss: 0.5345721871341218\n",
            "Epoch: 180. Loss: 0.5334388318949308\n",
            "Epoch: 190. Loss: 0.5323976213160058\n",
            "Epoch: 200. Loss: 0.531433844215503\n",
            "Epoch: 210. Loss: 0.5305366211055135\n",
            "Epoch: 220. Loss: 0.5296977581561897\n",
            "Epoch: 230. Loss: 0.5289109527446088\n",
            "Epoch: 240. Loss: 0.5281712423081075\n",
            "Epoch: 250. Loss: 0.5274746215347984\n",
            "Epoch: 260. Loss: 0.5268177760341013\n",
            "Epoch: 270. Loss: 0.5261978966312141\n",
            "Epoch: 280. Loss: 0.5256125494962856\n",
            "Epoch: 290. Loss: 0.525059584968923\n",
            "Epoch: 300. Loss: 0.5245370732253942\n",
            "Epoch: 310. Loss: 0.524043258589289\n",
            "Epoch: 320. Loss: 0.5235765268112865\n",
            "Epoch: 330. Loss: 0.523135381388857\n",
            "Epoch: 340. Loss: 0.5227184262032237\n",
            "Epoch: 350. Loss: 0.5223243525852278\n",
            "Epoch: 360. Loss: 0.5219519294988877\n",
            "Epoch: 370. Loss: 0.5215999959309033\n",
            "Epoch: 380. Loss: 0.5212674548510052\n",
            "Epoch: 390. Loss: 0.5209532682998222\n",
            "Epoch: 400. Loss: 0.520656453294017\n",
            "Epoch: 410. Loss: 0.5203760783309248\n",
            "Epoch: 420. Loss: 0.5201112603393088\n",
            "Epoch: 430. Loss: 0.5198611619677505\n",
            "Epoch: 440. Loss: 0.5196249891335907\n",
            "Epoch: 450. Loss: 0.5194019887773519\n",
            "Epoch: 460. Loss: 0.5191914467830422\n",
            "Epoch: 470. Loss: 0.5189926860356653\n",
            "Epoch: 480. Loss: 0.518805064594977\n",
            "Epoch: 490. Loss: 0.5186279739700227\n",
            "tensor(0.8790, dtype=torch.float64)\n",
            "2022-01-23 00:00:00\n",
            "Epoch: 0. Loss: 4.95870213279276\n",
            "Epoch: 10. Loss: 0.7677128037216336\n",
            "Epoch: 20. Loss: 0.6129333928818956\n",
            "Epoch: 30. Loss: 0.5755373197828734\n",
            "Epoch: 40. Loss: 0.5627880096314483\n",
            "Epoch: 50. Loss: 0.5564309349232898\n",
            "Epoch: 60. Loss: 0.5521436401261479\n",
            "Epoch: 70. Loss: 0.5488319095334995\n",
            "Epoch: 80. Loss: 0.5461452547323535\n",
            "Epoch: 90. Loss: 0.5439287275692691\n",
            "Epoch: 100. Loss: 0.5420894469855362\n",
            "Epoch: 110. Loss: 0.5405598865810982\n",
            "Epoch: 120. Loss: 0.5392863463892986\n",
            "Epoch: 130. Loss: 0.538224658500225\n",
            "Epoch: 140. Loss: 0.5373381413054786\n",
            "Epoch: 150. Loss: 0.5365963130746186\n",
            "Epoch: 160. Loss: 0.5359739054326477\n",
            "Epoch: 170. Loss: 0.535450035424332\n",
            "Epoch: 180. Loss: 0.5350074925614589\n",
            "Epoch: 190. Loss: 0.5346321250398159\n",
            "Epoch: 200. Loss: 0.5343123156529911\n",
            "Epoch: 210. Loss: 0.5340385385073559\n",
            "Epoch: 220. Loss: 0.5338029872763597\n",
            "Epoch: 230. Loss: 0.5335992656733938\n",
            "Epoch: 240. Loss: 0.5334221312215086\n",
            "Epoch: 250. Loss: 0.533267284124541\n",
            "Epoch: 260. Loss: 0.5331311939394708\n",
            "Epoch: 270. Loss: 0.533010957693341\n",
            "Epoch: 280. Loss: 0.5329041840023617\n",
            "Epoch: 290. Loss: 0.5328088985925431\n",
            "Epoch: 300. Loss: 0.5327234673703521\n",
            "Epoch: 310. Loss: 0.532646533843145\n",
            "Epoch: 320. Loss: 0.5325769682457542\n",
            "Epoch: 330. Loss: 0.5325138261994187\n",
            "Epoch: 340. Loss: 0.5324563151220117\n",
            "Epoch: 350. Loss: 0.532403766934505\n",
            "Epoch: 360. Loss: 0.5323556158776216\n",
            "Epoch: 370. Loss: 0.5323113804736431\n",
            "Epoch: 380. Loss: 0.5322706488492908\n",
            "Epoch: 390. Loss: 0.5322330667833318\n",
            "Epoch: 400. Loss: 0.5321983279629047\n",
            "Epoch: 410. Loss: 0.5321661660304262\n",
            "Epoch: 420. Loss: 0.5321363480824136\n",
            "Epoch: 430. Loss: 0.5321086693460138\n",
            "Epoch: 440. Loss: 0.5320829488112797\n",
            "Epoch: 450. Loss: 0.5320590256395378\n",
            "Epoch: 460. Loss: 0.5320367562024395\n",
            "Epoch: 470. Loss: 0.5320160116339994\n",
            "Epoch: 480. Loss: 0.5319966758003307\n",
            "Epoch: 490. Loss: 0.5319786436099229\n",
            "tensor(0.4579, dtype=torch.float64)\n",
            "2022-01-30 00:00:00\n",
            "Epoch: 0. Loss: 1.1127543900325776\n",
            "Epoch: 10. Loss: 0.8060691059606252\n",
            "Epoch: 20. Loss: 0.6394318454267339\n",
            "Epoch: 30. Loss: 0.5854382591215068\n",
            "Epoch: 40. Loss: 0.572094587586511\n",
            "Epoch: 50. Loss: 0.566856123692694\n",
            "Epoch: 60. Loss: 0.5636344699642648\n",
            "Epoch: 70. Loss: 0.5612299533380571\n",
            "Epoch: 80. Loss: 0.559309594402214\n",
            "Epoch: 90. Loss: 0.5577327032327551\n",
            "Epoch: 100. Loss: 0.5564176931352737\n",
            "Epoch: 110. Loss: 0.5553085682315778\n",
            "Epoch: 120. Loss: 0.5543638487130276\n",
            "Epoch: 130. Loss: 0.5535517629166308\n",
            "Epoch: 140. Loss: 0.5528475951354331\n",
            "Epoch: 150. Loss: 0.5522319564166656\n",
            "Epoch: 160. Loss: 0.5516895478453225\n",
            "Epoch: 170. Loss: 0.5512082362206876\n",
            "Epoch: 180. Loss: 0.5507783494746055\n",
            "Epoch: 190. Loss: 0.550392134794884\n",
            "Epoch: 200. Loss: 0.5500433400675747\n",
            "Epoch: 210. Loss: 0.5497268897760125\n",
            "Epoch: 220. Loss: 0.549438633612633\n",
            "Epoch: 230. Loss: 0.5491751512128321\n",
            "Epoch: 240. Loss: 0.5489336002745976\n",
            "Epoch: 250. Loss: 0.5487115982551447\n",
            "Epoch: 260. Loss: 0.5485071300760859\n",
            "Epoch: 270. Loss: 0.5483184759900378\n",
            "Epoch: 280. Loss: 0.5481441550874638\n",
            "Epoch: 290. Loss: 0.5479828809454488\n",
            "Epoch: 300. Loss: 0.5478335267101362\n",
            "Epoch: 310. Loss: 0.5476950975152338\n",
            "Epoch: 320. Loss: 0.54756670861131\n",
            "Epoch: 330. Loss: 0.5474475679460822\n",
            "Epoch: 340. Loss: 0.5473369622188086\n",
            "Epoch: 350. Loss: 0.5472342456509368\n",
            "Epoch: 360. Loss: 0.5471388308848235\n",
            "Epoch: 370. Loss: 0.5470501815537497\n",
            "Epoch: 380. Loss: 0.5469678061682801\n",
            "Epoch: 390. Loss: 0.5468912530429255\n",
            "Epoch: 400. Loss: 0.5468201060482306\n",
            "Epoch: 410. Loss: 0.5467539810208255\n",
            "Epoch: 420. Loss: 0.5466925227007595\n",
            "Epoch: 430. Loss: 0.5466354020939646\n",
            "Epoch: 440. Loss: 0.5465823141798335\n",
            "Epoch: 450. Loss: 0.5465329759010913\n",
            "Epoch: 460. Loss: 0.5464871243864924\n",
            "Epoch: 470. Loss: 0.5464445153672555\n",
            "Epoch: 480. Loss: 0.546404921756227\n",
            "Epoch: 490. Loss: 0.5463681323650558\n",
            "tensor(0.6348, dtype=torch.float64)\n",
            "2022-02-06 00:00:00\n",
            "Epoch: 0. Loss: 0.8203903749413036\n",
            "Epoch: 10. Loss: 0.7181111299432412\n",
            "Epoch: 20. Loss: 0.662356650780193\n",
            "Epoch: 30. Loss: 0.6303930718872733\n",
            "Epoch: 40. Loss: 0.6100619429563416\n",
            "Epoch: 50. Loss: 0.5956456344228479\n",
            "Epoch: 60. Loss: 0.5847064398202266\n",
            "Epoch: 70. Loss: 0.5761464463190034\n",
            "Epoch: 80. Loss: 0.5693781476592268\n",
            "Epoch: 90. Loss: 0.5640189136911506\n",
            "Epoch: 100. Loss: 0.5597839334263511\n",
            "Epoch: 110. Loss: 0.556447644126928\n",
            "Epoch: 120. Loss: 0.5538277260869715\n",
            "Epoch: 130. Loss: 0.551776452005773\n",
            "Epoch: 140. Loss: 0.5501746043076409\n",
            "Epoch: 150. Loss: 0.5489265341117388\n",
            "Epoch: 160. Loss: 0.5479559573757719\n",
            "Epoch: 170. Loss: 0.5472023664827094\n",
            "Epoch: 180. Loss: 0.5466179988221054\n",
            "Epoch: 190. Loss: 0.5461653095410417\n",
            "Epoch: 200. Loss: 0.545814891047313\n",
            "Epoch: 210. Loss: 0.5455437792257136\n",
            "Epoch: 220. Loss: 0.5453340875227538\n",
            "Epoch: 230. Loss: 0.5451719142221718\n",
            "Epoch: 240. Loss: 0.5450464741202834\n",
            "Epoch: 250. Loss: 0.5449494123769544\n",
            "Epoch: 260. Loss: 0.5448742648502852\n",
            "Epoch: 270. Loss: 0.544816035292351\n",
            "Epoch: 280. Loss: 0.5447708651728849\n",
            "Epoch: 290. Loss: 0.5447357765332814\n",
            "Epoch: 300. Loss: 0.5447084721675417\n",
            "Epoch: 310. Loss: 0.5446871806405758\n",
            "Epoch: 320. Loss: 0.5446705362702624\n",
            "Epoch: 330. Loss: 0.5446574863061606\n",
            "Epoch: 340. Loss: 0.5446472192195323\n",
            "Epoch: 350. Loss: 0.5446391093527865\n",
            "Epoch: 360. Loss: 0.5446326742279074\n",
            "Epoch: 370. Loss: 0.5446275416387645\n",
            "Epoch: 380. Loss: 0.5446234242976637\n",
            "Epoch: 390. Loss: 0.5446201003097862\n",
            "Epoch: 400. Loss: 0.5446173981405663\n",
            "Epoch: 410. Loss: 0.544615185044875\n",
            "Epoch: 420. Loss: 0.544613358162256\n",
            "Epoch: 430. Loss: 0.5446118376645904\n",
            "Epoch: 440. Loss: 0.544610561483304\n",
            "Epoch: 450. Loss: 0.5446094812518875\n",
            "Epoch: 460. Loss: 0.5446085591833142\n",
            "Epoch: 470. Loss: 0.5446077656665456\n",
            "Epoch: 480. Loss: 0.5446070774160814\n",
            "Epoch: 490. Loss: 0.5446064760468424\n",
            "tensor(0.6803, dtype=torch.float64)\n",
            "2022-02-13 00:00:00\n",
            "Epoch: 0. Loss: 1.5322408320410812\n",
            "Epoch: 10. Loss: 1.1333562180862875\n",
            "Epoch: 20. Loss: 0.8239913853978488\n",
            "Epoch: 30. Loss: 0.6276774200438573\n",
            "Epoch: 40. Loss: 0.5581305370497931\n",
            "Epoch: 50. Loss: 0.5466583742746481\n",
            "Epoch: 60. Loss: 0.5448430550699231\n",
            "Epoch: 70. Loss: 0.5442613960224247\n",
            "Epoch: 80. Loss: 0.5438695637517545\n",
            "Epoch: 90. Loss: 0.5435270526624869\n",
            "Epoch: 100. Loss: 0.5432114248232843\n",
            "Epoch: 110. Loss: 0.5429175120081041\n",
            "Epoch: 120. Loss: 0.5426430968592937\n",
            "Epoch: 130. Loss: 0.542386627718433\n",
            "Epoch: 140. Loss: 0.5421467964751063\n",
            "Epoch: 150. Loss: 0.5419224397504381\n",
            "Epoch: 160. Loss: 0.5417125042688334\n",
            "Epoch: 170. Loss: 0.5415160280666041\n",
            "Epoch: 180. Loss: 0.5413321273665409\n",
            "Epoch: 190. Loss: 0.5411599864841495\n",
            "Epoch: 200. Loss: 0.5409988497863703\n",
            "Epoch: 210. Loss: 0.5408480151697136\n",
            "Epoch: 220. Loss: 0.540706828697011\n",
            "Epoch: 230. Loss: 0.5405746801280137\n",
            "Epoch: 240. Loss: 0.5404509991446367\n",
            "Epoch: 250. Loss: 0.5403352521198255\n",
            "Epoch: 260. Loss: 0.5402269393152281\n",
            "Epoch: 270. Loss: 0.540125592420215\n",
            "Epoch: 280. Loss: 0.5400307723655107\n",
            "Epoch: 290. Loss: 0.5399420673603811\n",
            "Epoch: 300. Loss: 0.5398590911141985\n",
            "Epoch: 310. Loss: 0.5397814812121964\n",
            "Epoch: 320. Loss: 0.5397088976220304\n",
            "Epoch: 330. Loss: 0.539641021312916\n",
            "Epoch: 340. Loss: 0.5395775529730162\n",
            "Epoch: 350. Loss: 0.5395182118137098\n",
            "Epoch: 360. Loss: 0.5394627344516172\n",
            "Epoch: 370. Loss: 0.5394108738609631\n",
            "Epoch: 380. Loss: 0.5393623983901608\n",
            "Epoch: 390. Loss: 0.5393170908374881\n",
            "Epoch: 400. Loss: 0.5392747475814996\n",
            "Epoch: 410. Loss: 0.5392351777623954\n",
            "Epoch: 420. Loss: 0.5391982025110394\n",
            "Epoch: 430. Loss: 0.5391636542226732\n",
            "Epoch: 440. Loss: 0.5391313758726605\n",
            "Epoch: 450. Loss: 0.5391012203718331\n",
            "Epoch: 460. Loss: 0.5390730499591962\n",
            "Epoch: 470. Loss: 0.5390467356299127\n",
            "Epoch: 480. Loss: 0.5390221565966207\n",
            "Epoch: 490. Loss: 0.5389991997822587\n",
            "tensor(0.7771, dtype=torch.float64)\n",
            "2022-02-20 00:00:00\n",
            "Epoch: 0. Loss: 1.7425125499256975\n",
            "Epoch: 10. Loss: 1.2900564159802401\n",
            "Epoch: 20. Loss: 0.9151121948594167\n",
            "Epoch: 30. Loss: 0.6642368353367442\n",
            "Epoch: 40. Loss: 0.5664769383783888\n",
            "Epoch: 50. Loss: 0.5489709055264604\n",
            "Epoch: 60. Loss: 0.5455558864698625\n",
            "Epoch: 70. Loss: 0.5439604084455768\n",
            "Epoch: 80. Loss: 0.5427814455084147\n",
            "Epoch: 90. Loss: 0.5418152266501854\n",
            "Epoch: 100. Loss: 0.5410017502073281\n",
            "Epoch: 110. Loss: 0.5403064067090187\n",
            "Epoch: 120. Loss: 0.5397043071670587\n",
            "Epoch: 130. Loss: 0.5391767487433785\n",
            "Epoch: 140. Loss: 0.5387095167666088\n",
            "Epoch: 150. Loss: 0.5382917339590122\n",
            "Epoch: 160. Loss: 0.5379150203967583\n",
            "Epoch: 170. Loss: 0.537572871652931\n",
            "Epoch: 180. Loss: 0.537260196572961\n",
            "Epoch: 190. Loss: 0.536972972819616\n",
            "Epoch: 200. Loss: 0.5367079895752507\n",
            "Epoch: 210. Loss: 0.536462654900159\n",
            "Epoch: 220. Loss: 0.5362348511637327\n",
            "Epoch: 230. Loss: 0.5360228262967608\n",
            "Epoch: 240. Loss: 0.5358251117911738\n",
            "Epoch: 250. Loss: 0.5356404607109682\n",
            "Epoch: 260. Loss: 0.5354678007018707\n",
            "Epoch: 270. Loss: 0.5353061982620433\n",
            "Epoch: 280. Loss: 0.5351548314812838\n",
            "Epoch: 290. Loss: 0.5350129691586996\n",
            "Epoch: 300. Loss: 0.5348799547321844\n",
            "Epoch: 310. Loss: 0.5347551938437399\n",
            "Epoch: 320. Loss: 0.5346381446568561\n",
            "Epoch: 330. Loss: 0.5345283102610435\n",
            "Epoch: 340. Loss: 0.5344252326627716\n",
            "Epoch: 350. Loss: 0.5343284879853514\n",
            "Epoch: 360. Loss: 0.534237682592981\n",
            "Epoch: 370. Loss: 0.5341524499238881\n",
            "Epoch: 380. Loss: 0.53407244787001\n",
            "Epoch: 390. Loss: 0.5339973565801824\n",
            "Epoch: 400. Loss: 0.5339268765936349\n",
            "Epoch: 410. Loss: 0.5338607272330514\n",
            "Epoch: 420. Loss: 0.5337986452034232\n",
            "Epoch: 430. Loss: 0.5337403833557146\n",
            "Epoch: 440. Loss: 0.5336857095840238\n",
            "Epoch: 450. Loss: 0.5336344058322252\n",
            "Epoch: 460. Loss: 0.5335862671915904\n",
            "Epoch: 470. Loss: 0.5335411010750607\n",
            "Epoch: 480. Loss: 0.5334987264570045\n",
            "Epoch: 490. Loss: 0.5334589731696829\n",
            "tensor(0.5815, dtype=torch.float64)\n",
            "2022-02-27 00:00:00\n",
            "Epoch: 0. Loss: 2.407756728659273\n",
            "Epoch: 10. Loss: 0.6216704964359787\n",
            "Epoch: 20. Loss: 0.5810961499240314\n",
            "Epoch: 30. Loss: 0.5628791677042997\n",
            "Epoch: 40. Loss: 0.5534238259844598\n",
            "Epoch: 50. Loss: 0.5479973798844153\n",
            "Epoch: 60. Loss: 0.5446628923433825\n",
            "Epoch: 70. Loss: 0.5425412249632282\n",
            "Epoch: 80. Loss: 0.5411642901741427\n",
            "Epoch: 90. Loss: 0.5402570712874184\n",
            "Epoch: 100. Loss: 0.5396508991602273\n",
            "Epoch: 110. Loss: 0.5392403012881573\n",
            "Epoch: 120. Loss: 0.5389584758506205\n",
            "Epoch: 130. Loss: 0.5387626047334224\n",
            "Epoch: 140. Loss: 0.5386248951488677\n",
            "Epoch: 150. Loss: 0.5385270659313501\n",
            "Epoch: 160. Loss: 0.538456926997725\n",
            "Epoch: 170. Loss: 0.5384062382051789\n",
            "Epoch: 180. Loss: 0.5383693551033131\n",
            "Epoch: 190. Loss: 0.5383423623993709\n",
            "Epoch: 200. Loss: 0.538322512488927\n",
            "Epoch: 210. Loss: 0.5383078568138361\n",
            "Epoch: 220. Loss: 0.5382970005218739\n",
            "Epoch: 230. Loss: 0.5382889369588594\n",
            "Epoch: 240. Loss: 0.5382829345301702\n",
            "Epoch: 250. Loss: 0.5382784583812256\n",
            "Epoch: 260. Loss: 0.5382751155424004\n",
            "Epoch: 270. Loss: 0.538272616096602\n",
            "Epoch: 280. Loss: 0.5382707454267072\n",
            "Epoch: 290. Loss: 0.5382693442155068\n",
            "Epoch: 300. Loss: 0.538268293928604\n",
            "Epoch: 310. Loss: 0.5382675062125406\n",
            "Epoch: 320. Loss: 0.5382669151123614\n",
            "Epoch: 330. Loss: 0.5382664713343599\n",
            "Epoch: 340. Loss: 0.5382661380016055\n",
            "Epoch: 350. Loss: 0.5382658875047595\n",
            "Epoch: 360. Loss: 0.5382656991600313\n",
            "Epoch: 370. Loss: 0.5382655574640778\n",
            "Epoch: 380. Loss: 0.5382654507916864\n",
            "Epoch: 390. Loss: 0.5382653704226839\n",
            "Epoch: 400. Loss: 0.5382653098141201\n",
            "Epoch: 410. Loss: 0.5382652640554623\n",
            "Epoch: 420. Loss: 0.5382652294605211\n",
            "Epoch: 430. Loss: 0.5382652032616356\n",
            "Epoch: 440. Loss: 0.5382651833803954\n",
            "Epoch: 450. Loss: 0.538265168255691\n",
            "Epoch: 460. Loss: 0.5382651567147203\n",
            "Epoch: 470. Loss: 0.5382651478762022\n",
            "Epoch: 480. Loss: 0.5382651410777419\n",
            "Epoch: 490. Loss: 0.5382651358213127\n",
            "tensor(0.8030, dtype=torch.float64)\n",
            "2022-03-06 00:00:00\n",
            "Epoch: 0. Loss: 1.3826506721772907\n",
            "Epoch: 10. Loss: 0.856206424055217\n",
            "Epoch: 20. Loss: 0.7228589135220552\n",
            "Epoch: 30. Loss: 0.6600549522290902\n",
            "Epoch: 40. Loss: 0.6265198992390265\n",
            "Epoch: 50. Loss: 0.6065240441387285\n",
            "Epoch: 60. Loss: 0.5930663158262217\n",
            "Epoch: 70. Loss: 0.5833273964736004\n",
            "Epoch: 80. Loss: 0.5760467472703319\n",
            "Epoch: 90. Loss: 0.570514220565612\n",
            "Epoch: 100. Loss: 0.5662559140492134\n",
            "Epoch: 110. Loss: 0.5629331646862891\n",
            "Epoch: 120. Loss: 0.5602997930839333\n",
            "Epoch: 130. Loss: 0.5581767105992936\n",
            "Epoch: 140. Loss: 0.5564339013790686\n",
            "Epoch: 150. Loss: 0.5549770611592978\n",
            "Epoch: 160. Loss: 0.5537377181445369\n",
            "Epoch: 170. Loss: 0.55266601382322\n",
            "Epoch: 180. Loss: 0.551725480114299\n",
            "Epoch: 190. Loss: 0.5508892851188688\n",
            "Epoch: 200. Loss: 0.5501375434191766\n",
            "Epoch: 210. Loss: 0.5494553908717893\n",
            "Epoch: 220. Loss: 0.5488316056263308\n",
            "Epoch: 230. Loss: 0.5482576186163642\n",
            "Epoch: 240. Loss: 0.5477268017610736\n",
            "Epoch: 250. Loss: 0.5472339544655063\n",
            "Epoch: 260. Loss: 0.5467749320364198\n",
            "Epoch: 270. Loss: 0.5463463759419945\n",
            "Epoch: 280. Loss: 0.5459455173754295\n",
            "Epoch: 290. Loss: 0.5455700337383715\n",
            "Epoch: 300. Loss: 0.5452179434392023\n",
            "Epoch: 310. Loss: 0.5448875285075844\n",
            "Epoch: 320. Loss: 0.5445772774542523\n",
            "Epoch: 330. Loss: 0.5442858428997023\n",
            "Epoch: 340. Loss: 0.5440120099995932\n",
            "Epoch: 350. Loss: 0.5437546727785477\n",
            "Epoch: 360. Loss: 0.5435128162676298\n",
            "Epoch: 370. Loss: 0.5432855029089327\n",
            "Epoch: 380. Loss: 0.5430718621037532\n",
            "Epoch: 390. Loss: 0.5428710820817915\n",
            "Epoch: 400. Loss: 0.5426824034885135\n",
            "Epoch: 410. Loss: 0.5425051142484789\n",
            "Epoch: 420. Loss: 0.5423385453800449\n",
            "Epoch: 430. Loss: 0.5421820675230575\n",
            "Epoch: 440. Loss: 0.5420350880043611\n",
            "Epoch: 450. Loss: 0.5418970483123287\n",
            "Epoch: 460. Loss: 0.5417674218856684\n",
            "Epoch: 470. Loss: 0.5416457121467276\n",
            "Epoch: 480. Loss: 0.5415314507278589\n",
            "Epoch: 490. Loss: 0.5414241958528472\n",
            "tensor(0.9583, dtype=torch.float64)\n",
            "2022-03-13 00:00:00\n",
            "Epoch: 0. Loss: 0.8087190798970266\n",
            "Epoch: 10. Loss: 0.6735569863427218\n",
            "Epoch: 20. Loss: 0.629026372700245\n",
            "Epoch: 30. Loss: 0.6149045225909365\n",
            "Epoch: 40. Loss: 0.6079240016829166\n",
            "Epoch: 50. Loss: 0.6029758150804416\n",
            "Epoch: 60. Loss: 0.5989786235282357\n",
            "Epoch: 70. Loss: 0.5955881574581408\n",
            "Epoch: 80. Loss: 0.5926320191417612\n",
            "Epoch: 90. Loss: 0.5900025348304924\n",
            "Epoch: 100. Loss: 0.5876273053736889\n",
            "Epoch: 110. Loss: 0.5854562677202553\n",
            "Epoch: 120. Loss: 0.5834541650765568\n",
            "Epoch: 130. Loss: 0.5815957048891048\n",
            "Epoch: 140. Loss: 0.5798623532060462\n",
            "Epoch: 150. Loss: 0.5782401923224487\n",
            "Epoch: 160. Loss: 0.5767184822290379\n",
            "Epoch: 170. Loss: 0.5752886908287613\n",
            "Epoch: 180. Loss: 0.573943837488633\n",
            "Epoch: 190. Loss: 0.5726780467274819\n",
            "Epoch: 200. Loss: 0.5714862433383336\n",
            "Epoch: 210. Loss: 0.5703639430799206\n",
            "Epoch: 220. Loss: 0.5693071082144973\n",
            "Epoch: 230. Loss: 0.5683120472326915\n",
            "Epoch: 240. Loss: 0.5673753448131775\n",
            "Epoch: 250. Loss: 0.5664938125506539\n",
            "Epoch: 260. Loss: 0.5656644539979315\n",
            "Epoch: 270. Loss: 0.5648844396002792\n",
            "Epoch: 280. Loss: 0.5641510884783615\n",
            "Epoch: 290. Loss: 0.5634618549558235\n",
            "Epoch: 300. Loss: 0.5628143183719543\n",
            "Epoch: 310. Loss: 0.5622061751642495\n",
            "Epoch: 320. Loss: 0.561635232513841\n",
            "Epoch: 330. Loss: 0.5610994030615207\n",
            "Epoch: 340. Loss: 0.5605967003524376\n",
            "Epoch: 350. Loss: 0.5601252347731454\n",
            "Epoch: 360. Loss: 0.5596832098190456\n",
            "Epoch: 370. Loss: 0.55926891858267\n",
            "Epoch: 380. Loss: 0.5588807403901219\n",
            "Epoch: 390. Loss: 0.558517137538883\n",
            "Epoch: 400. Loss: 0.5581766521082234\n",
            "Epoch: 410. Loss: 0.5578579028259085\n",
            "Epoch: 420. Loss: 0.5575595819833481\n",
            "Epoch: 430. Loss: 0.5572804523969498\n",
            "Epoch: 440. Loss: 0.5570193444170194\n",
            "Epoch: 450. Loss: 0.5567751529877114\n",
            "Epoch: 460. Loss: 0.5565468347626803\n",
            "Epoch: 470. Loss: 0.5563334052815497\n",
            "Epoch: 480. Loss: 0.556133936212302\n",
            "Epoch: 490. Loss: 0.5559475526643768\n",
            "tensor(0.4193, dtype=torch.float64)\n",
            "2022-03-20 00:00:00\n",
            "Epoch: 0. Loss: 1.9193354942421266\n",
            "Epoch: 10. Loss: 0.7305379820024983\n",
            "Epoch: 20. Loss: 0.6156632723288716\n",
            "Epoch: 30. Loss: 0.5857087033946402\n",
            "Epoch: 40. Loss: 0.5761591718834158\n",
            "Epoch: 50. Loss: 0.5711550178024106\n",
            "Epoch: 60. Loss: 0.5677812077688383\n",
            "Epoch: 70. Loss: 0.5653308769077292\n",
            "Epoch: 80. Loss: 0.5634980700695646\n",
            "Epoch: 90. Loss: 0.5620984678630238\n",
            "Epoch: 100. Loss: 0.5610089328601375\n",
            "Epoch: 110. Loss: 0.5601445716750099\n",
            "Epoch: 120. Loss: 0.5594459778850631\n",
            "Epoch: 130. Loss: 0.5588711073863929\n",
            "Epoch: 140. Loss: 0.5583898935160979\n",
            "Epoch: 150. Loss: 0.5579806191931247\n",
            "Epoch: 160. Loss: 0.5576274444558642\n",
            "Epoch: 170. Loss: 0.5573187036244711\n",
            "Epoch: 180. Loss: 0.557045720740081\n",
            "Epoch: 190. Loss: 0.5568019777839252\n",
            "Epoch: 200. Loss: 0.5565825255734845\n",
            "Epoch: 210. Loss: 0.556383563288772\n",
            "Epoch: 220. Loss: 0.5562021362790472\n",
            "Epoch: 230. Loss: 0.5560359175420091\n",
            "Epoch: 240. Loss: 0.5558830488425942\n",
            "Epoch: 250. Loss: 0.5557420246235856\n",
            "Epoch: 260. Loss: 0.5556116067960523\n",
            "Epoch: 270. Loss: 0.5554907619237636\n",
            "Epoch: 280. Loss: 0.5553786147167137\n",
            "Epoch: 290. Loss: 0.5552744134460169\n",
            "Epoch: 300. Loss: 0.5551775041011171\n",
            "Epoch: 310. Loss: 0.5550873109767459\n",
            "Epoch: 320. Loss: 0.5550033220017195\n",
            "Epoch: 330. Loss: 0.5549250775741034\n",
            "Epoch: 340. Loss: 0.5548521619962845\n",
            "Epoch: 350. Loss: 0.5547841968435115\n",
            "Epoch: 360. Loss: 0.5547208357750449\n",
            "Epoch: 370. Loss: 0.5546617604257924\n",
            "Epoch: 380. Loss: 0.5546066771108427\n",
            "Epoch: 390. Loss: 0.5545553141448663\n",
            "Epoch: 400. Loss: 0.5545074196295712\n",
            "Epoch: 410. Loss: 0.5544627596001654\n",
            "Epoch: 420. Loss: 0.5544211164496583\n",
            "Epoch: 430. Loss: 0.5543822875704096\n",
            "Epoch: 440. Loss: 0.5543460841675657\n",
            "Epoch: 450. Loss: 0.5543123302102746\n",
            "Epoch: 460. Loss: 0.5542808614949191\n",
            "Epoch: 470. Loss: 0.5542515248007878\n",
            "Epoch: 480. Loss: 0.5542241771231956\n",
            "Epoch: 490. Loss: 0.5541986849724854\n",
            "tensor(0.7570, dtype=torch.float64)\n",
            "2022-03-27 00:00:00\n",
            "Epoch: 0. Loss: 2.3332045710527427\n",
            "Epoch: 10. Loss: 0.899301051769287\n",
            "Epoch: 20. Loss: 0.6878757235449943\n",
            "Epoch: 30. Loss: 0.619894809763485\n",
            "Epoch: 40. Loss: 0.5932720301175013\n",
            "Epoch: 50. Loss: 0.5796113421644759\n",
            "Epoch: 60. Loss: 0.5720669689182866\n",
            "Epoch: 70. Loss: 0.567792173115983\n",
            "Epoch: 80. Loss: 0.565304989803752\n",
            "Epoch: 90. Loss: 0.5638047249666699\n",
            "Epoch: 100. Loss: 0.5628565475496973\n",
            "Epoch: 110. Loss: 0.5622232516299721\n",
            "Epoch: 120. Loss: 0.5617743366730731\n",
            "Epoch: 130. Loss: 0.5614370936671786\n",
            "Epoch: 140. Loss: 0.5611703327480126\n",
            "Epoch: 150. Loss: 0.5609502279861054\n",
            "Epoch: 160. Loss: 0.560762640515312\n",
            "Epoch: 170. Loss: 0.5605989180676487\n",
            "Epoch: 180. Loss: 0.5604535721690842\n",
            "Epoch: 190. Loss: 0.5603229777752142\n",
            "Epoch: 200. Loss: 0.5602046348696812\n",
            "Epoch: 210. Loss: 0.5600967421972903\n",
            "Epoch: 220. Loss: 0.5599979464522019\n",
            "Epoch: 230. Loss: 0.5599071914555817\n",
            "Epoch: 240. Loss: 0.55982362522571\n",
            "Epoch: 250. Loss: 0.5597465411806986\n",
            "Epoch: 260. Loss: 0.5596753398807108\n",
            "Epoch: 270. Loss: 0.5596095034103908\n",
            "Epoch: 280. Loss: 0.5595485777283352\n",
            "Epoch: 290. Loss: 0.5594921601633469\n",
            "Epoch: 300. Loss: 0.559439890318254\n",
            "Epoch: 310. Loss: 0.5593914432841947\n",
            "Epoch: 320. Loss: 0.5593465244572948\n",
            "Epoch: 330. Loss: 0.5593048654904472\n",
            "Epoch: 340. Loss: 0.559266221065235\n",
            "Epoch: 350. Loss: 0.5592303662675685\n",
            "Epoch: 360. Loss: 0.5591970944156739\n",
            "Epoch: 370. Loss: 0.5591662152328939\n",
            "Epoch: 380. Loss: 0.5591375532877919\n",
            "Epoch: 390. Loss: 0.5591109466449491\n",
            "Epoch: 400. Loss: 0.5590862456845846\n",
            "Epoch: 410. Loss: 0.559063312059643\n",
            "Epoch: 420. Loss: 0.5590420177665731\n",
            "Epoch: 430. Loss: 0.559022244311538\n",
            "Epoch: 440. Loss: 0.5590038819578396\n",
            "Epoch: 450. Loss: 0.5589868290433386\n",
            "Epoch: 460. Loss: 0.5589709913588853\n",
            "Epoch: 470. Loss: 0.558956281580458\n",
            "Epoch: 480. Loss: 0.5589426187489946\n",
            "Epoch: 490. Loss: 0.5589299277928826\n",
            "tensor(0.7660, dtype=torch.float64)\n",
            "2022-04-03 00:00:00\n",
            "Epoch: 0. Loss: 2.979111758961142\n",
            "Epoch: 10. Loss: 1.3058670729511686\n",
            "Epoch: 20. Loss: 0.8525104819140314\n",
            "Epoch: 30. Loss: 0.6876076409015144\n",
            "Epoch: 40. Loss: 0.6297645706685191\n",
            "Epoch: 50. Loss: 0.6064810747051772\n",
            "Epoch: 60. Loss: 0.5951132985645194\n",
            "Epoch: 70. Loss: 0.5887103166577663\n",
            "Epoch: 80. Loss: 0.5847272082263519\n",
            "Epoch: 90. Loss: 0.5820349748737045\n",
            "Epoch: 100. Loss: 0.5800722817504915\n",
            "Epoch: 110. Loss: 0.5785440719758117\n",
            "Epoch: 120. Loss: 0.5772900358535491\n",
            "Epoch: 130. Loss: 0.5762204957730579\n",
            "Epoch: 140. Loss: 0.5752836017457394\n",
            "Epoch: 150. Loss: 0.5744481445858745\n",
            "Epoch: 160. Loss: 0.5736944061569594\n",
            "Epoch: 170. Loss: 0.5730092181128575\n",
            "Epoch: 180. Loss: 0.5723832538162217\n",
            "Epoch: 190. Loss: 0.5718095198992779\n",
            "Epoch: 200. Loss: 0.5712825002753957\n",
            "Epoch: 210. Loss: 0.5707976596597942\n",
            "Epoch: 220. Loss: 0.5703511480323783\n",
            "Epoch: 230. Loss: 0.5699396192293837\n",
            "Epoch: 240. Loss: 0.5695601155535112\n",
            "Epoch: 250. Loss: 0.5692099913829107\n",
            "Epoch: 260. Loss: 0.5688868603772891\n",
            "Epoch: 270. Loss: 0.5685885573548384\n",
            "Epoch: 280. Loss: 0.5683131095700916\n",
            "Epoch: 290. Loss: 0.5680587142174887\n",
            "Epoch: 300. Loss: 0.5678237202047166\n",
            "Epoch: 310. Loss: 0.5676066129620383\n",
            "Epoch: 320. Loss: 0.567406001489535\n",
            "Epoch: 330. Loss: 0.5672206071122243\n",
            "Epoch: 340. Loss: 0.5670492535812142\n",
            "Epoch: 350. Loss: 0.5668908582667687\n",
            "Epoch: 360. Loss: 0.5667444242595444\n",
            "Epoch: 370. Loss: 0.5666090332432117\n",
            "Epoch: 380. Loss: 0.5664838390336524\n",
            "Epoch: 390. Loss: 0.5663680617021787\n",
            "Epoch: 400. Loss: 0.5662609822160479\n",
            "Epoch: 410. Loss: 0.5661619375410776\n",
            "Epoch: 420. Loss: 0.5660703161597712\n",
            "Epoch: 430. Loss: 0.5659855539649364\n",
            "Epoch: 440. Loss: 0.5659071304939375\n",
            "Epoch: 450. Loss: 0.5658345654728598\n",
            "Epoch: 460. Loss: 0.5657674156432673\n",
            "Epoch: 470. Loss: 0.5657052718470735\n",
            "Epoch: 480. Loss: 0.565647756347482\n",
            "Epoch: 490. Loss: 0.5655945203660476\n",
            "tensor(0.6804, dtype=torch.float64)\n",
            "2022-04-10 00:00:00\n",
            "Epoch: 0. Loss: 3.480712425421853\n",
            "Epoch: 10. Loss: 0.9016901424575512\n",
            "Epoch: 20. Loss: 0.7547032006143626\n",
            "Epoch: 30. Loss: 0.6985633884558204\n",
            "Epoch: 40. Loss: 0.660908219091014\n",
            "Epoch: 50. Loss: 0.6333866588017208\n",
            "Epoch: 60. Loss: 0.6132020392382502\n",
            "Epoch: 70. Loss: 0.5986834965409275\n",
            "Epoch: 80. Loss: 0.588454532621572\n",
            "Epoch: 90. Loss: 0.5813499293545917\n",
            "Epoch: 100. Loss: 0.5764472384693123\n",
            "Epoch: 110. Loss: 0.5730631301492166\n",
            "Epoch: 120. Loss: 0.5707140814562129\n",
            "Epoch: 130. Loss: 0.5690671052824976\n",
            "Epoch: 140. Loss: 0.5678961560685654\n",
            "Epoch: 150. Loss: 0.5670488081289625\n",
            "Epoch: 160. Loss: 0.5664225189467058\n",
            "Epoch: 170. Loss: 0.5659482984292272\n",
            "Epoch: 180. Loss: 0.5655796605413788\n",
            "Epoch: 190. Loss: 0.5652852049956206\n",
            "Epoch: 200. Loss: 0.5650436505770169\n",
            "Epoch: 210. Loss: 0.5648405117039814\n",
            "Epoch: 220. Loss: 0.5646658733808261\n",
            "Epoch: 230. Loss: 0.5645129000769661\n",
            "Epoch: 240. Loss: 0.5643768353416312\n",
            "Epoch: 250. Loss: 0.5642543299020498\n",
            "Epoch: 260. Loss: 0.5641429898915086\n",
            "Epoch: 270. Loss: 0.5640410727525917\n",
            "Epoch: 280. Loss: 0.5639472823013989\n",
            "Epoch: 290. Loss: 0.5638606304296911\n",
            "Epoch: 300. Loss: 0.5637803436201639\n",
            "Epoch: 310. Loss: 0.5637057996173025\n",
            "Epoch: 320. Loss: 0.5636364844035062\n",
            "Epoch: 330. Loss: 0.5635719628574443\n",
            "Epoch: 340. Loss: 0.5635118586397222\n",
            "Epoch: 350. Loss: 0.5634558403082557\n",
            "Epoch: 360. Loss: 0.563403611645688\n",
            "Epoch: 370. Loss: 0.5633549048403005\n",
            "Epoch: 380. Loss: 0.5633094756052957\n",
            "Epoch: 390. Loss: 0.5632670996196892\n",
            "Epoch: 400. Loss: 0.5632275698748602\n",
            "Epoch: 410. Loss: 0.5631906946459401\n",
            "Epoch: 420. Loss: 0.5631562958982279\n",
            "Epoch: 430. Loss: 0.5631242080000706\n",
            "Epoch: 440. Loss: 0.5630942766549324\n",
            "Epoch: 450. Loss: 0.563066357993178\n",
            "Epoch: 460. Loss: 0.563040317782859\n",
            "Epoch: 470. Loss: 0.5630160307314419\n",
            "Epoch: 480. Loss: 0.5629933798589766\n",
            "Epoch: 490. Loss: 0.5629722559289885\n",
            "tensor(0.6034, dtype=torch.float64)\n",
            "2022-04-17 00:00:00\n",
            "Epoch: 0. Loss: 1.15207268667869\n",
            "Epoch: 10. Loss: 0.7412308923170983\n",
            "Epoch: 20. Loss: 0.6572949372151757\n",
            "Epoch: 30. Loss: 0.6239722193682489\n",
            "Epoch: 40. Loss: 0.6073238110992998\n",
            "Epoch: 50. Loss: 0.5975092965637114\n",
            "Epoch: 60. Loss: 0.5912665785224402\n",
            "Epoch: 70. Loss: 0.5871060788685742\n",
            "Epoch: 80. Loss: 0.58421394907652\n",
            "Epoch: 90. Loss: 0.5821168527198536\n",
            "Epoch: 100. Loss: 0.5805323269926341\n",
            "Epoch: 110. Loss: 0.57928836156098\n",
            "Epoch: 120. Loss: 0.5782779217963235\n",
            "Epoch: 130. Loss: 0.5774328710973177\n",
            "Epoch: 140. Loss: 0.5767088332612355\n",
            "Epoch: 150. Loss: 0.5760762538133203\n",
            "Epoch: 160. Loss: 0.5755150116897186\n",
            "Epoch: 170. Loss: 0.5750111004156637\n",
            "Epoch: 180. Loss: 0.5745545420204683\n",
            "Epoch: 190. Loss: 0.5741380521403358\n",
            "Epoch: 200. Loss: 0.5737561728168327\n",
            "Epoch: 210. Loss: 0.573404702043753\n",
            "Epoch: 220. Loss: 0.5730803145901172\n",
            "Epoch: 230. Loss: 0.5727803076861714\n",
            "Epoch: 240. Loss: 0.5725024290330568\n",
            "Epoch: 250. Loss: 0.5722447595119482\n",
            "Epoch: 260. Loss: 0.5720056324618297\n",
            "Epoch: 270. Loss: 0.5717835775296082\n",
            "Epoch: 280. Loss: 0.5715772811074993\n",
            "Epoch: 290. Loss: 0.5713855580190346\n",
            "Epoch: 300. Loss: 0.5712073308725499\n",
            "Epoch: 310. Loss: 0.5710416146737051\n",
            "Epoch: 320. Loss: 0.5708875050736436\n",
            "Epoch: 330. Loss: 0.5707441691561278\n",
            "Epoch: 340. Loss: 0.5706108380209308\n",
            "Epoch: 350. Loss: 0.5704868006588414\n",
            "Epoch: 360. Loss: 0.5703713987739072\n",
            "Epoch: 370. Loss: 0.5702640223165036\n",
            "Epoch: 380. Loss: 0.5701641055636131\n",
            "Epoch: 390. Loss: 0.5700711236318465\n",
            "Epoch: 400. Loss: 0.5699845893419903\n",
            "Epoch: 410. Loss: 0.569904050376414\n",
            "Epoch: 420. Loss: 0.569829086686062\n",
            "Epoch: 430. Loss: 0.5697593081142879\n",
            "Epoch: 440. Loss: 0.5696943522120997\n",
            "Epoch: 450. Loss: 0.5696338822244971\n",
            "Epoch: 460. Loss: 0.5695775852312162\n",
            "Epoch: 470. Loss: 0.5695251704278521\n",
            "Epoch: 480. Loss: 0.5694763675352806\n",
            "Epoch: 490. Loss: 0.5694309253268137\n",
            "tensor(0.5224, dtype=torch.float64)\n",
            "2022-04-24 00:00:00\n",
            "Epoch: 0. Loss: 1.1919295715314244\n",
            "Epoch: 10. Loss: 0.8802070866612672\n",
            "Epoch: 20. Loss: 0.7085322885682632\n",
            "Epoch: 30. Loss: 0.6338190654992192\n",
            "Epoch: 40. Loss: 0.6004633701864905\n",
            "Epoch: 50. Loss: 0.5840446317372775\n",
            "Epoch: 60. Loss: 0.5758383327389123\n",
            "Epoch: 70. Loss: 0.5717262569051037\n",
            "Epoch: 80. Loss: 0.5696059749550053\n",
            "Epoch: 90. Loss: 0.5684398060846146\n",
            "Epoch: 100. Loss: 0.5677319053620116\n",
            "Epoch: 110. Loss: 0.5672488757691733\n",
            "Epoch: 120. Loss: 0.566881138968714\n",
            "Epoch: 130. Loss: 0.5665769398908008\n",
            "Epoch: 140. Loss: 0.5663114760598916\n",
            "Epoch: 150. Loss: 0.5660725378947976\n",
            "Epoch: 160. Loss: 0.5658538314413984\n",
            "Epoch: 170. Loss: 0.5656518676085306\n",
            "Epoch: 180. Loss: 0.5654645085846801\n",
            "Epoch: 190. Loss: 0.5652902857951732\n",
            "Epoch: 200. Loss: 0.565128077917693\n",
            "Epoch: 210. Loss: 0.5649769573745524\n",
            "Epoch: 220. Loss: 0.5648361159135786\n",
            "Epoch: 230. Loss: 0.5647048274816353\n",
            "Epoch: 240. Loss: 0.5645824288024569\n",
            "Epoch: 250. Loss: 0.5644683084525153\n",
            "Epoch: 260. Loss: 0.5643619000916128\n",
            "Epoch: 270. Loss: 0.5642626777877613\n",
            "Epoch: 280. Loss: 0.5641701524505585\n",
            "Epoch: 290. Loss: 0.5640838688950768\n",
            "Epoch: 300. Loss: 0.5640034032995139\n",
            "Epoch: 310. Loss: 0.563928360935354\n",
            "Epoch: 320. Loss: 0.5638583741047248\n",
            "Epoch: 330. Loss: 0.563793100247216\n",
            "Epoch: 340. Loss: 0.5637322201923447\n",
            "Epoch: 350. Loss: 0.5636754365411829\n",
            "Epoch: 360. Loss: 0.5636224721646899\n",
            "Epoch: 370. Loss: 0.5635730688086895\n",
            "Epoch: 380. Loss: 0.5635269857969434\n",
            "Epoch: 390. Loss: 0.5634839988248365\n",
            "Epoch: 400. Loss: 0.5634438988369705\n",
            "Epoch: 410. Loss: 0.5634064909825895\n",
            "Epoch: 420. Loss: 0.5633715936432782\n",
            "Epoch: 430. Loss: 0.5633390375278207\n",
            "Epoch: 440. Loss: 0.5633086648295016\n",
            "Epoch: 450. Loss: 0.5632803284414831\n",
            "Epoch: 460. Loss: 0.5632538912262086\n",
            "Epoch: 470. Loss: 0.5632292253350786\n",
            "Epoch: 480. Loss: 0.563206211574907\n",
            "Epoch: 490. Loss: 0.5631847388179164\n",
            "tensor(0.3248, dtype=torch.float64)\n",
            "2022-05-01 00:00:00\n",
            "Epoch: 0. Loss: 1.912095387742027\n",
            "Epoch: 10. Loss: 1.1846673654876527\n",
            "Epoch: 20. Loss: 1.0057594328352746\n",
            "Epoch: 30. Loss: 0.8731039089103688\n",
            "Epoch: 40. Loss: 0.7655397993208717\n",
            "Epoch: 50. Loss: 0.6846879478786146\n",
            "Epoch: 60. Loss: 0.6309825880815442\n",
            "Epoch: 70. Loss: 0.5997688375035297\n",
            "Epoch: 80. Loss: 0.5834088699719419\n",
            "Epoch: 90. Loss: 0.5752727910309645\n",
            "Epoch: 100. Loss: 0.5712489722286587\n",
            "Epoch: 110. Loss: 0.5691991336459261\n",
            "Epoch: 120. Loss: 0.56809374158003\n",
            "Epoch: 130. Loss: 0.5674484698880174\n",
            "Epoch: 140. Loss: 0.5670347635319488\n",
            "Epoch: 150. Loss: 0.5667430098598992\n",
            "Epoch: 160. Loss: 0.5665193539642575\n",
            "Epoch: 170. Loss: 0.5663365008702418\n",
            "Epoch: 180. Loss: 0.5661800765100924\n",
            "Epoch: 190. Loss: 0.5660421556733611\n",
            "Epoch: 200. Loss: 0.5659181313383168\n",
            "Epoch: 210. Loss: 0.5658051654395523\n",
            "Epoch: 220. Loss: 0.5657014013957399\n",
            "Epoch: 230. Loss: 0.5656055512116741\n",
            "Epoch: 240. Loss: 0.5655166713007037\n",
            "Epoch: 250. Loss: 0.5654340361306565\n",
            "Epoch: 260. Loss: 0.565357064240413\n",
            "Epoch: 270. Loss: 0.5652852732942063\n",
            "Epoch: 280. Loss: 0.5652182518273514\n",
            "Epoch: 290. Loss: 0.5651556409275713\n",
            "Epoch: 300. Loss: 0.5650971220227278\n",
            "Epoch: 310. Loss: 0.5650424085278696\n",
            "Epoch: 320. Loss: 0.5649912399897686\n",
            "Epoch: 330. Loss: 0.5649433778800075\n",
            "Epoch: 340. Loss: 0.5648986024948088\n",
            "Epoch: 350. Loss: 0.564856710609195\n",
            "Epoch: 360. Loss: 0.564817513652806\n",
            "Epoch: 370. Loss: 0.5647808362519208\n",
            "Epoch: 380. Loss: 0.5647465150328117\n",
            "Epoch: 390. Loss: 0.5647143976150842\n",
            "Epoch: 400. Loss: 0.5646843417460718\n",
            "Epoch: 410. Loss: 0.5646562145424463\n",
            "Epoch: 420. Loss: 0.5646298918154119\n",
            "Epoch: 430. Loss: 0.5646052574628014\n",
            "Epoch: 440. Loss: 0.5645822029161177\n",
            "Epoch: 450. Loss: 0.5645606266338244\n",
            "Epoch: 460. Loss: 0.5645404336344193\n",
            "Epoch: 470. Loss: 0.5645215350643767\n",
            "Epoch: 480. Loss: 0.5645038477971321\n",
            "Epoch: 490. Loss: 0.5644872940600432\n",
            "tensor(0.4333, dtype=torch.float64)\n",
            "2022-05-08 00:00:00\n",
            "Epoch: 0. Loss: 1.8845369958361138\n",
            "Epoch: 10. Loss: 1.4230523944867945\n",
            "Epoch: 20. Loss: 1.050615153608727\n",
            "Epoch: 30. Loss: 0.7957523009133434\n",
            "Epoch: 40. Loss: 0.6650421886091706\n",
            "Epoch: 50. Loss: 0.6166944246670001\n",
            "Epoch: 60. Loss: 0.5978316054965855\n",
            "Epoch: 70. Loss: 0.5875428407683855\n",
            "Epoch: 80. Loss: 0.5809491046874746\n",
            "Epoch: 90. Loss: 0.5765727565167591\n",
            "Epoch: 100. Loss: 0.5736482684563455\n",
            "Epoch: 110. Loss: 0.5716880350358854\n",
            "Epoch: 120. Loss: 0.5703701249879511\n",
            "Epoch: 130. Loss: 0.5694810456652903\n",
            "Epoch: 140. Loss: 0.5688790029730972\n",
            "Epoch: 150. Loss: 0.5684696244725379\n",
            "Epoch: 160. Loss: 0.5681899346780757\n",
            "Epoch: 170. Loss: 0.5679977910917371\n",
            "Epoch: 180. Loss: 0.5678649137047892\n",
            "Epoch: 190. Loss: 0.567772274703268\n",
            "Epoch: 200. Loss: 0.567707039844952\n",
            "Epoch: 210. Loss: 0.5676605325690082\n",
            "Epoch: 220. Loss: 0.5676268744531056\n",
            "Epoch: 230. Loss: 0.5676020745558453\n",
            "Epoch: 240. Loss: 0.5675834177389781\n",
            "Epoch: 250. Loss: 0.5675690527916994\n",
            "Epoch: 260. Loss: 0.5675577144868217\n",
            "Epoch: 270. Loss: 0.5675485356598554\n",
            "Epoch: 280. Loss: 0.5675409199429066\n",
            "Epoch: 290. Loss: 0.5675344554515535\n",
            "Epoch: 300. Loss: 0.5675288561723424\n",
            "Epoch: 310. Loss: 0.5675239221160424\n",
            "Epoch: 320. Loss: 0.5675195122006242\n",
            "Epoch: 330. Loss: 0.5675155257792334\n",
            "Epoch: 340. Loss: 0.5675118900448816\n",
            "Epoch: 350. Loss: 0.5675085514334189\n",
            "Epoch: 360. Loss: 0.5675054697488148\n",
            "Epoch: 370. Loss: 0.5675026141432508\n",
            "Epoch: 380. Loss: 0.5674999603617762\n",
            "Epoch: 390. Loss: 0.5674974888496807\n",
            "Epoch: 400. Loss: 0.5674951834488302\n",
            "Epoch: 410. Loss: 0.5674930304964031\n",
            "Epoch: 420. Loss: 0.567491018198809\n",
            "Epoch: 430. Loss: 0.5674891361940176\n",
            "Epoch: 440. Loss: 0.5674873752430787\n",
            "Epoch: 450. Loss: 0.5674857270104009\n",
            "Epoch: 460. Loss: 0.5674841839051737\n",
            "Epoch: 470. Loss: 0.5674827389650519\n",
            "Epoch: 480. Loss: 0.5674813857691896\n",
            "Epoch: 490. Loss: 0.5674801183717816\n",
            "tensor(0.6142, dtype=torch.float64)\n",
            "2022-05-15 00:00:00\n",
            "Epoch: 0. Loss: 4.553841839388374\n",
            "Epoch: 10. Loss: 0.8321256119872295\n",
            "Epoch: 20. Loss: 0.6681161101493046\n",
            "Epoch: 30. Loss: 0.6322238493957023\n",
            "Epoch: 40. Loss: 0.6164323683011349\n",
            "Epoch: 50. Loss: 0.6066226169237724\n",
            "Epoch: 60. Loss: 0.5999179553214138\n",
            "Epoch: 70. Loss: 0.595211590917837\n",
            "Epoch: 80. Loss: 0.5918471784282666\n",
            "Epoch: 90. Loss: 0.5893943895879612\n",
            "Epoch: 100. Loss: 0.5875666700644028\n",
            "Epoch: 110. Loss: 0.5861718896928285\n",
            "Epoch: 120. Loss: 0.5850802992045546\n",
            "Epoch: 130. Loss: 0.5842036669852859\n",
            "Epoch: 140. Loss: 0.5834816254132729\n",
            "Epoch: 150. Loss: 0.5828726428279148\n",
            "Epoch: 160. Loss: 0.582347988310818\n",
            "Epoch: 170. Loss: 0.5818876612099195\n",
            "Epoch: 180. Loss: 0.5814776291933664\n",
            "Epoch: 190. Loss: 0.5811079476152713\n",
            "Epoch: 200. Loss: 0.5807714768492858\n",
            "Epoch: 210. Loss: 0.5804630070028645\n",
            "Epoch: 220. Loss: 0.5801786606455658\n",
            "Epoch: 230. Loss: 0.5799154853038218\n",
            "Epoch: 240. Loss: 0.5796711754024171\n",
            "Epoch: 250. Loss: 0.5794438824158121\n",
            "Epoch: 260. Loss: 0.5792320850634916\n",
            "Epoch: 270. Loss: 0.5790345003384164\n",
            "Epoch: 280. Loss: 0.5788500222861039\n",
            "Epoch: 290. Loss: 0.5786776796389244\n",
            "Epoch: 300. Loss: 0.5785166062654721\n",
            "Epoch: 310. Loss: 0.5783660203383005\n",
            "Epoch: 320. Loss: 0.5782252094437996\n",
            "Epoch: 330. Loss: 0.5780935197538589\n",
            "Epoch: 340. Loss: 0.5779703479859368\n",
            "Epoch: 350. Loss: 0.5778551352889866\n",
            "Epoch: 360. Loss: 0.5777473624705265\n",
            "Epoch: 370. Loss: 0.5776465461679268\n",
            "Epoch: 380. Loss: 0.5775522356938753\n",
            "Epoch: 390. Loss: 0.5774640103717229\n",
            "Epoch: 400. Loss: 0.5773814772343477\n",
            "Epoch: 410. Loss: 0.5773042689993672\n",
            "Epoch: 420. Loss: 0.5772320422600593\n",
            "Epoch: 430. Loss: 0.57716447584934\n",
            "Epoch: 440. Loss: 0.5771012693463791\n",
            "Epoch: 450. Loss: 0.5770421417037701\n",
            "Epoch: 460. Loss: 0.5769868299788888\n",
            "Epoch: 470. Loss: 0.5769350881570067\n",
            "Epoch: 480. Loss: 0.576886686056468\n",
            "Epoch: 490. Loss: 0.5768414083081588\n",
            "tensor(0.6543, dtype=torch.float64)\n",
            "2022-05-22 00:00:00\n",
            "Epoch: 0. Loss: 1.93066511774756\n",
            "Epoch: 10. Loss: 1.122011655758949\n",
            "Epoch: 20. Loss: 0.8168250372793259\n",
            "Epoch: 30. Loss: 0.7110351307541135\n",
            "Epoch: 40. Loss: 0.6556656857724215\n",
            "Epoch: 50. Loss: 0.6225273700263869\n",
            "Epoch: 60. Loss: 0.6036171973251421\n",
            "Epoch: 70. Loss: 0.5933016917695878\n",
            "Epoch: 80. Loss: 0.5877218803867588\n",
            "Epoch: 90. Loss: 0.5846387089001212\n",
            "Epoch: 100. Loss: 0.5828719175359348\n",
            "Epoch: 110. Loss: 0.5818187513694397\n",
            "Epoch: 120. Loss: 0.5811685523485786\n",
            "Epoch: 130. Loss: 0.5807558851314837\n",
            "Epoch: 140. Loss: 0.5804886832966901\n",
            "Epoch: 150. Loss: 0.5803133013185401\n",
            "Epoch: 160. Loss: 0.580197167018926\n",
            "Epoch: 170. Loss: 0.5801198412051045\n",
            "Epoch: 180. Loss: 0.5800681847882196\n",
            "Epoch: 190. Loss: 0.5800336105376497\n",
            "Epoch: 200. Loss: 0.5800104454066711\n",
            "Epoch: 210. Loss: 0.579994916340486\n",
            "Epoch: 220. Loss: 0.5799845038426845\n",
            "Epoch: 230. Loss: 0.5799775216240078\n",
            "Epoch: 240. Loss: 0.5799728396623228\n",
            "Epoch: 250. Loss: 0.5799697002499689\n",
            "Epoch: 260. Loss: 0.5799675952091712\n",
            "Epoch: 270. Loss: 0.5799661837083676\n",
            "Epoch: 280. Loss: 0.5799652371728863\n",
            "Epoch: 290. Loss: 0.5799646023317131\n",
            "Epoch: 300. Loss: 0.5799641764235208\n",
            "Epoch: 310. Loss: 0.5799638905609708\n",
            "Epoch: 320. Loss: 0.5799636985702359\n",
            "Epoch: 330. Loss: 0.5799635695050608\n",
            "Epoch: 340. Loss: 0.5799634826265053\n",
            "Epoch: 350. Loss: 0.579963424036878\n",
            "Epoch: 360. Loss: 0.5799633844231871\n",
            "Epoch: 370. Loss: 0.5799633575446257\n",
            "Epoch: 380. Loss: 0.5799633392188966\n",
            "Epoch: 390. Loss: 0.5799633266429323\n",
            "Epoch: 400. Loss: 0.5799633179377397\n",
            "Epoch: 410. Loss: 0.5799633118434433\n",
            "Epoch: 420. Loss: 0.5799633075149762\n",
            "Epoch: 430. Loss: 0.5799633043852033\n",
            "Epoch: 440. Loss: 0.5799633020732279\n",
            "Epoch: 450. Loss: 0.5799633003229632\n",
            "Epoch: 460. Loss: 0.579963298961984\n",
            "Epoch: 470. Loss: 0.5799632978739582\n",
            "Epoch: 480. Loss: 0.5799632969801786\n",
            "Epoch: 490. Loss: 0.5799632962271898\n",
            "tensor(0.5996, dtype=torch.float64)\n",
            "2022-05-29 00:00:00\n",
            "Epoch: 0. Loss: 3.0905831293559265\n",
            "Epoch: 10. Loss: 1.072797698676542\n",
            "Epoch: 20. Loss: 0.7830095466486585\n",
            "Epoch: 30. Loss: 0.6895187090850954\n",
            "Epoch: 40. Loss: 0.6411312691141743\n",
            "Epoch: 50. Loss: 0.614541846021935\n",
            "Epoch: 60. Loss: 0.6007440772701235\n",
            "Epoch: 70. Loss: 0.5938021767174616\n",
            "Epoch: 80. Loss: 0.590259663005805\n",
            "Epoch: 90. Loss: 0.5883608350859316\n",
            "Epoch: 100. Loss: 0.5872684456678874\n",
            "Epoch: 110. Loss: 0.5865878868155112\n",
            "Epoch: 120. Loss: 0.5861295571755681\n",
            "Epoch: 130. Loss: 0.5857987764251279\n",
            "Epoch: 140. Loss: 0.5855458356289737\n",
            "Epoch: 150. Loss: 0.5853431710276122\n",
            "Epoch: 160. Loss: 0.5851746690152663\n",
            "Epoch: 170. Loss: 0.5850304583404927\n",
            "Epoch: 180. Loss: 0.5849042487333708\n",
            "Epoch: 190. Loss: 0.5847918969824869\n",
            "Epoch: 200. Loss: 0.5846905936645983\n",
            "Epoch: 210. Loss: 0.5845983808205272\n",
            "Epoch: 220. Loss: 0.5845138555780022\n",
            "Epoch: 230. Loss: 0.5844359831782396\n",
            "Epoch: 240. Loss: 0.5843639767841012\n",
            "Epoch: 250. Loss: 0.5842972191757678\n",
            "Epoch: 260. Loss: 0.5842352112228546\n",
            "Epoch: 270. Loss: 0.5841775376939897\n",
            "Epoch: 280. Loss: 0.5841238443891501\n",
            "Epoch: 290. Loss: 0.5840738227108545\n",
            "Epoch: 300. Loss: 0.584027199144612\n",
            "Epoch: 310. Loss: 0.5839837279919932\n",
            "Epoch: 320. Loss: 0.5839431862674934\n",
            "Epoch: 330. Loss: 0.5839053700417718\n",
            "Epoch: 340. Loss: 0.5838700917576221\n",
            "Epoch: 350. Loss: 0.5838371782054063\n",
            "Epoch: 360. Loss: 0.5838064689503001\n",
            "Epoch: 370. Loss: 0.5837778150733597\n",
            "Epoch: 380. Loss: 0.5837510781343784\n",
            "Epoch: 390. Loss: 0.5837261292948637\n",
            "Epoch: 400. Loss: 0.5837028485595469\n",
            "Epoch: 410. Loss: 0.5836811241081306\n",
            "Epoch: 420. Loss: 0.5836608516978047\n",
            "Epoch: 430. Loss: 0.5836419341229375\n",
            "Epoch: 440. Loss: 0.5836242807222647\n",
            "Epoch: 450. Loss: 0.5836078069265452\n",
            "Epoch: 460. Loss: 0.5835924338414229\n",
            "Epoch: 470. Loss: 0.5835780878614615\n",
            "Epoch: 480. Loss: 0.5835647003121526\n",
            "Epoch: 490. Loss: 0.5835522071172958\n",
            "tensor(0.7810, dtype=torch.float64)\n",
            "2022-06-05 00:00:00\n",
            "Epoch: 0. Loss: 1.111649420014169\n",
            "Epoch: 10. Loss: 0.7654667913092885\n",
            "Epoch: 20. Loss: 0.6770941539048305\n",
            "Epoch: 30. Loss: 0.6337336712632252\n",
            "Epoch: 40. Loss: 0.612819076146386\n",
            "Epoch: 50. Loss: 0.6031691453066991\n",
            "Epoch: 60. Loss: 0.5986560542993081\n",
            "Epoch: 70. Loss: 0.59641479438496\n",
            "Epoch: 80. Loss: 0.5951984983805609\n",
            "Epoch: 90. Loss: 0.5944677664592364\n",
            "Epoch: 100. Loss: 0.5939827468059207\n",
            "Epoch: 110. Loss: 0.593631783841482\n",
            "Epoch: 120. Loss: 0.5933598538362802\n",
            "Epoch: 130. Loss: 0.593138092791822\n",
            "Epoch: 140. Loss: 0.5929503829436845\n",
            "Epoch: 150. Loss: 0.5927871859443926\n",
            "Epoch: 160. Loss: 0.5926425583059844\n",
            "Epoch: 170. Loss: 0.5925126243231996\n",
            "Epoch: 180. Loss: 0.5923947505497098\n",
            "Epoch: 190. Loss: 0.5922870768289283\n",
            "Epoch: 200. Loss: 0.5921882386567381\n",
            "Epoch: 210. Loss: 0.5920971974198811\n",
            "Epoch: 220. Loss: 0.5920131340179036\n",
            "Epoch: 230. Loss: 0.5919353809393681\n",
            "Epoch: 240. Loss: 0.5918633782260952\n",
            "Epoch: 250. Loss: 0.5917966445310082\n",
            "Epoch: 260. Loss: 0.5917347578303579\n",
            "Epoch: 270. Loss: 0.5916773423684629\n",
            "Epoch: 280. Loss: 0.5916240596567476\n",
            "Epoch: 290. Loss: 0.5915746021292331\n",
            "Epoch: 300. Loss: 0.5915286885522445\n",
            "Epoch: 310. Loss: 0.5914860606034585\n",
            "Epoch: 320. Loss: 0.5914464802397245\n",
            "Epoch: 330. Loss: 0.5914097276051121\n",
            "Epoch: 340. Loss: 0.5913755993161661\n",
            "Epoch: 350. Loss: 0.5913439070168667\n",
            "Epoch: 360. Loss: 0.5913144761318869\n",
            "Epoch: 370. Loss: 0.5912871447702545\n",
            "Epoch: 380. Loss: 0.591261762746871\n",
            "Epoch: 390. Loss: 0.591238190699392\n",
            "Epoch: 400. Loss: 0.591216299284573\n",
            "Epoch: 410. Loss: 0.5911959684425615\n",
            "Epoch: 420. Loss: 0.5911770867205133\n",
            "Epoch: 430. Loss: 0.591159550648888\n",
            "Epoch: 440. Loss: 0.5911432641651081\n",
            "Epoch: 450. Loss: 0.5911281380802101\n",
            "Epoch: 460. Loss: 0.5911140895847874\n",
            "Epoch: 470. Loss: 0.5911010417910106\n",
            "Epoch: 480. Loss: 0.5910889233078946\n",
            "Epoch: 490. Loss: 0.5910776678472724\n",
            "tensor(0.6072, dtype=torch.float64)\n",
            "2022-06-12 00:00:00\n",
            "Epoch: 0. Loss: 2.475666985231131\n",
            "Epoch: 10. Loss: 0.7658363116679971\n",
            "Epoch: 20. Loss: 0.6805775620049352\n",
            "Epoch: 30. Loss: 0.6515306770092779\n",
            "Epoch: 40. Loss: 0.6333265044812\n",
            "Epoch: 50. Loss: 0.6213351508261907\n",
            "Epoch: 60. Loss: 0.6135272414989739\n",
            "Epoch: 70. Loss: 0.6084859912814093\n",
            "Epoch: 80. Loss: 0.6052437263183417\n",
            "Epoch: 90. Loss: 0.6031607467294173\n",
            "Epoch: 100. Loss: 0.6018219536330397\n",
            "Epoch: 110. Loss: 0.6009604489151115\n",
            "Epoch: 120. Loss: 0.6004052551522251\n",
            "Epoch: 130. Loss: 0.6000469155859774\n",
            "Epoch: 140. Loss: 0.5998152954933201\n",
            "Epoch: 150. Loss: 0.5996653836788216\n",
            "Epoch: 160. Loss: 0.5995682393261422\n",
            "Epoch: 170. Loss: 0.5995052201090347\n",
            "Epoch: 180. Loss: 0.5994642970024239\n",
            "Epoch: 190. Loss: 0.5994376964344692\n",
            "Epoch: 200. Loss: 0.5994203882204591\n",
            "Epoch: 210. Loss: 0.5994091137128806\n",
            "Epoch: 220. Loss: 0.5994017598782493\n",
            "Epoch: 230. Loss: 0.5993969554345129\n",
            "Epoch: 240. Loss: 0.5993938098574976\n",
            "Epoch: 250. Loss: 0.5993917444896572\n",
            "Epoch: 260. Loss: 0.5993903831260472\n",
            "Epoch: 270. Loss: 0.5993894810653773\n",
            "Epoch: 280. Loss: 0.5993888790677983\n",
            "Epoch: 290. Loss: 0.599388473456713\n",
            "Epoch: 300. Loss: 0.5993881966936841\n",
            "Epoch: 310. Loss: 0.5993880047522852\n",
            "Epoch: 320. Loss: 0.5993878689082348\n",
            "Epoch: 330. Loss: 0.5993877703994894\n",
            "Epoch: 340. Loss: 0.599387696952124\n",
            "Epoch: 350. Loss: 0.599387640519559\n",
            "Epoch: 360. Loss: 0.5993875958110457\n",
            "Epoch: 370. Loss: 0.5993875593336687\n",
            "Epoch: 380. Loss: 0.5993875287685083\n",
            "Epoch: 390. Loss: 0.5993875025642975\n",
            "Epoch: 400. Loss: 0.5993874796726514\n",
            "Epoch: 410. Loss: 0.5993874593754698\n",
            "Epoch: 420. Loss: 0.5993874411723542\n",
            "Epoch: 430. Loss: 0.5993874247071126\n",
            "Epoch: 440. Loss: 0.599387409719724\n",
            "Epoch: 450. Loss: 0.5993873960148923\n",
            "Epoch: 460. Loss: 0.5993873834414155\n",
            "Epoch: 470. Loss: 0.5993873718786044\n",
            "Epoch: 480. Loss: 0.5993873612273077\n",
            "Epoch: 490. Loss: 0.5993873514039401\n",
            "tensor(0.6236, dtype=torch.float64)\n",
            "2022-06-19 00:00:00\n",
            "Epoch: 0. Loss: 1.5792750055231166\n",
            "Epoch: 10. Loss: 0.6686366006874775\n",
            "Epoch: 20. Loss: 0.6524489135488104\n",
            "Epoch: 30. Loss: 0.6485228223371314\n",
            "Epoch: 40. Loss: 0.6452959854400317\n",
            "Epoch: 50. Loss: 0.6424051852026967\n",
            "Epoch: 60. Loss: 0.6397587129387128\n",
            "Epoch: 70. Loss: 0.6373127286137054\n",
            "Epoch: 80. Loss: 0.6350419391564205\n",
            "Epoch: 90. Loss: 0.6329293495698667\n",
            "Epoch: 100. Loss: 0.6309620632675805\n",
            "Epoch: 110. Loss: 0.629129448060368\n",
            "Epoch: 120. Loss: 0.6274222730556983\n",
            "Epoch: 130. Loss: 0.6258322653927427\n",
            "Epoch: 140. Loss: 0.6243518620862538\n",
            "Epoch: 150. Loss: 0.6229740606889658\n",
            "Epoch: 160. Loss: 0.621692324414384\n",
            "Epoch: 170. Loss: 0.6205005193732362\n",
            "Epoch: 180. Loss: 0.6193928716030611\n",
            "Epoch: 190. Loss: 0.6183639365781493\n",
            "Epoch: 200. Loss: 0.6174085766397972\n",
            "Epoch: 210. Loss: 0.6165219434270818\n",
            "Epoch: 220. Loss: 0.6156994634217507\n",
            "Epoch: 230. Loss: 0.6149368253932558\n",
            "Epoch: 240. Loss: 0.6142299689740814\n",
            "Epoch: 250. Loss: 0.6135750738898109\n",
            "Epoch: 260. Loss: 0.6129685495624294\n",
            "Epoch: 270. Loss: 0.6124070249318984\n",
            "Epoch: 280. Loss: 0.61188733842198\n",
            "Epoch: 290. Loss: 0.61140652802663\n",
            "Epoch: 300. Loss: 0.6109618215231729\n",
            "Epoch: 310. Loss: 0.610550626834895\n",
            "Epoch: 320. Loss: 0.6101705225733367\n",
            "Epoch: 330. Loss: 0.6098192487926654\n",
            "Epoch: 340. Loss: 0.6094946979871949\n",
            "Epoch: 350. Loss: 0.6091949063598386\n",
            "Epoch: 360. Loss: 0.6089180453849945\n",
            "Epoch: 370. Loss: 0.6086624136846718\n",
            "Epoch: 380. Loss: 0.6084264292319802\n",
            "Epoch: 390. Loss: 0.6082086218916497\n",
            "Epoch: 400. Loss: 0.6080076263031738\n",
            "Epoch: 410. Loss: 0.6078221751085325\n",
            "Epoch: 420. Loss: 0.6076510925232856\n",
            "Epoch: 430. Loss: 0.6074932882471235\n",
            "Epoch: 440. Loss: 0.6073477517076985\n",
            "Epoch: 450. Loss: 0.607213546629709\n",
            "Epoch: 460. Loss: 0.6070898059197136\n",
            "Epoch: 470. Loss: 0.6069757268560142\n",
            "Epoch: 480. Loss: 0.6068705665720653\n",
            "Epoch: 490. Loss: 0.606773637821272\n",
            "tensor(0.5811, dtype=torch.float64)\n",
            "2022-06-26 00:00:00\n",
            "Epoch: 0. Loss: 2.4912334682072608\n",
            "Epoch: 10. Loss: 1.4743622129437512\n",
            "Epoch: 20. Loss: 1.2716337188467957\n",
            "Epoch: 30. Loss: 1.1363745423980884\n",
            "Epoch: 40. Loss: 1.0243133393461372\n",
            "Epoch: 50. Loss: 0.9281760856748014\n",
            "Epoch: 60. Loss: 0.8468948349252123\n",
            "Epoch: 70. Loss: 0.7802941518331497\n",
            "Epoch: 80. Loss: 0.727918244824577\n",
            "Epoch: 90. Loss: 0.6886091873952424\n",
            "Epoch: 100. Loss: 0.6604485725669995\n",
            "Epoch: 110. Loss: 0.6410649420168895\n",
            "Epoch: 120. Loss: 0.6281095133958369\n",
            "Epoch: 130. Loss: 0.6196096092727847\n",
            "Epoch: 140. Loss: 0.6140864471500129\n",
            "Epoch: 150. Loss: 0.6105087334685024\n",
            "Epoch: 160. Loss: 0.6081875074065017\n",
            "Epoch: 170. Loss: 0.6066735098122541\n",
            "Epoch: 180. Loss: 0.6056775016403835\n",
            "Epoch: 190. Loss: 0.6050143767667175\n",
            "Epoch: 200. Loss: 0.604565909380978\n",
            "Epoch: 210. Loss: 0.6042565664690808\n",
            "Epoch: 220. Loss: 0.6040380094259986\n",
            "Epoch: 230. Loss: 0.6038792229881375\n",
            "Epoch: 240. Loss: 0.6037602410561778\n",
            "Epoch: 250. Loss: 0.603668158200222\n",
            "Epoch: 260. Loss: 0.6035945909596748\n",
            "Epoch: 270. Loss: 0.6035340590386602\n",
            "Epoch: 280. Loss: 0.6034829510095935\n",
            "Epoch: 290. Loss: 0.6034388621303745\n",
            "Epoch: 300. Loss: 0.6034001695481214\n",
            "Epoch: 310. Loss: 0.6033657592479513\n",
            "Epoch: 320. Loss: 0.6033348501859861\n",
            "Epoch: 330. Loss: 0.6033068807719649\n",
            "Epoch: 340. Loss: 0.6032814354171364\n",
            "Epoch: 350. Loss: 0.6032581968665109\n",
            "Epoch: 360. Loss: 0.6032369151492715\n",
            "Epoch: 370. Loss: 0.6032173872559304\n",
            "Epoch: 380. Loss: 0.6031994437510301\n",
            "Epoch: 390. Loss: 0.6031829398789956\n",
            "Epoch: 400. Loss: 0.6031677495880367\n",
            "Epoch: 410. Loss: 0.6031537614552442\n",
            "Epoch: 420. Loss: 0.6031408758556799\n",
            "Epoch: 430. Loss: 0.603129002950142\n",
            "Epoch: 440. Loss: 0.6031180612159164\n",
            "Epoch: 450. Loss: 0.6031079763414307\n",
            "Epoch: 460. Loss: 0.6030986803681606\n",
            "Epoch: 470. Loss: 0.6030901110035091\n",
            "Epoch: 480. Loss: 0.603082211054521\n",
            "Epoch: 490. Loss: 0.6030749279492422\n",
            "tensor(0.6360, dtype=torch.float64)\n",
            "2022-07-03 00:00:00\n",
            "Epoch: 0. Loss: 1.6810824487577682\n",
            "Epoch: 10. Loss: 0.9178309154947273\n",
            "Epoch: 20. Loss: 0.735427455439703\n",
            "Epoch: 30. Loss: 0.672799302310873\n",
            "Epoch: 40. Loss: 0.6391049652360772\n",
            "Epoch: 50. Loss: 0.6226761437329315\n",
            "Epoch: 60. Loss: 0.6153904248303256\n",
            "Epoch: 70. Loss: 0.6121928906077352\n",
            "Epoch: 80. Loss: 0.6106527251183268\n",
            "Epoch: 90. Loss: 0.6097717866314175\n",
            "Epoch: 100. Loss: 0.6091676458505065\n",
            "Epoch: 110. Loss: 0.6086948937972312\n",
            "Epoch: 120. Loss: 0.6082959100238837\n",
            "Epoch: 130. Loss: 0.6079457552840122\n",
            "Epoch: 140. Loss: 0.6076321635914883\n",
            "Epoch: 150. Loss: 0.6073481724464466\n",
            "Epoch: 160. Loss: 0.6070892874619405\n",
            "Epoch: 170. Loss: 0.6068523077778628\n",
            "Epoch: 180. Loss: 0.6066347886620216\n",
            "Epoch: 190. Loss: 0.6064347675461019\n",
            "Epoch: 200. Loss: 0.6062506101983908\n",
            "Epoch: 210. Loss: 0.6060809177463816\n",
            "Epoch: 220. Loss: 0.6059244674820569\n",
            "Epoch: 230. Loss: 0.6057801737350652\n",
            "Epoch: 240. Loss: 0.6056470612095605\n",
            "Epoch: 250. Loss: 0.6055242462837935\n",
            "Epoch: 260. Loss: 0.6054109234949221\n",
            "Epoch: 270. Loss: 0.6053063554521095\n",
            "Epoch: 280. Loss: 0.6052098650504839\n",
            "Epoch: 290. Loss: 0.6051208292560009\n",
            "Epoch: 300. Loss: 0.6050386739855149\n",
            "Epoch: 310. Loss: 0.604962869770115\n",
            "Epoch: 320. Loss: 0.6048929279956526\n",
            "Epoch: 330. Loss: 0.6048283975829931\n",
            "Epoch: 340. Loss: 0.6047688620150447\n",
            "Epoch: 350. Loss: 0.6047139366465911\n",
            "Epoch: 360. Loss: 0.6046632662518527\n",
            "Epoch: 370. Loss: 0.6046165227770854\n",
            "Epoch: 380. Loss: 0.6045734032737149\n",
            "Epoch: 390. Loss: 0.6045336279929523\n",
            "Epoch: 400. Loss: 0.6044969386265411\n",
            "Epoch: 410. Loss: 0.6044630966808461\n",
            "Epoch: 420. Loss: 0.6044318819733088\n",
            "Epoch: 430. Loss: 0.6044030912416369\n",
            "Epoch: 440. Loss: 0.6043765368571127\n",
            "Epoch: 450. Loss: 0.6043520456342216\n",
            "Epoch: 460. Loss: 0.6043294577294646\n",
            "Epoch: 470. Loss: 0.6043086256227891\n",
            "Epoch: 480. Loss: 0.6042894131755815\n",
            "Epoch: 490. Loss: 0.6042716947595927\n",
            "tensor(0.9986, dtype=torch.float64)\n",
            "2022-07-10 00:00:00\n",
            "Epoch: 0. Loss: 3.148290145729886\n",
            "Epoch: 10. Loss: 2.213566302644348\n",
            "Epoch: 20. Loss: 1.419357595247522\n",
            "Epoch: 30. Loss: 0.9045161911302806\n",
            "Epoch: 40. Loss: 0.7144405945703147\n",
            "Epoch: 50. Loss: 0.660258563248646\n",
            "Epoch: 60. Loss: 0.6374676672062811\n",
            "Epoch: 70. Loss: 0.6255102036588978\n",
            "Epoch: 80. Loss: 0.6185581169580628\n",
            "Epoch: 90. Loss: 0.6142498266309746\n",
            "Epoch: 100. Loss: 0.6114851273279626\n",
            "Epoch: 110. Loss: 0.6096816573963192\n",
            "Epoch: 120. Loss: 0.6084969349706081\n",
            "Epoch: 130. Loss: 0.6077160551101899\n",
            "Epoch: 140. Loss: 0.6071998766372617\n",
            "Epoch: 150. Loss: 0.6068572538809434\n",
            "Epoch: 160. Loss: 0.6066283432949587\n",
            "Epoch: 170. Loss: 0.6064739089768251\n",
            "Epoch: 180. Loss: 0.6063682888206466\n",
            "Epoch: 190. Loss: 0.6062947366698797\n",
            "Epoch: 200. Loss: 0.6062423421773178\n",
            "Epoch: 210. Loss: 0.6062040023068656\n",
            "Epoch: 220. Loss: 0.6061750913757806\n",
            "Epoch: 230. Loss: 0.6061525925481309\n",
            "Epoch: 240. Loss: 0.6061345325137043\n",
            "Epoch: 250. Loss: 0.606119614444698\n",
            "Epoch: 260. Loss: 0.6061069801367523\n",
            "Epoch: 270. Loss: 0.6060960560706423\n",
            "Epoch: 280. Loss: 0.6060864538675813\n",
            "Epoch: 290. Loss: 0.6060779059402296\n",
            "Epoch: 300. Loss: 0.6060702238890837\n",
            "Epoch: 310. Loss: 0.6060632715856098\n",
            "Epoch: 320. Loss: 0.606056947733747\n",
            "Epoch: 330. Loss: 0.6060511745472579\n",
            "Epoch: 340. Loss: 0.6060458903738024\n",
            "Epoch: 350. Loss: 0.6060410448672149\n",
            "Epoch: 360. Loss: 0.6060365958065819\n",
            "Epoch: 370. Loss: 0.6060325069811872\n",
            "Epoch: 380. Loss: 0.6060287467668856\n",
            "Epoch: 390. Loss: 0.606025287152461\n",
            "Epoch: 400. Loss: 0.606022103060191\n",
            "Epoch: 410. Loss: 0.6060191718599953\n",
            "Epoch: 420. Loss: 0.6060164730120736\n",
            "Epoch: 430. Loss: 0.6060139877958298\n",
            "Epoch: 440. Loss: 0.6060116990976325\n",
            "Epoch: 450. Loss: 0.6060095912394756\n",
            "Epoch: 460. Loss: 0.6060076498367531\n",
            "Epoch: 470. Loss: 0.6060058616773347\n",
            "Epoch: 480. Loss: 0.606004214616704\n",
            "Epoch: 490. Loss: 0.6060026974856008\n",
            "tensor(0.5929, dtype=torch.float64)\n",
            "2022-07-17 00:00:00\n",
            "Epoch: 0. Loss: 0.9949729198719397\n",
            "Epoch: 10. Loss: 0.7082000535275882\n",
            "Epoch: 20. Loss: 0.6664816092061323\n",
            "Epoch: 30. Loss: 0.6461679504780644\n",
            "Epoch: 40. Loss: 0.6344845880581109\n",
            "Epoch: 50. Loss: 0.6275374249443552\n",
            "Epoch: 60. Loss: 0.6232688947602928\n",
            "Epoch: 70. Loss: 0.6205636564408102\n",
            "Epoch: 80. Loss: 0.6187969809763999\n",
            "Epoch: 90. Loss: 0.6176066713802087\n",
            "Epoch: 100. Loss: 0.6167770177712631\n",
            "Epoch: 110. Loss: 0.6161770860605623\n",
            "Epoch: 120. Loss: 0.6157262917238634\n",
            "Epoch: 130. Loss: 0.6153744751257245\n",
            "Epoch: 140. Loss: 0.6150900811118434\n",
            "Epoch: 150. Loss: 0.6148530322116487\n",
            "Epoch: 160. Loss: 0.6146503843003918\n",
            "Epoch: 170. Loss: 0.6144736571065662\n",
            "Epoch: 180. Loss: 0.6143171834853559\n",
            "Epoch: 190. Loss: 0.6141770831801481\n",
            "Epoch: 200. Loss: 0.6140506217042088\n",
            "Epoch: 210. Loss: 0.6139358079065017\n",
            "Epoch: 220. Loss: 0.6138311400942779\n",
            "Epoch: 230. Loss: 0.6137354449658518\n",
            "Epoch: 240. Loss: 0.613647774729092\n",
            "Epoch: 250. Loss: 0.6135673408239449\n",
            "Epoch: 260. Loss: 0.613493470755562\n",
            "Epoch: 270. Loss: 0.6134255795784157\n",
            "Epoch: 280. Loss: 0.6133631507141594\n",
            "Epoch: 290. Loss: 0.6133057227527342\n",
            "Epoch: 300. Loss: 0.6132528801199439\n",
            "Epoch: 310. Loss: 0.6132042462701704\n",
            "Epoch: 320. Loss: 0.6131594785512247\n",
            "Epoch: 330. Loss: 0.6131182641964048\n",
            "Epoch: 340. Loss: 0.6130803170935625\n",
            "Epoch: 350. Loss: 0.6130453751043414\n",
            "Epoch: 360. Loss: 0.6130131977851044\n",
            "Epoch: 370. Loss: 0.6129835644110099\n",
            "Epoch: 380. Loss: 0.612956272236654\n",
            "Epoch: 390. Loss: 0.612931134947296\n",
            "Epoch: 400. Loss: 0.6129079812680279\n",
            "Epoch: 410. Loss: 0.6128866537070383\n",
            "Epoch: 420. Loss: 0.6128670074149362\n",
            "Epoch: 430. Loss: 0.6128489091460589\n",
            "Epoch: 440. Loss: 0.6128322363104292\n",
            "Epoch: 450. Loss: 0.6128168761069698\n",
            "Epoch: 460. Loss: 0.6128027247300126\n",
            "Epoch: 470. Loss: 0.6127896866422251\n",
            "Epoch: 480. Loss: 0.6127776739079144\n",
            "Epoch: 490. Loss: 0.6127666055813611\n",
            "tensor(0.5872, dtype=torch.float64)\n",
            "2022-07-24 00:00:00\n",
            "Epoch: 0. Loss: 4.9182649566245304\n",
            "Epoch: 10. Loss: 2.644256017414052\n",
            "Epoch: 20. Loss: 1.4438534607267666\n",
            "Epoch: 30. Loss: 0.9638594898323255\n",
            "Epoch: 40. Loss: 0.7950536404022032\n",
            "Epoch: 50. Loss: 0.7165467058394019\n",
            "Epoch: 60. Loss: 0.6770712850678497\n",
            "Epoch: 70. Loss: 0.657102596415843\n",
            "Epoch: 80. Loss: 0.6463782499252725\n",
            "Epoch: 90. Loss: 0.6400593029441697\n",
            "Epoch: 100. Loss: 0.6359750437691744\n",
            "Epoch: 110. Loss: 0.6331233755508466\n",
            "Epoch: 120. Loss: 0.6310055303097349\n",
            "Epoch: 130. Loss: 0.6293531705440247\n",
            "Epoch: 140. Loss: 0.6280127821738061\n",
            "Epoch: 150. Loss: 0.6268923332664225\n",
            "Epoch: 160. Loss: 0.6259344257702618\n",
            "Epoch: 170. Loss: 0.6251018736160802\n",
            "Epoch: 180. Loss: 0.6243696175949235\n",
            "Epoch: 190. Loss: 0.623720063377123\n",
            "Epoch: 200. Loss: 0.6231403371205976\n",
            "Epoch: 210. Loss: 0.6226206406235586\n",
            "Epoch: 220. Loss: 0.6221532475280828\n",
            "Epoch: 230. Loss: 0.6217318784434162\n",
            "Epoch: 240. Loss: 0.6213513029924091\n",
            "Epoch: 250. Loss: 0.6210070796338055\n",
            "Epoch: 260. Loss: 0.6206953804284071\n",
            "Epoch: 270. Loss: 0.6204128691128085\n",
            "Epoch: 280. Loss: 0.6201566133263302\n",
            "Epoch: 290. Loss: 0.6199240192502804\n",
            "Epoch: 300. Loss: 0.619712781359994\n",
            "Epoch: 310. Loss: 0.6195208426753901\n",
            "Epoch: 320. Loss: 0.61934636253597\n",
            "Epoch: 330. Loss: 0.6191876899397549\n",
            "Epoch: 340. Loss: 0.619043341120607\n",
            "Epoch: 350. Loss: 0.6189119804425764\n",
            "Epoch: 360. Loss: 0.6187924039520836\n",
            "Epoch: 370. Loss: 0.6186835251025076\n",
            "Epoch: 380. Loss: 0.6185843622837388\n",
            "Epoch: 390. Loss: 0.6184940278714622\n",
            "Epoch: 400. Loss: 0.618411718569808\n",
            "Epoch: 410. Loss: 0.618336706864285\n",
            "Epoch: 420. Loss: 0.6182683334345704\n",
            "Epoch: 430. Loss: 0.6182060004019235\n",
            "Epoch: 440. Loss: 0.6181491653058576\n",
            "Epoch: 450. Loss: 0.6180973357206294\n",
            "Epoch: 460. Loss: 0.618050064435064\n",
            "Epoch: 470. Loss: 0.6180069451299166\n",
            "Epoch: 480. Loss: 0.6179676084958644\n",
            "Epoch: 490. Loss: 0.6179317187426853\n",
            "tensor(0.6442, dtype=torch.float64)\n",
            "2022-07-31 00:00:00\n",
            "Epoch: 0. Loss: 1.946097485774792\n",
            "Epoch: 10. Loss: 1.1089961896929645\n",
            "Epoch: 20. Loss: 0.9110007471172608\n",
            "Epoch: 30. Loss: 0.8191813841877558\n",
            "Epoch: 40. Loss: 0.7606003360509689\n",
            "Epoch: 50. Loss: 0.72401761510645\n",
            "Epoch: 60. Loss: 0.7019587780754226\n",
            "Epoch: 70. Loss: 0.6880442056670272\n",
            "Epoch: 80. Loss: 0.6782346308537406\n",
            "Epoch: 90. Loss: 0.6705369535775314\n",
            "Epoch: 100. Loss: 0.6640946286178423\n",
            "Epoch: 110. Loss: 0.65854488032764\n",
            "Epoch: 120. Loss: 0.6537140307746111\n",
            "Epoch: 130. Loss: 0.6494976451556883\n",
            "Epoch: 140. Loss: 0.6458176977812381\n",
            "Epoch: 150. Loss: 0.6426081528562847\n",
            "Epoch: 160. Loss: 0.6398103763191133\n",
            "Epoch: 170. Loss: 0.637371765608103\n",
            "Epoch: 180. Loss: 0.6352453227864276\n",
            "Epoch: 190. Loss: 0.633389415013431\n",
            "Epoch: 200. Loss: 0.6317674954323766\n",
            "Epoch: 210. Loss: 0.630347741424782\n",
            "Epoch: 220. Loss: 0.6291026275881095\n",
            "Epoch: 230. Loss: 0.6280084655474558\n",
            "Epoch: 240. Loss: 0.627044940664838\n",
            "Epoch: 250. Loss: 0.6261946683060674\n",
            "Epoch: 260. Loss: 0.6254427842744703\n",
            "Epoch: 270. Loss: 0.6247765771584358\n",
            "Epoch: 280. Loss: 0.6241851651940589\n",
            "Epoch: 290. Loss: 0.6236592167544847\n",
            "Epoch: 300. Loss: 0.6231907114518538\n",
            "Epoch: 310. Loss: 0.6227727377309044\n",
            "Epoch: 320. Loss: 0.6223993224303063\n",
            "Epoch: 330. Loss: 0.6220652878347475\n",
            "Epoch: 340. Loss: 0.6217661320490974\n",
            "Epoch: 350. Loss: 0.6214979289645672\n",
            "Epoch: 360. Loss: 0.621257244570055\n",
            "Epoch: 370. Loss: 0.6210410668376962\n",
            "Epoch: 380. Loss: 0.6208467468512427\n",
            "Epoch: 390. Loss: 0.6206719492358097\n",
            "Epoch: 400. Loss: 0.620514610283896\n",
            "Epoch: 410. Loss: 0.620372902457113\n",
            "Epoch: 420. Loss: 0.6202452041803456\n",
            "Epoch: 430. Loss: 0.6201300740409527\n",
            "Epoch: 440. Loss: 0.620026228666177\n",
            "Epoch: 450. Loss: 0.6199325236828727\n",
            "Epoch: 460. Loss: 0.6198479372701465\n",
            "Epoch: 470. Loss: 0.6197715559019616\n",
            "Epoch: 480. Loss: 0.6197025619469121\n",
            "Epoch: 490. Loss: 0.6196402228493821\n",
            "tensor(0.6410, dtype=torch.float64)\n",
            "2022-08-07 00:00:00\n",
            "Epoch: 0. Loss: 2.2990670804194555\n",
            "Epoch: 10. Loss: 1.1650140049635191\n",
            "Epoch: 20. Loss: 0.7133866882943939\n",
            "Epoch: 30. Loss: 0.6598324228424086\n",
            "Epoch: 40. Loss: 0.6386596989952393\n",
            "Epoch: 50. Loss: 0.6305272433843379\n",
            "Epoch: 60. Loss: 0.6273885440175726\n",
            "Epoch: 70. Loss: 0.6258659597283391\n",
            "Epoch: 80. Loss: 0.624856227879588\n",
            "Epoch: 90. Loss: 0.6240381548355274\n",
            "Epoch: 100. Loss: 0.6233197111187176\n",
            "Epoch: 110. Loss: 0.6226718917880391\n",
            "Epoch: 120. Loss: 0.6220829210123834\n",
            "Epoch: 130. Loss: 0.6215460203033026\n",
            "Epoch: 140. Loss: 0.6210561287415619\n",
            "Epoch: 150. Loss: 0.6206089794744494\n",
            "Epoch: 160. Loss: 0.6202008043535125\n",
            "Epoch: 170. Loss: 0.6198282149309022\n",
            "Epoch: 180. Loss: 0.619488139235394\n",
            "Epoch: 190. Loss: 0.6191777801360101\n",
            "Epoch: 200. Loss: 0.6188945842785846\n",
            "Epoch: 210. Loss: 0.6186362172626632\n",
            "Epoch: 220. Loss: 0.6184005429468699\n",
            "Epoch: 230. Loss: 0.6181856056785704\n",
            "Epoch: 240. Loss: 0.6179896146986021\n",
            "Epoch: 250. Loss: 0.6178109302333331\n",
            "Epoch: 260. Loss: 0.6176480509473361\n",
            "Epoch: 270. Loss: 0.6174996025320989\n",
            "Epoch: 280. Loss: 0.6173643272717354\n",
            "Epoch: 290. Loss: 0.6172410744689787\n",
            "Epoch: 300. Loss: 0.6171287916422207\n",
            "Epoch: 310. Loss: 0.6170265164223891\n",
            "Epoch: 320. Loss: 0.6169333690904504\n",
            "Epoch: 330. Loss: 0.6168485457045417\n",
            "Epoch: 340. Loss: 0.6167713117715531\n",
            "Epoch: 350. Loss: 0.6167009964223031\n",
            "Epoch: 360. Loss: 0.6166369870528371\n",
            "Epoch: 370. Loss: 0.6165787243971838\n",
            "Epoch: 380. Loss: 0.6165256979993293\n",
            "Epoch: 390. Loss: 0.6164774420543443\n",
            "Epoch: 400. Loss: 0.616433531590607\n",
            "Epoch: 410. Loss: 0.616393578966944\n",
            "Epoch: 420. Loss: 0.6163572306602754\n",
            "Epoch: 430. Loss: 0.6163241643210254\n",
            "Epoch: 440. Loss: 0.616294086075157\n",
            "Epoch: 450. Loss: 0.6162667280531771\n",
            "Epoch: 460. Loss: 0.6162418461278957\n",
            "Epoch: 470. Loss: 0.6162192178440494\n",
            "Epoch: 480. Loss: 0.6161986405241663\n",
            "Epoch: 490. Loss: 0.6161799295362294\n",
            "tensor(0.7343, dtype=torch.float64)\n",
            "2022-08-14 00:00:00\n",
            "Epoch: 0. Loss: 4.178631929123699\n",
            "Epoch: 10. Loss: 1.894021211795996\n",
            "Epoch: 20. Loss: 1.0026970299329492\n",
            "Epoch: 30. Loss: 0.6678655937589008\n",
            "Epoch: 40. Loss: 0.6318836964145293\n",
            "Epoch: 50. Loss: 0.6244323918325805\n",
            "Epoch: 60. Loss: 0.6200005502177423\n",
            "Epoch: 70. Loss: 0.6170873159564361\n",
            "Epoch: 80. Loss: 0.6151380514408434\n",
            "Epoch: 90. Loss: 0.6138249753625682\n",
            "Epoch: 100. Loss: 0.6129361532763594\n",
            "Epoch: 110. Loss: 0.6123305919479782\n",
            "Epoch: 120. Loss: 0.6119139779688332\n",
            "Epoch: 130. Loss: 0.611623376966796\n",
            "Epoch: 140. Loss: 0.6114169671508288\n",
            "Epoch: 150. Loss: 0.6112670553494463\n",
            "Epoch: 160. Loss: 0.6111553441207789\n",
            "Epoch: 170. Loss: 0.6110697505818149\n",
            "Epoch: 180. Loss: 0.6110022854053988\n",
            "Epoch: 190. Loss: 0.6109476481398007\n",
            "Epoch: 200. Loss: 0.6109023017913453\n",
            "Epoch: 210. Loss: 0.6108638655461748\n",
            "Epoch: 220. Loss: 0.6108307173910994\n",
            "Epoch: 230. Loss: 0.6108017345758124\n",
            "Epoch: 240. Loss: 0.6107761242767146\n",
            "Epoch: 250. Loss: 0.6107533131317848\n",
            "Epoch: 260. Loss: 0.6107328751247576\n",
            "Epoch: 270. Loss: 0.6107144844183221\n",
            "Epoch: 280. Loss: 0.6106978844069965\n",
            "Epoch: 290. Loss: 0.6106828673133854\n",
            "Epoch: 300. Loss: 0.6106692606416686\n",
            "Epoch: 310. Loss: 0.6106569180967506\n",
            "Epoch: 320. Loss: 0.6106457134182094\n",
            "Epoch: 330. Loss: 0.6106355361234636\n",
            "Epoch: 340. Loss: 0.6106262885079072\n",
            "Epoch: 350. Loss: 0.6106178834785579\n",
            "Epoch: 360. Loss: 0.610610242945864\n",
            "Epoch: 370. Loss: 0.6106032965941679\n",
            "Epoch: 380. Loss: 0.6105969809133832\n",
            "Epoch: 390. Loss: 0.6105912384146313\n",
            "Epoch: 400. Loss: 0.6105860169786607\n",
            "Epoch: 410. Loss: 0.6105812693027982\n",
            "Epoch: 420. Loss: 0.6105769524232176\n",
            "Epoch: 430. Loss: 0.610573027296523\n",
            "Epoch: 440. Loss: 0.6105694584293789\n",
            "Epoch: 450. Loss: 0.6105662135480661\n",
            "Epoch: 460. Loss: 0.6105632633019324\n",
            "Epoch: 470. Loss: 0.6105605809961385\n",
            "Epoch: 480. Loss: 0.6105581423500674\n",
            "Epoch: 490. Loss: 0.6105559252784671\n",
            "tensor(0.6417, dtype=torch.float64)\n",
            "2022-08-21 00:00:00\n",
            "Epoch: 0. Loss: 2.9723903608297575\n",
            "Epoch: 10. Loss: 1.6490966028466023\n",
            "Epoch: 20. Loss: 1.3061843691670394\n",
            "Epoch: 30. Loss: 1.113766949227186\n",
            "Epoch: 40. Loss: 0.9569330238399558\n",
            "Epoch: 50. Loss: 0.8278389294978736\n",
            "Epoch: 60. Loss: 0.7309306059695169\n",
            "Epoch: 70. Loss: 0.6686522720501584\n",
            "Epoch: 80. Loss: 0.6362305882685336\n",
            "Epoch: 90. Loss: 0.6227562417627506\n",
            "Epoch: 100. Loss: 0.6180285083055216\n",
            "Epoch: 110. Loss: 0.6164509165716281\n",
            "Epoch: 120. Loss: 0.6158615638502504\n",
            "Epoch: 130. Loss: 0.6155742321832778\n",
            "Epoch: 140. Loss: 0.6153884502541339\n",
            "Epoch: 150. Loss: 0.6152454532252859\n",
            "Epoch: 160. Loss: 0.615126280464877\n",
            "Epoch: 170. Loss: 0.6150234664133499\n",
            "Epoch: 180. Loss: 0.6149332386488359\n",
            "Epoch: 190. Loss: 0.6148532635923648\n",
            "Epoch: 200. Loss: 0.6147819108051868\n",
            "Epoch: 210. Loss: 0.6147179605181547\n",
            "Epoch: 220. Loss: 0.6146604597393439\n",
            "Epoch: 230. Loss: 0.6146086394903478\n",
            "Epoch: 240. Loss: 0.6145618628577604\n",
            "Epoch: 250. Loss: 0.6145195908779243\n",
            "Epoch: 260. Loss: 0.6144813595131986\n",
            "Epoch: 270. Loss: 0.6144467637644603\n",
            "Epoch: 280. Loss: 0.6144154464555405\n",
            "Epoch: 290. Loss: 0.6143870901132751\n",
            "Epoch: 300. Loss: 0.6143614109230334\n",
            "Epoch: 310. Loss: 0.6143381540953387\n",
            "Epoch: 320. Loss: 0.6143170902086446\n",
            "Epoch: 330. Loss: 0.6142980122418682\n",
            "Epoch: 340. Loss: 0.6142807331066309\n",
            "Epoch: 350. Loss: 0.6142650835517821\n",
            "Epoch: 360. Loss: 0.6142509103536163\n",
            "Epoch: 370. Loss: 0.6142380747318898\n",
            "Epoch: 380. Loss: 0.6142264509493283\n",
            "Epoch: 390. Loss: 0.6142159250639625\n",
            "Epoch: 400. Loss: 0.614206393811439\n",
            "Epoch: 410. Loss: 0.6141977635997518\n",
            "Epoch: 420. Loss: 0.6141899496025081\n",
            "Epoch: 430. Loss: 0.6141828749394251\n",
            "Epoch: 440. Loss: 0.6141764699346365\n",
            "Epoch: 450. Loss: 0.6141706714447814\n",
            "Epoch: 460. Loss: 0.6141654222499222\n",
            "Epoch: 470. Loss: 0.614160670501193\n",
            "Epoch: 480. Loss: 0.6141563692197661\n",
            "Epoch: 490. Loss: 0.6141524758423079\n",
            "tensor(0.6177, dtype=torch.float64)\n",
            "2022-08-28 00:00:00\n",
            "Epoch: 0. Loss: 3.535843174314024\n",
            "Epoch: 10. Loss: 1.2330886437553772\n",
            "Epoch: 20. Loss: 0.7798340766114613\n",
            "Epoch: 30. Loss: 0.7286984617874385\n",
            "Epoch: 40. Loss: 0.7069218929450778\n",
            "Epoch: 50. Loss: 0.6943559045393389\n",
            "Epoch: 60. Loss: 0.6860827201119115\n",
            "Epoch: 70. Loss: 0.6797377663119769\n",
            "Epoch: 80. Loss: 0.6743222862642843\n",
            "Epoch: 90. Loss: 0.6694569182138658\n",
            "Epoch: 100. Loss: 0.6649991967050708\n",
            "Epoch: 110. Loss: 0.6608888722718435\n",
            "Epoch: 120. Loss: 0.6570930962012194\n",
            "Epoch: 130. Loss: 0.6535883415065135\n",
            "Epoch: 140. Loss: 0.6503546438204778\n",
            "Epoch: 150. Loss: 0.6473737742541884\n",
            "Epoch: 160. Loss: 0.6446286408767481\n",
            "Epoch: 170. Loss: 0.6421030766605798\n",
            "Epoch: 180. Loss: 0.6397817541649264\n",
            "Epoch: 190. Loss: 0.6376501451237168\n",
            "Epoch: 200. Loss: 0.6356944972537402\n",
            "Epoch: 210. Loss: 0.6339018176003954\n",
            "Epoch: 220. Loss: 0.6322598575962552\n",
            "Epoch: 230. Loss: 0.6307570974064871\n",
            "Epoch: 240. Loss: 0.6293827283424447\n",
            "Epoch: 250. Loss: 0.628126632831422\n",
            "Epoch: 260. Loss: 0.62697936187068\n",
            "Epoch: 270. Loss: 0.6259321101654759\n",
            "Epoch: 280. Loss: 0.6249766893075686\n",
            "Epoch: 290. Loss: 0.6241054994278686\n",
            "Epoch: 300. Loss: 0.623311499779935\n",
            "Epoch: 310. Loss: 0.6225881786984582\n",
            "Epoch: 320. Loss: 0.6219295233422868\n",
            "Epoch: 330. Loss: 0.6213299895846475\n",
            "Epoch: 340. Loss: 0.6207844723607597\n",
            "Epoch: 350. Loss: 0.6202882767297186\n",
            "Epoch: 360. Loss: 0.6198370898562514\n",
            "Epoch: 370. Loss: 0.6194269540706451\n",
            "Epoch: 380. Loss: 0.6190542411227329\n",
            "Epoch: 390. Loss: 0.6187156277087827\n",
            "Epoch: 400. Loss: 0.6184080723184345\n",
            "Epoch: 410. Loss: 0.6181287934222873\n",
            "Epoch: 420. Loss: 0.6178752489989296\n",
            "Epoch: 430. Loss: 0.6176451173826882\n",
            "Epoch: 440. Loss: 0.6174362793996252\n",
            "Epoch: 450. Loss: 0.617246801748847\n",
            "Epoch: 460. Loss: 0.6170749215785205\n",
            "Epoch: 470. Loss: 0.616919032200656\n",
            "Epoch: 480. Loss: 0.6167776698853538\n",
            "Epoch: 490. Loss: 0.616649501673409\n",
            "tensor(0.4538, dtype=torch.float64)\n",
            "2022-09-04 00:00:00\n",
            "Epoch: 0. Loss: 1.7243862851301959\n",
            "Epoch: 10. Loss: 1.0570353316670076\n",
            "Epoch: 20. Loss: 0.8901065492408192\n",
            "Epoch: 30. Loss: 0.79830535414582\n",
            "Epoch: 40. Loss: 0.7332009194304631\n",
            "Epoch: 50. Loss: 0.6888578171645694\n",
            "Epoch: 60. Loss: 0.6607322320809887\n",
            "Epoch: 70. Loss: 0.6437960953001928\n",
            "Epoch: 80. Loss: 0.6338347185204528\n",
            "Epoch: 90. Loss: 0.6279639052770224\n",
            "Epoch: 100. Loss: 0.6244304829622701\n",
            "Epoch: 110. Loss: 0.6222300570397908\n",
            "Epoch: 120. Loss: 0.6207996357668583\n",
            "Epoch: 130. Loss: 0.6198238072687298\n",
            "Epoch: 140. Loss: 0.6191239997274637\n",
            "Epoch: 150. Loss: 0.6185974930060815\n",
            "Epoch: 160. Loss: 0.6181840692104699\n",
            "Epoch: 170. Loss: 0.6178476583575713\n",
            "Epoch: 180. Loss: 0.6175661155985926\n",
            "Epoch: 190. Loss: 0.6173254503410418\n",
            "Epoch: 200. Loss: 0.6171165248815834\n",
            "Epoch: 210. Loss: 0.6169331416791533\n",
            "Epoch: 220. Loss: 0.6167709215573559\n",
            "Epoch: 230. Loss: 0.6166266376712268\n",
            "Epoch: 240. Loss: 0.6164978148829392\n",
            "Epoch: 250. Loss: 0.6163824852079758\n",
            "Epoch: 260. Loss: 0.616279035905609\n",
            "Epoch: 270. Loss: 0.616186113099555\n",
            "Epoch: 280. Loss: 0.6161025590402373\n",
            "Epoch: 290. Loss: 0.6160273700038479\n",
            "Epoch: 300. Loss: 0.6159596670442784\n",
            "Epoch: 310. Loss: 0.6158986749019131\n",
            "Epoch: 320. Loss: 0.6158437062104118\n",
            "Epoch: 330. Loss: 0.6157941492419429\n",
            "Epoch: 340. Loss: 0.6157494580930735\n",
            "Epoch: 350. Loss: 0.6157091446145879\n",
            "Epoch: 360. Loss: 0.6156727716334945\n",
            "Epoch: 370. Loss: 0.6156399471666423\n",
            "Epoch: 380. Loss: 0.6156103194197984\n",
            "Epoch: 390. Loss: 0.615583572425986\n",
            "Epoch: 400. Loss: 0.6155594222157127\n",
            "Epoch: 410. Loss: 0.6155376134374777\n",
            "Epoch: 420. Loss: 0.6155179163645638\n",
            "Epoch: 430. Loss: 0.6155001242365179\n",
            "Epoch: 440. Loss: 0.61548405089279\n",
            "Epoch: 450. Loss: 0.6154695286628211\n",
            "Epoch: 460. Loss: 0.6154564064821857\n",
            "Epoch: 470. Loss: 0.6154445482086347\n",
            "Epoch: 480. Loss: 0.6154338311153563\n",
            "Epoch: 490. Loss: 0.6154241445416613\n",
            "tensor(0.4597, dtype=torch.float64)\n",
            "2022-09-11 00:00:00\n",
            "Epoch: 0. Loss: 2.5864074352213366\n",
            "Epoch: 10. Loss: 1.2002115180670618\n",
            "Epoch: 20. Loss: 0.6514092968346042\n",
            "Epoch: 30. Loss: 0.6298500892484962\n",
            "Epoch: 40. Loss: 0.6257696324740258\n",
            "Epoch: 50. Loss: 0.6235072146000562\n",
            "Epoch: 60. Loss: 0.6221649151256903\n",
            "Epoch: 70. Loss: 0.6213499826990021\n",
            "Epoch: 80. Loss: 0.6208471493346963\n",
            "Epoch: 90. Loss: 0.6205318945048417\n",
            "Epoch: 100. Loss: 0.6203304822513872\n",
            "Epoch: 110. Loss: 0.6201987441588492\n",
            "Epoch: 120. Loss: 0.6201100404449437\n",
            "Epoch: 130. Loss: 0.6200482192120735\n",
            "Epoch: 140. Loss: 0.6200034389882211\n",
            "Epoch: 150. Loss: 0.6199696705969389\n",
            "Epoch: 160. Loss: 0.6199431957646859\n",
            "Epoch: 170. Loss: 0.6199217014841006\n",
            "Epoch: 180. Loss: 0.6199037321609004\n",
            "Epoch: 190. Loss: 0.6198883573922062\n",
            "Epoch: 200. Loss: 0.6198749700643942\n",
            "Epoch: 210. Loss: 0.6198631633871418\n",
            "Epoch: 220. Loss: 0.6198526558279537\n",
            "Epoch: 230. Loss: 0.61984324515944\n",
            "Epoch: 240. Loss: 0.6198347802257772\n",
            "Epoch: 250. Loss: 0.6198271435090885\n",
            "Epoch: 260. Loss: 0.619820240288857\n",
            "Epoch: 270. Loss: 0.6198139918341433\n",
            "Epoch: 280. Loss: 0.6198083310691185\n",
            "Epoch: 290. Loss: 0.619803199761144\n",
            "Epoch: 300. Loss: 0.6197985466511464\n",
            "Epoch: 310. Loss: 0.6197943261716764\n",
            "Epoch: 320. Loss: 0.6197904975355283\n",
            "Epoch: 330. Loss: 0.6197870240616203\n",
            "Epoch: 340. Loss: 0.619783872655958\n",
            "Epoch: 350. Loss: 0.6197810133967316\n",
            "Epoch: 360. Loss: 0.6197784191916763\n",
            "Epoch: 370. Loss: 0.6197760654875223\n",
            "Epoch: 380. Loss: 0.61977393001854\n",
            "Epoch: 390. Loss: 0.6197719925856188\n",
            "Epoch: 400. Loss: 0.6197702348600748\n",
            "Epoch: 410. Loss: 0.6197686402081051\n",
            "Epoch: 420. Loss: 0.6197671935329079\n",
            "Epoch: 430. Loss: 0.6197658811321938\n",
            "Epoch: 440. Loss: 0.6197646905692935\n",
            "Epoch: 450. Loss: 0.6197636105563848\n",
            "Epoch: 460. Loss: 0.6197626308485956\n",
            "Epoch: 470. Loss: 0.6197617421479082\n",
            "Epoch: 480. Loss: 0.6197609360159266\n",
            "Epoch: 490. Loss: 0.6197602047946679\n",
            "tensor(0.6065, dtype=torch.float64)\n",
            "2022-09-18 00:00:00\n",
            "Epoch: 0. Loss: 4.146811319184207\n",
            "Epoch: 10. Loss: 1.6807780335401754\n",
            "Epoch: 20. Loss: 1.190447288679811\n",
            "Epoch: 30. Loss: 0.9534167452471797\n",
            "Epoch: 40. Loss: 0.8251121130386478\n",
            "Epoch: 50. Loss: 0.7631967837299733\n",
            "Epoch: 60. Loss: 0.7340013046259741\n",
            "Epoch: 70. Loss: 0.7174480341933298\n",
            "Epoch: 80. Loss: 0.7055121572672928\n",
            "Epoch: 90. Loss: 0.6955628991559072\n",
            "Epoch: 100. Loss: 0.686768577940731\n",
            "Epoch: 110. Loss: 0.6788557503392707\n",
            "Epoch: 120. Loss: 0.6717200664192592\n",
            "Epoch: 130. Loss: 0.6653047405093775\n",
            "Epoch: 140. Loss: 0.6595629306548825\n",
            "Epoch: 150. Loss: 0.6544470521926916\n",
            "Epoch: 160. Loss: 0.6499068962188923\n",
            "Epoch: 170. Loss: 0.6458905423546198\n",
            "Epoch: 180. Loss: 0.6423459669910206\n",
            "Epoch: 190. Loss: 0.6392225874282719\n",
            "Epoch: 200. Loss: 0.6364724812927472\n",
            "Epoch: 210. Loss: 0.6340512229017733\n",
            "Epoch: 220. Loss: 0.6319183636465855\n",
            "Epoch: 230. Loss: 0.6300376173469489\n",
            "Epoch: 240. Loss: 0.6283768206964563\n",
            "Epoch: 250. Loss: 0.626907735770271\n",
            "Epoch: 260. Loss: 0.6256057525117777\n",
            "Epoch: 270. Loss: 0.6244495377622397\n",
            "Epoch: 280. Loss: 0.6234206658804127\n",
            "Epoch: 290. Loss: 0.6225032555492843\n",
            "Epoch: 300. Loss: 0.6216836286088901\n",
            "Epoch: 310. Loss: 0.6209499998895237\n",
            "Epoch: 320. Loss: 0.6202922019721195\n",
            "Epoch: 330. Loss: 0.6197014453346756\n",
            "Epoch: 340. Loss: 0.6191701121481221\n",
            "Epoch: 350. Loss: 0.6186915807492104\n",
            "Epoch: 360. Loss: 0.6182600772616309\n",
            "Epoch: 370. Loss: 0.6178705507301583\n",
            "Epoch: 380. Loss: 0.6175185683009032\n",
            "Epoch: 390. Loss: 0.6172002272979242\n",
            "Epoch: 400. Loss: 0.6169120814274882\n",
            "Epoch: 410. Loss: 0.6166510787323858\n",
            "Epoch: 420. Loss: 0.6164145092885938\n",
            "Epoch: 430. Loss: 0.616199960969265\n",
            "Epoch: 440. Loss: 0.6160052818904087\n",
            "Epoch: 450. Loss: 0.6158285483985044\n",
            "Epoch: 460. Loss: 0.615668037665776\n",
            "Epoch: 470. Loss: 0.6155222041285173\n",
            "Epoch: 480. Loss: 0.6153896591428017\n",
            "Epoch: 490. Loss: 0.615269153345031\n",
            "tensor(0.5402, dtype=torch.float64)\n",
            "2022-09-25 00:00:00\n",
            "Epoch: 0. Loss: 2.6138551206802627\n",
            "Epoch: 10. Loss: 1.5762166242058018\n",
            "Epoch: 20. Loss: 1.089826510441577\n",
            "Epoch: 30. Loss: 0.8548251275607013\n",
            "Epoch: 40. Loss: 0.728267737950383\n",
            "Epoch: 50. Loss: 0.6740609315810608\n",
            "Epoch: 60. Loss: 0.6522238805855272\n",
            "Epoch: 70. Loss: 0.6418377035374232\n",
            "Epoch: 80. Loss: 0.6359444307318135\n",
            "Epoch: 90. Loss: 0.6322050950382476\n",
            "Epoch: 100. Loss: 0.6296741429894264\n",
            "Epoch: 110. Loss: 0.6278839875821359\n",
            "Epoch: 120. Loss: 0.6265681152391374\n",
            "Epoch: 130. Loss: 0.6255633137785811\n",
            "Epoch: 140. Loss: 0.6247667828674214\n",
            "Epoch: 150. Loss: 0.6241131610844722\n",
            "Epoch: 160. Loss: 0.623560696553957\n",
            "Epoch: 170. Loss: 0.6230825399775847\n",
            "Epoch: 180. Loss: 0.6226612151168744\n",
            "Epoch: 190. Loss: 0.6222851232428105\n",
            "Epoch: 200. Loss: 0.6219463504416304\n",
            "Epoch: 210. Loss: 0.6216393020251447\n",
            "Epoch: 220. Loss: 0.6213598565967007\n",
            "Epoch: 230. Loss: 0.6211048436178163\n",
            "Epoch: 240. Loss: 0.6208717208771437\n",
            "Epoch: 250. Loss: 0.6206583747955513\n",
            "Epoch: 260. Loss: 0.6204629959182281\n",
            "Epoch: 270. Loss: 0.6202840003276329\n",
            "Epoch: 280. Loss: 0.6201199790940256\n",
            "Epoch: 290. Loss: 0.6199696648782251\n",
            "Epoch: 300. Loss: 0.6198319090790132\n",
            "Epoch: 310. Loss: 0.6197056655208709\n",
            "Epoch: 320. Loss: 0.6195899782563904\n",
            "Epoch: 330. Loss: 0.6194839720123891\n",
            "Epoch: 340. Loss: 0.6193868443849292\n",
            "Epoch: 350. Loss: 0.6192978592357007\n",
            "Epoch: 360. Loss: 0.6192163409514028\n",
            "Epoch: 370. Loss: 0.6191416693537997\n",
            "Epoch: 380. Loss: 0.6190732751241904\n",
            "Epoch: 390. Loss: 0.6190106356521116\n",
            "Epoch: 400. Loss: 0.6189532712461544\n",
            "Epoch: 410. Loss: 0.618900741662057\n",
            "Epoch: 420. Loss: 0.61885264291403\n",
            "Epoch: 430. Loss: 0.6188086043421755\n",
            "Epoch: 440. Loss: 0.6187682859134225\n",
            "Epoch: 450. Loss: 0.6187313757365265\n",
            "Epoch: 460. Loss: 0.6186975877739613\n",
            "Epoch: 470. Loss: 0.6186666597352483\n",
            "Epoch: 480. Loss: 0.6186383511376569\n",
            "Epoch: 490. Loss: 0.6186124415213781\n",
            "tensor(0.5437, dtype=torch.float64)\n",
            "2022-10-02 00:00:00\n",
            "Epoch: 0. Loss: 2.473751138636806\n",
            "Epoch: 10. Loss: 1.130865320857274\n",
            "Epoch: 20. Loss: 0.850656859229629\n",
            "Epoch: 30. Loss: 0.7412968862626407\n",
            "Epoch: 40. Loss: 0.696133929559559\n",
            "Epoch: 50. Loss: 0.6742957782003298\n",
            "Epoch: 60. Loss: 0.661445841357949\n",
            "Epoch: 70. Loss: 0.6529710372502092\n",
            "Epoch: 80. Loss: 0.6470528837368142\n",
            "Epoch: 90. Loss: 0.6427785641588529\n",
            "Epoch: 100. Loss: 0.6396032720263095\n",
            "Epoch: 110. Loss: 0.6371738920727711\n",
            "Epoch: 120. Loss: 0.6352557595612366\n",
            "Epoch: 130. Loss: 0.63369279464156\n",
            "Epoch: 140. Loss: 0.6323817324676575\n",
            "Epoch: 150. Loss: 0.6312544842941779\n",
            "Epoch: 160. Loss: 0.6302660651051814\n",
            "Epoch: 170. Loss: 0.6293864764893854\n",
            "Epoch: 180. Loss: 0.6285953558909692\n",
            "Epoch: 190. Loss: 0.6278785094016267\n",
            "Epoch: 200. Loss: 0.6272256953440171\n",
            "Epoch: 210. Loss: 0.6266292216683154\n",
            "Epoch: 220. Loss: 0.6260830644103005\n",
            "Epoch: 230. Loss: 0.6255823156308862\n",
            "Epoch: 240. Loss: 0.625122837698262\n",
            "Epoch: 250. Loss: 0.6247010458556983\n",
            "Epoch: 260. Loss: 0.6243137701292082\n",
            "Epoch: 270. Loss: 0.6239581661458323\n",
            "Epoch: 280. Loss: 0.6236316560740008\n",
            "Epoch: 290. Loss: 0.6233318881481172\n",
            "Epoch: 300. Loss: 0.6230567077222585\n",
            "Epoch: 310. Loss: 0.6228041355525444\n",
            "Epoch: 320. Loss: 0.6225723506918693\n",
            "Epoch: 330. Loss: 0.6223596764059676\n",
            "Epoch: 340. Loss: 0.6221645681416417\n",
            "Epoch: 350. Loss: 0.6219856029539755\n",
            "Epoch: 360. Loss: 0.6218214700260662\n",
            "Epoch: 370. Loss: 0.6216709620512066\n",
            "Epoch: 380. Loss: 0.6215329673293849\n",
            "Epoch: 390. Loss: 0.6214064624791615\n",
            "Epoch: 400. Loss: 0.6212905056955658\n",
            "Epoch: 410. Loss: 0.6211842305025255\n",
            "Epoch: 420. Loss: 0.6210868399592507\n",
            "Epoch: 430. Loss: 0.6209976012867889\n",
            "Epoch: 440. Loss: 0.6209158408853334\n",
            "Epoch: 450. Loss: 0.620840939715824\n",
            "Epoch: 460. Loss: 0.6207723290215099\n",
            "Epoch: 470. Loss: 0.620709486366803\n",
            "Epoch: 480. Loss: 0.6206519319721361\n",
            "Epoch: 490. Loss: 0.6205992253247677\n",
            "tensor(0.6375, dtype=torch.float64)\n",
            "2022-10-09 00:00:00\n",
            "Epoch: 0. Loss: 1.1309270883532163\n",
            "Epoch: 10. Loss: 0.9177432798180963\n",
            "Epoch: 20. Loss: 0.8044550773798946\n",
            "Epoch: 30. Loss: 0.7340328742228611\n",
            "Epoch: 40. Loss: 0.692766430601517\n",
            "Epoch: 50. Loss: 0.6692307152111668\n",
            "Epoch: 60. Loss: 0.6556860828405732\n",
            "Epoch: 70. Loss: 0.6476913984428284\n",
            "Epoch: 80. Loss: 0.6428049381572472\n",
            "Epoch: 90. Loss: 0.6396847217767145\n",
            "Epoch: 100. Loss: 0.6375866132891953\n",
            "Epoch: 110. Loss: 0.636093600837825\n",
            "Epoch: 120. Loss: 0.6349693410523246\n",
            "Epoch: 130. Loss: 0.6340782277756937\n",
            "Epoch: 140. Loss: 0.6333413437241148\n",
            "Epoch: 150. Loss: 0.6327119529219992\n",
            "Epoch: 160. Loss: 0.6321617431793516\n",
            "Epoch: 170. Loss: 0.6316730420147656\n",
            "Epoch: 180. Loss: 0.631234382419126\n",
            "Epoch: 190. Loss: 0.6308379620987498\n",
            "Epoch: 200. Loss: 0.6304781799745769\n",
            "Epoch: 210. Loss: 0.6301507887183966\n",
            "Epoch: 220. Loss: 0.6298524009735684\n",
            "Epoch: 230. Loss: 0.629580199263407\n",
            "Epoch: 240. Loss: 0.6293317635071003\n",
            "Epoch: 250. Loss: 0.6291049666159358\n",
            "Epoch: 260. Loss: 0.6288979096266591\n",
            "Epoch: 270. Loss: 0.6287088799061641\n",
            "Epoch: 280. Loss: 0.6285363229234897\n",
            "Epoch: 290. Loss: 0.628378822100764\n",
            "Epoch: 300. Loss: 0.628235083570992\n",
            "Epoch: 310. Loss: 0.6281039240058859\n",
            "Epoch: 320. Loss: 0.6279842604459914\n",
            "Epoch: 330. Loss: 0.6278751015077801\n",
            "Epoch: 340. Loss: 0.6277755395966074\n",
            "Epoch: 350. Loss: 0.6276847439004011\n",
            "Epoch: 360. Loss: 0.627601954022792\n",
            "Epoch: 370. Loss: 0.6275264741626557\n",
            "Epoch: 380. Loss: 0.627457667775005\n",
            "Epoch: 390. Loss: 0.6273949526645768\n",
            "Epoch: 400. Loss: 0.6273377964733217\n",
            "Epoch: 410. Loss: 0.6272857125291588\n",
            "Epoch: 420. Loss: 0.6272382560274286\n",
            "Epoch: 430. Loss: 0.6271950205193607\n",
            "Epoch: 440. Loss: 0.6271556346840882\n",
            "Epoch: 450. Loss: 0.6271197593625586\n",
            "Epoch: 460. Loss: 0.627087084833271\n",
            "Epoch: 470. Loss: 0.6270573283112\n",
            "Epoch: 480. Loss: 0.6270302316525719\n",
            "Epoch: 490. Loss: 0.6270055592493949\n",
            "tensor(0.4393, dtype=torch.float64)\n",
            "2022-10-16 00:00:00\n",
            "Epoch: 0. Loss: 6.818301801522125\n",
            "Epoch: 10. Loss: 1.147574246490565\n",
            "Epoch: 20. Loss: 0.6968698375880283\n",
            "Epoch: 30. Loss: 0.6533533903359628\n",
            "Epoch: 40. Loss: 0.6414858263653727\n",
            "Epoch: 50. Loss: 0.637107579971133\n",
            "Epoch: 60. Loss: 0.6350927632270839\n",
            "Epoch: 70. Loss: 0.6340632935538705\n",
            "Epoch: 80. Loss: 0.6335100716277021\n",
            "Epoch: 90. Loss: 0.6332024548901471\n",
            "Epoch: 100. Loss: 0.6330253696705843\n",
            "Epoch: 110. Loss: 0.6329189504609055\n",
            "Epoch: 120. Loss: 0.6328514347795988\n",
            "Epoch: 130. Loss: 0.6328057767713056\n",
            "Epoch: 140. Loss: 0.6327727461935418\n",
            "Epoch: 150. Loss: 0.6327472978322913\n",
            "Epoch: 160. Loss: 0.6327266406133973\n",
            "Epoch: 170. Loss: 0.6327092038096656\n",
            "Epoch: 180. Loss: 0.6326940809968016\n",
            "Epoch: 190. Loss: 0.6326807299146416\n",
            "Epoch: 200. Loss: 0.6326688099566732\n",
            "Epoch: 210. Loss: 0.6326580938947631\n",
            "Epoch: 220. Loss: 0.6326484197358249\n",
            "Epoch: 230. Loss: 0.632639664315131\n",
            "Epoch: 240. Loss: 0.6326317286847065\n",
            "Epoch: 250. Loss: 0.6326245299162931\n",
            "Epoch: 260. Loss: 0.6326179964036058\n",
            "Epoch: 270. Loss: 0.6326120650827122\n",
            "Epoch: 280. Loss: 0.6326066797119735\n",
            "Epoch: 290. Loss: 0.6326017897446279\n",
            "Epoch: 300. Loss: 0.6325973495394565\n",
            "Epoch: 310. Loss: 0.63259331777019\n",
            "Epoch: 320. Loss: 0.6325896569568539\n",
            "Epoch: 330. Loss: 0.6325863330762491\n",
            "Epoch: 340. Loss: 0.6325833152272718\n",
            "Epoch: 350. Loss: 0.6325805753368919\n",
            "Epoch: 360. Loss: 0.6325780878981706\n",
            "Epoch: 370. Loss: 0.632575829734782\n",
            "Epoch: 380. Loss: 0.6325737797882472\n",
            "Epoch: 390. Loss: 0.6325719189250951\n",
            "Epoch: 400. Loss: 0.6325702297617769\n",
            "Epoch: 410. Loss: 0.632568696505539\n",
            "Epoch: 420. Loss: 0.632567304809729\n",
            "Epoch: 430. Loss: 0.632566041642188\n",
            "Epoch: 440. Loss: 0.6325648951655382\n",
            "Epoch: 450. Loss: 0.6325638546282885\n",
            "Epoch: 460. Loss: 0.6325629102657883\n",
            "Epoch: 470. Loss: 0.6325620532101425\n",
            "Epoch: 480. Loss: 0.632561275408289\n",
            "Epoch: 490. Loss: 0.6325605695475021\n",
            "tensor(0.6701, dtype=torch.float64)\n",
            "2022-10-23 00:00:00\n",
            "Epoch: 0. Loss: 3.2987278051229016\n",
            "Epoch: 10. Loss: 0.7095532266516601\n",
            "Epoch: 20. Loss: 0.6541293781541602\n",
            "Epoch: 30. Loss: 0.6450651513730377\n",
            "Epoch: 40. Loss: 0.6414097283979676\n",
            "Epoch: 50. Loss: 0.6396875840286893\n",
            "Epoch: 60. Loss: 0.6388235672111378\n",
            "Epoch: 70. Loss: 0.6383782308089238\n",
            "Epoch: 80. Loss: 0.638145612927437\n",
            "Epoch: 90. Loss: 0.6380231304088914\n",
            "Epoch: 100. Loss: 0.6379582568764598\n",
            "Epoch: 110. Loss: 0.6379237121140819\n",
            "Epoch: 120. Loss: 0.6379052081228936\n",
            "Epoch: 130. Loss: 0.6378952197195609\n",
            "Epoch: 140. Loss: 0.6378897676909037\n",
            "Epoch: 150. Loss: 0.6378867414696783\n",
            "Epoch: 160. Loss: 0.6378850188101038\n",
            "Epoch: 170. Loss: 0.637884001606821\n",
            "Epoch: 180. Loss: 0.6378833703073596\n",
            "Epoch: 190. Loss: 0.6378829536497815\n",
            "Epoch: 200. Loss: 0.6378826594388461\n",
            "Epoch: 210. Loss: 0.6378824377051714\n",
            "Epoch: 220. Loss: 0.6378822610752188\n",
            "Epoch: 230. Loss: 0.6378821143007092\n",
            "Epoch: 240. Loss: 0.6378819886676498\n",
            "Epoch: 250. Loss: 0.6378818790070954\n",
            "Epoch: 260. Loss: 0.6378817820941286\n",
            "Epoch: 270. Loss: 0.6378816957880863\n",
            "Epoch: 280. Loss: 0.6378816185689106\n",
            "Epoch: 290. Loss: 0.6378815492854232\n",
            "Epoch: 300. Loss: 0.6378814870171657\n",
            "Epoch: 310. Loss: 0.6378814309972569\n",
            "Epoch: 320. Loss: 0.6378813805681701\n",
            "Epoch: 330. Loss: 0.6378813351553939\n",
            "Epoch: 340. Loss: 0.6378812942509181\n",
            "Epoch: 350. Loss: 0.6378812574022075\n",
            "Epoch: 360. Loss: 0.6378812242043311\n",
            "Epoch: 370. Loss: 0.6378811942939715\n",
            "Epoch: 380. Loss: 0.6378811673446168\n",
            "Epoch: 390. Loss: 0.6378811430625447\n",
            "Epoch: 400. Loss: 0.6378811211833746\n",
            "Epoch: 410. Loss: 0.637881101469051\n",
            "Epoch: 420. Loss: 0.6378810837051756\n",
            "Epoch: 430. Loss: 0.6378810676986292\n",
            "Epoch: 440. Loss: 0.637881053275446\n",
            "Epoch: 450. Loss: 0.6378810402789034\n",
            "Epoch: 460. Loss: 0.6378810285678085\n",
            "Epoch: 470. Loss: 0.6378810180149552\n",
            "Epoch: 480. Loss: 0.6378810085057353\n",
            "Epoch: 490. Loss: 0.6378809999368904\n",
            "tensor(0.4433, dtype=torch.float64)\n",
            "2022-10-30 00:00:00\n",
            "Epoch: 0. Loss: 1.3757627144987474\n",
            "Epoch: 10. Loss: 0.729147435180628\n",
            "Epoch: 20. Loss: 0.6864043593158752\n",
            "Epoch: 30. Loss: 0.6746912732488461\n",
            "Epoch: 40. Loss: 0.668698440060476\n",
            "Epoch: 50. Loss: 0.6646797155368694\n",
            "Epoch: 60. Loss: 0.6615356024412069\n",
            "Epoch: 70. Loss: 0.6589236306297512\n",
            "Epoch: 80. Loss: 0.6567087473456344\n",
            "Epoch: 90. Loss: 0.6548163776282919\n",
            "Epoch: 100. Loss: 0.6531939364102531\n",
            "Epoch: 110. Loss: 0.651799831920924\n",
            "Epoch: 120. Loss: 0.6505996666310653\n",
            "Epoch: 130. Loss: 0.649564527716935\n",
            "Epoch: 140. Loss: 0.6486699821522012\n",
            "Epoch: 150. Loss: 0.6478953565329607\n",
            "Epoch: 160. Loss: 0.6472231599794399\n",
            "Epoch: 170. Loss: 0.6466385980827535\n",
            "Epoch: 180. Loss: 0.6461291567187222\n",
            "Epoch: 190. Loss: 0.6456842451828633\n",
            "Epoch: 200. Loss: 0.6452948915419551\n",
            "Epoch: 210. Loss: 0.6449534841161871\n",
            "Epoch: 220. Loss: 0.6446535533612753\n",
            "Epoch: 230. Loss: 0.6443895887122097\n",
            "Epoch: 240. Loss: 0.6441568853359178\n",
            "Epoch: 250. Loss: 0.6439514162191867\n",
            "Epoch: 260. Loss: 0.6437697255480936\n",
            "Epoch: 270. Loss: 0.6436088398722923\n",
            "Epoch: 280. Loss: 0.6434661940596753\n",
            "Epoch: 290. Loss: 0.6433395695146079\n",
            "Epoch: 300. Loss: 0.6432270425468719\n",
            "Epoch: 310. Loss: 0.6431269411364599\n",
            "Epoch: 320. Loss: 0.6430378086437555\n",
            "Epoch: 330. Loss: 0.6429583732701192\n",
            "Epoch: 340. Loss: 0.6428875222863389\n",
            "Epoch: 350. Loss: 0.642824280221792\n",
            "Epoch: 360. Loss: 0.6427677903512595\n",
            "Epoch: 370. Loss: 0.642717298934328\n",
            "Epoch: 380. Loss: 0.6426721417587252\n",
            "Epoch: 390. Loss: 0.6426317326176395\n",
            "Epoch: 400. Loss: 0.6425955534153228\n",
            "Epoch: 410. Loss: 0.6425631456477422\n",
            "Epoch: 420. Loss: 0.6425341030479497\n",
            "Epoch: 430. Loss: 0.64250806522098\n",
            "Epoch: 440. Loss: 0.6424847121219288\n",
            "Epoch: 450. Loss: 0.6424637592545773\n",
            "Epoch: 460. Loss: 0.6424449534875049\n",
            "Epoch: 470. Loss: 0.6424280694007997\n",
            "Epoch: 480. Loss: 0.6424129060899028\n",
            "Epoch: 490. Loss: 0.6423992843642732\n",
            "tensor(0.5636, dtype=torch.float64)\n",
            "2022-11-06 00:00:00\n",
            "Epoch: 0. Loss: 2.8400073555442855\n",
            "Epoch: 10. Loss: 1.1029172412001245\n",
            "Epoch: 20. Loss: 0.73836675528833\n",
            "Epoch: 30. Loss: 0.6856581118387302\n",
            "Epoch: 40. Loss: 0.6657850133707792\n",
            "Epoch: 50. Loss: 0.6574949447064203\n",
            "Epoch: 60. Loss: 0.6537484706950915\n",
            "Epoch: 70. Loss: 0.6519629032937669\n",
            "Epoch: 80. Loss: 0.6510863798744322\n",
            "Epoch: 90. Loss: 0.6506483420424151\n",
            "Epoch: 100. Loss: 0.650426488307391\n",
            "Epoch: 110. Loss: 0.6503126549446568\n",
            "Epoch: 120. Loss: 0.6502533086207046\n",
            "Epoch: 130. Loss: 0.6502216654814378\n",
            "Epoch: 140. Loss: 0.6502042238404109\n",
            "Epoch: 150. Loss: 0.650194138395834\n",
            "Epoch: 160. Loss: 0.6501879212320115\n",
            "Epoch: 170. Loss: 0.6501837857865264\n",
            "Epoch: 180. Loss: 0.650180810787387\n",
            "Epoch: 190. Loss: 0.6501785164344505\n",
            "Epoch: 200. Loss: 0.6501766489159224\n",
            "Epoch: 210. Loss: 0.6501750705981516\n",
            "Epoch: 220. Loss: 0.6501737039559157\n",
            "Epoch: 230. Loss: 0.6501725028840165\n",
            "Epoch: 240. Loss: 0.6501714379796727\n",
            "Epoch: 250. Loss: 0.6501704889617334\n",
            "Epoch: 260. Loss: 0.6501696407401208\n",
            "Epoch: 270. Loss: 0.6501688813552046\n",
            "Epoch: 280. Loss: 0.6501682008774544\n",
            "Epoch: 290. Loss: 0.650167590802265\n",
            "Epoch: 300. Loss: 0.650167043701932\n",
            "Epoch: 310. Loss: 0.6501665530127911\n",
            "Epoch: 320. Loss: 0.6501661128948555\n",
            "Epoch: 330. Loss: 0.6501657181316086\n",
            "Epoch: 340. Loss: 0.6501653640531393\n",
            "Epoch: 350. Loss: 0.6501650464737593\n",
            "Epoch: 360. Loss: 0.6501647616393291\n",
            "Epoch: 370. Loss: 0.6501645061816302\n",
            "Epoch: 380. Loss: 0.6501642770782183\n",
            "Epoch: 390. Loss: 0.6501640716167724\n",
            "Epoch: 400. Loss: 0.6501638873632707\n",
            "Epoch: 410. Loss: 0.6501637221334976\n",
            "Epoch: 420. Loss: 0.6501635739674886\n",
            "Epoch: 430. Loss: 0.6501634411065967\n",
            "Epoch: 440. Loss: 0.6501633219728975\n",
            "Epoch: 450. Loss: 0.6501632151506928\n",
            "Epoch: 460. Loss: 0.6501631193698978\n",
            "Epoch: 470. Loss: 0.6501630334911224\n",
            "Epoch: 480. Loss: 0.6501629564922706\n",
            "Epoch: 490. Loss: 0.6501628874565084\n",
            "tensor(0.5751, dtype=torch.float64)\n",
            "2022-11-13 00:00:00\n",
            "Epoch: 0. Loss: 1.3862822247425193\n",
            "Epoch: 10. Loss: 0.8134665780524816\n",
            "Epoch: 20. Loss: 0.6924094053934489\n",
            "Epoch: 30. Loss: 0.6693083137184321\n",
            "Epoch: 40. Loss: 0.6644508358033885\n",
            "Epoch: 50. Loss: 0.6625925315099574\n",
            "Epoch: 60. Loss: 0.6613884511190924\n",
            "Epoch: 70. Loss: 0.6604194961344811\n",
            "Epoch: 80. Loss: 0.6595836929801845\n",
            "Epoch: 90. Loss: 0.6588436761779423\n",
            "Epoch: 100. Loss: 0.6581803532555802\n",
            "Epoch: 110. Loss: 0.6575819051667025\n",
            "Epoch: 120. Loss: 0.6570401154531108\n",
            "Epoch: 130. Loss: 0.656548764005264\n",
            "Epoch: 140. Loss: 0.6561028216842902\n",
            "Epoch: 150. Loss: 0.6556980228192669\n",
            "Epoch: 160. Loss: 0.6553306311354302\n",
            "Epoch: 170. Loss: 0.6549973075565815\n",
            "Epoch: 180. Loss: 0.6546950322405723\n",
            "Epoch: 190. Loss: 0.6544210557019485\n",
            "Epoch: 200. Loss: 0.6541728657085352\n",
            "Epoch: 210. Loss: 0.6539481629064\n",
            "Epoch: 220. Loss: 0.6537448414480059\n",
            "Epoch: 230. Loss: 0.6535609726529246\n",
            "Epoch: 240. Loss: 0.6533947906539898\n",
            "Epoch: 250. Loss: 0.653244679465553\n",
            "Epoch: 260. Loss: 0.6531091611625692\n",
            "Epoch: 270. Loss: 0.6529868849899368\n",
            "Epoch: 280. Loss: 0.652876617289066\n",
            "Epoch: 290. Loss: 0.6527772321636752\n",
            "Epoch: 300. Loss: 0.6526877028252678\n",
            "Epoch: 310. Loss: 0.6526070935688543\n",
            "Epoch: 320. Loss: 0.6525345523354914\n",
            "Epoch: 330. Loss: 0.6524693038221854\n",
            "Epoch: 340. Loss: 0.6524106431027029\n",
            "Epoch: 350. Loss: 0.6523579297253255\n",
            "Epoch: 360. Loss: 0.6523105822557999\n",
            "Epoch: 370. Loss: 0.6522680732357568\n",
            "Epoch: 380. Loss: 0.6522299245287495\n",
            "Epoch: 390. Loss: 0.6521957030277983\n",
            "Epoch: 400. Loss: 0.6521650166999459\n",
            "Epoch: 410. Loss: 0.6521375109448133\n",
            "Epoch: 420. Loss: 0.652112865245537\n",
            "Epoch: 430. Loss: 0.6520907900917471\n",
            "Epoch: 440. Loss: 0.6520710241554437\n",
            "Epoch: 450. Loss: 0.652053331701749\n",
            "Epoch: 460. Loss: 0.6520375002175588\n",
            "Epoch: 470. Loss: 0.6520233382421176\n",
            "Epoch: 480. Loss: 0.6520106733844764\n",
            "Epoch: 490. Loss: 0.6519993505136963\n",
            "tensor(0.5228, dtype=torch.float64)\n",
            "2022-11-20 00:00:00\n",
            "Epoch: 0. Loss: 2.144018957322123\n",
            "Epoch: 10. Loss: 0.8108983254414623\n",
            "Epoch: 20. Loss: 0.7207046543638679\n",
            "Epoch: 30. Loss: 0.6858123493496981\n",
            "Epoch: 40. Loss: 0.6679590963147157\n",
            "Epoch: 50. Loss: 0.6583989798263586\n",
            "Epoch: 60. Loss: 0.653001651576433\n",
            "Epoch: 70. Loss: 0.6498751077240845\n",
            "Epoch: 80. Loss: 0.6480596986705374\n",
            "Epoch: 90. Loss: 0.6470134352173209\n",
            "Epoch: 100. Loss: 0.6464160813867132\n",
            "Epoch: 110. Loss: 0.6460778216020497\n",
            "Epoch: 120. Loss: 0.645887449041821\n",
            "Epoch: 130. Loss: 0.6457807135041108\n",
            "Epoch: 140. Loss: 0.6457209456597033\n",
            "Epoch: 150. Loss: 0.6456874202633701\n",
            "Epoch: 160. Loss: 0.6456685103361534\n",
            "Epoch: 170. Loss: 0.6456577289397409\n",
            "Epoch: 180. Loss: 0.6456514710788048\n",
            "Epoch: 190. Loss: 0.6456477385078966\n",
            "Epoch: 200. Loss: 0.6456454250571839\n",
            "Epoch: 210. Loss: 0.6456439183619057\n",
            "Epoch: 220. Loss: 0.6456428788493984\n",
            "Epoch: 230. Loss: 0.6456421174730145\n",
            "Epoch: 240. Loss: 0.6456415282260509\n",
            "Epoch: 250. Loss: 0.6456410509502909\n",
            "Epoch: 260. Loss: 0.6456406508594087\n",
            "Epoch: 270. Loss: 0.6456403072690574\n",
            "Epoch: 280. Loss: 0.6456400073919644\n",
            "Epoch: 290. Loss: 0.6456397429173452\n",
            "Epoch: 300. Loss: 0.6456395081203395\n",
            "Epoch: 310. Loss: 0.6456392988122813\n",
            "Epoch: 320. Loss: 0.6456391117533677\n",
            "Epoch: 330. Loss: 0.6456389443199961\n",
            "Epoch: 340. Loss: 0.6456387943127463\n",
            "Epoch: 350. Loss: 0.6456386598424019\n",
            "Epoch: 360. Loss: 0.645638539259609\n",
            "Epoch: 370. Loss: 0.6456384311092325\n",
            "Epoch: 380. Loss: 0.6456383340989604\n",
            "Epoch: 390. Loss: 0.6456382470763569\n",
            "Epoch: 400. Loss: 0.6456381690111269\n",
            "Epoch: 410. Loss: 0.6456380989807572\n",
            "Epoch: 420. Loss: 0.6456380361584774\n",
            "Epoch: 430. Loss: 0.6456379798029187\n",
            "Epoch: 440. Loss: 0.6456379292490807\n",
            "Epoch: 450. Loss: 0.6456378839003567\n",
            "Epoch: 460. Loss: 0.6456378432214499\n",
            "Epoch: 470. Loss: 0.6456378067320526\n",
            "Epoch: 480. Loss: 0.6456377740011908\n",
            "Epoch: 490. Loss: 0.6456377446421593\n",
            "tensor(0.6401, dtype=torch.float64)\n",
            "2022-11-27 00:00:00\n",
            "Epoch: 0. Loss: 1.8699751511701541\n",
            "Epoch: 10. Loss: 0.8961230886732743\n",
            "Epoch: 20. Loss: 0.761718556059076\n",
            "Epoch: 30. Loss: 0.7151865866368607\n",
            "Epoch: 40. Loss: 0.6897361236829392\n",
            "Epoch: 50. Loss: 0.6741006363238716\n",
            "Epoch: 60. Loss: 0.664230763922238\n",
            "Epoch: 70. Loss: 0.6580053686141963\n",
            "Epoch: 80. Loss: 0.6540475658107798\n",
            "Epoch: 90. Loss: 0.6514576692253485\n",
            "Epoch: 100. Loss: 0.6496822888728138\n",
            "Epoch: 110. Loss: 0.6483984100387953\n",
            "Epoch: 120. Loss: 0.6474224094096716\n",
            "Epoch: 130. Loss: 0.6466498779411419\n",
            "Epoch: 140. Loss: 0.6460199883915744\n",
            "Epoch: 150. Loss: 0.6454957079495754\n",
            "Epoch: 160. Loss: 0.6450531701140928\n",
            "Epoch: 170. Loss: 0.6446760307026498\n",
            "Epoch: 180. Loss: 0.644352451837184\n",
            "Epoch: 190. Loss: 0.6440734544665309\n",
            "Epoch: 200. Loss: 0.6438319851186096\n",
            "Epoch: 210. Loss: 0.6436223610172696\n",
            "Epoch: 220. Loss: 0.643439921136918\n",
            "Epoch: 230. Loss: 0.6432807937448118\n",
            "Epoch: 240. Loss: 0.6431417330788922\n",
            "Epoch: 250. Loss: 0.6430199993324132\n",
            "Epoch: 260. Loss: 0.6429132672927913\n",
            "Epoch: 270. Loss: 0.6428195549226636\n",
            "Epoch: 280. Loss: 0.6427371664296224\n",
            "Epoch: 290. Loss: 0.6426646462297941\n",
            "Epoch: 300. Loss: 0.6426007413199467\n",
            "Epoch: 310. Loss: 0.6425443702683088\n",
            "Epoch: 320. Loss: 0.6424945974919706\n",
            "Epoch: 330. Loss: 0.6424506118037594\n",
            "Epoch: 340. Loss: 0.6424117084368052\n",
            "Epoch: 350. Loss: 0.6423772739213608\n",
            "Epoch: 360. Loss: 0.642346773314281\n",
            "Epoch: 370. Loss: 0.6423197393786101\n",
            "Epoch: 380. Loss: 0.6422957633866275\n",
            "Epoch: 390. Loss: 0.6422744872797488\n",
            "Epoch: 400. Loss: 0.642255596966584\n",
            "Epoch: 410. Loss: 0.642238816578958\n",
            "Epoch: 420. Loss: 0.6422239035368058\n",
            "Epoch: 430. Loss: 0.6422106442981423\n",
            "Epoch: 440. Loss: 0.6421988506909267\n",
            "Epoch: 450. Loss: 0.6421883567405462\n",
            "Epoch: 460. Loss: 0.6421790159205422\n",
            "Epoch: 470. Loss: 0.6421706987656788\n",
            "Epoch: 480. Loss: 0.6421632907959548\n",
            "Epoch: 490. Loss: 0.6421566907080557\n",
            "tensor(0.5836, dtype=torch.float64)\n",
            "2022-12-04 00:00:00\n",
            "Epoch: 0. Loss: 2.782080523444153\n",
            "Epoch: 10. Loss: 1.2309552662726981\n",
            "Epoch: 20. Loss: 0.723365605173122\n",
            "Epoch: 30. Loss: 0.6821879077073647\n",
            "Epoch: 40. Loss: 0.6674451617048275\n",
            "Epoch: 50. Loss: 0.6589028812642644\n",
            "Epoch: 60. Loss: 0.6535226270850548\n",
            "Epoch: 70. Loss: 0.6499098965963497\n",
            "Epoch: 80. Loss: 0.6473314204588373\n",
            "Epoch: 90. Loss: 0.6453815390407451\n",
            "Epoch: 100. Loss: 0.6438297600521009\n",
            "Epoch: 110. Loss: 0.6425423744124326\n",
            "Epoch: 120. Loss: 0.6414401607633102\n",
            "Epoch: 130. Loss: 0.64047500407242\n",
            "Epoch: 140. Loss: 0.6396167651084524\n",
            "Epoch: 150. Loss: 0.6388458185546392\n",
            "Epoch: 160. Loss: 0.6381487722132476\n",
            "Epoch: 170. Loss: 0.6375159919654955\n",
            "Epoch: 180. Loss: 0.6369401610123843\n",
            "Epoch: 190. Loss: 0.6364154355859222\n",
            "Epoch: 200. Loss: 0.635936946389844\n",
            "Epoch: 210. Loss: 0.6355005011713734\n",
            "Epoch: 220. Loss: 0.6351024046255187\n",
            "Epoch: 230. Loss: 0.6347393469134032\n",
            "Epoch: 240. Loss: 0.6344083324217337\n",
            "Epoch: 250. Loss: 0.6341066322305685\n",
            "Epoch: 260. Loss: 0.6338317506593631\n",
            "Epoch: 270. Loss: 0.6335814002874678\n",
            "Epoch: 280. Loss: 0.6333534821917534\n",
            "Epoch: 290. Loss: 0.6331460695090517\n",
            "Epoch: 300. Loss: 0.6329573932227767\n",
            "Epoch: 310. Loss: 0.6327858295304778\n",
            "Epoch: 320. Loss: 0.6326298884120215\n",
            "Epoch: 330. Loss: 0.6324882031684137\n",
            "Epoch: 340. Loss: 0.6323595207867022\n",
            "Epoch: 350. Loss: 0.6322426930346995\n",
            "Epoch: 360. Loss: 0.6321366682164997\n",
            "Epoch: 370. Loss: 0.6320404835352116\n",
            "Epoch: 380. Loss: 0.6319532580182542\n",
            "Epoch: 390. Loss: 0.6318741859659641\n",
            "Epoch: 400. Loss: 0.6318025308877876\n",
            "Epoch: 410. Loss: 0.6317376198929067\n",
            "Epoch: 420. Loss: 0.6316788385042386\n",
            "Epoch: 430. Loss: 0.6316256258665963\n",
            "Epoch: 440. Loss: 0.6315774703215379\n",
            "Epoch: 450. Loss: 0.6315339053230797\n",
            "Epoch: 460. Loss: 0.6314945056700585\n",
            "Epoch: 470. Loss: 0.6314588840324551\n",
            "Epoch: 480. Loss: 0.6314266877504747\n",
            "Epoch: 490. Loss: 0.6313975958865771\n",
            "tensor(0.7127, dtype=torch.float64)\n",
            "2022-12-11 00:00:00\n",
            "Epoch: 0. Loss: 2.53281310077016\n",
            "Epoch: 10. Loss: 1.3149760843128318\n",
            "Epoch: 20. Loss: 0.8334579637298501\n",
            "Epoch: 30. Loss: 0.7315573492437459\n",
            "Epoch: 40. Loss: 0.6935185634562384\n",
            "Epoch: 50. Loss: 0.6755112489015952\n",
            "Epoch: 60. Loss: 0.6662641497776134\n",
            "Epoch: 70. Loss: 0.6610282305857836\n",
            "Epoch: 80. Loss: 0.6577052528293164\n",
            "Epoch: 90. Loss: 0.6553687363011182\n",
            "Epoch: 100. Loss: 0.6536005649035486\n",
            "Epoch: 110. Loss: 0.6522003414047344\n",
            "Epoch: 120. Loss: 0.6510622765844611\n",
            "Epoch: 130. Loss: 0.6501235622632272\n",
            "Epoch: 140. Loss: 0.649342481868365\n",
            "Epoch: 150. Loss: 0.6486888352552309\n",
            "Epoch: 160. Loss: 0.6481395048889913\n",
            "Epoch: 170. Loss: 0.6476762126604687\n",
            "Epoch: 180. Loss: 0.6472842467334531\n",
            "Epoch: 190. Loss: 0.646951647625519\n",
            "Epoch: 200. Loss: 0.6466686348264136\n",
            "Epoch: 210. Loss: 0.6464271760346147\n",
            "Epoch: 220. Loss: 0.6462206515348018\n",
            "Epoch: 230. Loss: 0.6460435878229059\n",
            "Epoch: 240. Loss: 0.6458914443296054\n",
            "Epoch: 250. Loss: 0.6457604419487929\n",
            "Epoch: 260. Loss: 0.6456474248616532\n",
            "Epoch: 270. Loss: 0.6455497489915675\n",
            "Epoch: 280. Loss: 0.6454651917846357\n",
            "Epoch: 290. Loss: 0.6453918790729504\n",
            "Epoch: 300. Loss: 0.6453282256278968\n",
            "Epoch: 310. Loss: 0.645272886695041\n",
            "Epoch: 320. Loss: 0.6452247183518816\n",
            "Epoch: 330. Loss: 0.6451827449694338\n",
            "Epoch: 340. Loss: 0.6451461324087843\n",
            "Epoch: 350. Loss: 0.6451141658616885\n",
            "Epoch: 360. Loss: 0.6450862314643938\n",
            "Epoch: 370. Loss: 0.6450618009880238\n",
            "Epoch: 380. Loss: 0.6450404190466671\n",
            "Epoch: 390. Loss: 0.6450216923734609\n",
            "Epoch: 400. Loss: 0.6450052808015794\n",
            "Epoch: 410. Loss: 0.6449908896559103\n",
            "Epoch: 420. Loss: 0.6449782633161449\n",
            "Epoch: 430. Loss: 0.6449671797559663\n",
            "Epoch: 440. Loss: 0.6449574458982983\n",
            "Epoch: 450. Loss: 0.6449488936550086\n",
            "Epoch: 460. Loss: 0.6449413765424442\n",
            "Epoch: 470. Loss: 0.6449347667828298\n",
            "Epoch: 480. Loss: 0.6449289528167546\n",
            "Epoch: 490. Loss: 0.6449238371643992\n",
            "tensor(0.7777, dtype=torch.float64)\n",
            "2022-12-18 00:00:00\n",
            "Epoch: 0. Loss: 0.9789003493810998\n",
            "Epoch: 10. Loss: 0.7699329693442387\n",
            "Epoch: 20. Loss: 0.7133057860872742\n",
            "Epoch: 30. Loss: 0.6856607067974824\n",
            "Epoch: 40. Loss: 0.671279522161029\n",
            "Epoch: 50. Loss: 0.6638018323367236\n",
            "Epoch: 60. Loss: 0.6599241233755331\n",
            "Epoch: 70. Loss: 0.6578649699676307\n",
            "Epoch: 80. Loss: 0.6567015774368071\n",
            "Epoch: 90. Loss: 0.6559778220271991\n",
            "Epoch: 100. Loss: 0.6554754838701278\n",
            "Epoch: 110. Loss: 0.6550917658374672\n",
            "Epoch: 120. Loss: 0.6547781159995611\n",
            "Epoch: 130. Loss: 0.6545110210160318\n",
            "Epoch: 140. Loss: 0.6542784460614545\n",
            "Epoch: 150. Loss: 0.6540736434309864\n",
            "Epoch: 160. Loss: 0.6538923513205417\n",
            "Epoch: 170. Loss: 0.6537315301321748\n",
            "Epoch: 180. Loss: 0.6535887900245139\n",
            "Epoch: 190. Loss: 0.653462127752368\n",
            "Epoch: 200. Loss: 0.6533498017119915\n",
            "Epoch: 210. Loss: 0.6532502689193328\n",
            "Epoch: 220. Loss: 0.6531621500015787\n",
            "Epoch: 230. Loss: 0.6530842071240418\n",
            "Epoch: 240. Loss: 0.653015328136575\n",
            "Epoch: 250. Loss: 0.6529545139313563\n",
            "Epoch: 260. Loss: 0.6529008676486227\n",
            "Epoch: 270. Loss: 0.6528535850981638\n",
            "Epoch: 280. Loss: 0.6528119460909341\n",
            "Epoch: 290. Loss: 0.6527753065220886\n",
            "Epoch: 300. Loss: 0.652743091113711\n",
            "Epoch: 310. Loss: 0.6527147867565776\n",
            "Epoch: 320. Loss: 0.6526899364051765\n",
            "Epoch: 330. Loss: 0.6526681334876329\n",
            "Epoch: 340. Loss: 0.6526490167961778\n",
            "Epoch: 350. Loss: 0.6526322658261554\n",
            "Epoch: 360. Loss: 0.6526175965331622\n",
            "Epoch: 370. Loss: 0.6526047574791727\n",
            "Epoch: 380. Loss: 0.6525935263396185\n",
            "Epoch: 390. Loss: 0.6525837067444786\n",
            "Epoch: 400. Loss: 0.6525751254275445\n",
            "Epoch: 410. Loss: 0.6525676296591748\n",
            "Epoch: 420. Loss: 0.6525610849390475\n",
            "Epoch: 430. Loss: 0.6525553729266628\n",
            "Epoch: 440. Loss: 0.6525503895886187\n",
            "Epoch: 450. Loss: 0.6525460435429767\n",
            "Epoch: 460. Loss: 0.6525422545823362\n",
            "Epoch: 470. Loss: 0.6525389523585202\n",
            "Epoch: 480. Loss: 0.6525360752130517\n",
            "Epoch: 490. Loss: 0.6525335691388271\n",
            "tensor(0.5156, dtype=torch.float64)\n",
            "2022-12-25 00:00:00\n",
            "Epoch: 0. Loss: 1.478731185949562\n",
            "Epoch: 10. Loss: 0.8867400664208805\n",
            "Epoch: 20. Loss: 0.7542766083298822\n",
            "Epoch: 30. Loss: 0.7069314504064215\n",
            "Epoch: 40. Loss: 0.6823054381179218\n",
            "Epoch: 50. Loss: 0.6699062035238095\n",
            "Epoch: 60. Loss: 0.6638633892512347\n",
            "Epoch: 70. Loss: 0.6609600611518143\n",
            "Epoch: 80. Loss: 0.659541975999591\n",
            "Epoch: 90. Loss: 0.6588076567453308\n",
            "Epoch: 100. Loss: 0.6583863193550287\n",
            "Epoch: 110. Loss: 0.6581114029019838\n",
            "Epoch: 120. Loss: 0.6579091169081241\n",
            "Epoch: 130. Loss: 0.6577467087004393\n",
            "Epoch: 140. Loss: 0.6576093115433161\n",
            "Epoch: 150. Loss: 0.6574898111658021\n",
            "Epoch: 160. Loss: 0.6573844686254261\n",
            "Epoch: 170. Loss: 0.6572910429779018\n",
            "Epoch: 180. Loss: 0.6572079876286366\n",
            "Epoch: 190. Loss: 0.6571341048074386\n",
            "Epoch: 200. Loss: 0.6570683948557018\n",
            "Epoch: 210. Loss: 0.6570099883202167\n",
            "Epoch: 220. Loss: 0.6569581133643924\n",
            "Epoch: 230. Loss: 0.656912078390856\n",
            "Epoch: 240. Loss: 0.6568712613595614\n",
            "Epoch: 250. Loss: 0.6568351021844577\n",
            "Epoch: 260. Loss: 0.6568030966622646\n",
            "Epoch: 270. Loss: 0.6567747912631825\n",
            "Epoch: 280. Loss: 0.6567497784854328\n",
            "Epoch: 290. Loss: 0.6567276926344564\n",
            "Epoch: 300. Loss: 0.6567082059561723\n",
            "Epoch: 310. Loss: 0.6566910250837585\n",
            "Epoch: 320. Loss: 0.6566758877709261\n",
            "Epoch: 330. Loss: 0.6566625598909448\n",
            "Epoch: 340. Loss: 0.6566508326837809\n",
            "Epoch: 350. Loss: 0.6566405202353511\n",
            "Epoch: 360. Loss: 0.6566314571738818\n",
            "Epoch: 370. Loss: 0.6566234965690303\n",
            "Epoch: 380. Loss: 0.6566165080199716\n",
            "Epoch: 390. Loss: 0.6566103759191396\n",
            "Epoch: 400. Loss: 0.656604997878793\n",
            "Epoch: 410. Loss: 0.6566002833080848\n",
            "Epoch: 420. Loss: 0.6565961521288286\n",
            "Epoch: 430. Loss: 0.6565925336187143\n",
            "Epoch: 440. Loss: 0.6565893653712951\n",
            "Epoch: 450. Loss: 0.6565865923626595\n",
            "Epoch: 460. Loss: 0.656584166115311\n",
            "Epoch: 470. Loss: 0.6565820439503823\n",
            "Epoch: 480. Loss: 0.6565801883199168\n",
            "Epoch: 490. Loss: 0.6565785662115547\n",
            "tensor(0.5884, dtype=torch.float64)\n",
            "2023-01-01 00:00:00\n",
            "Epoch: 0. Loss: 2.8398213824668304\n",
            "Epoch: 10. Loss: 1.294304663036831\n",
            "Epoch: 20. Loss: 0.7321368385719506\n",
            "Epoch: 30. Loss: 0.690710096542963\n",
            "Epoch: 40. Loss: 0.6811638220922929\n",
            "Epoch: 50. Loss: 0.6767125530847588\n",
            "Epoch: 60. Loss: 0.6739164539314334\n",
            "Epoch: 70. Loss: 0.6717826187361199\n",
            "Epoch: 80. Loss: 0.6699861526254917\n",
            "Epoch: 90. Loss: 0.6684029542611818\n",
            "Epoch: 100. Loss: 0.6669782392884352\n",
            "Epoch: 110. Loss: 0.6656842309355594\n",
            "Epoch: 120. Loss: 0.6645045079926644\n",
            "Epoch: 130. Loss: 0.6634276943008144\n",
            "Epoch: 140. Loss: 0.6624448123923888\n",
            "Epoch: 150. Loss: 0.6615481551859832\n",
            "Epoch: 160. Loss: 0.6607307991816609\n",
            "Epoch: 170. Loss: 0.6599863897830784\n",
            "Epoch: 180. Loss: 0.6593090416694565\n",
            "Epoch: 190. Loss: 0.6586932874120152\n",
            "Epoch: 200. Loss: 0.6581340460391715\n",
            "Epoch: 210. Loss: 0.6576265996677024\n",
            "Epoch: 220. Loss: 0.6571665732969746\n",
            "Epoch: 230. Loss: 0.656749915821817\n",
            "Epoch: 240. Loss: 0.6563728815650264\n",
            "Epoch: 250. Loss: 0.6560320121457649\n",
            "Epoch: 260. Loss: 0.6557241187035804\n",
            "Epoch: 270. Loss: 0.6554462645678942\n",
            "Epoch: 280. Loss: 0.6551957484760554\n",
            "Epoch: 290. Loss: 0.6549700884326668\n",
            "Epoch: 300. Loss: 0.6547670062834913\n",
            "Epoch: 310. Loss: 0.6545844130555422\n",
            "Epoch: 320. Loss: 0.6544203950940053\n",
            "Epoch: 330. Loss: 0.6542732010079335\n",
            "Epoch: 340. Loss: 0.6541412294208191\n",
            "Epoch: 350. Loss: 0.6540230175093943\n",
            "Epoch: 360. Loss: 0.6539172303042847\n",
            "Epoch: 370. Loss: 0.6538226507192108\n",
            "Epoch: 380. Loss: 0.6537381702709872\n",
            "Epoch: 390. Loss: 0.6536627804502291\n",
            "Epoch: 400. Loss: 0.6535955647020208\n",
            "Epoch: 410. Loss: 0.6535356909764813\n",
            "Epoch: 420. Loss: 0.6534824048107388\n",
            "Epoch: 430. Loss: 0.6534350229060567\n",
            "Epoch: 440. Loss: 0.6533929271663828\n",
            "Epoch: 450. Loss: 0.653355559167263\n",
            "Epoch: 460. Loss: 0.6533224150266596\n",
            "Epoch: 470. Loss: 0.6532930406516642\n",
            "Epoch: 480. Loss: 0.6532670273372863\n",
            "Epoch: 490. Loss: 0.6532440076954427\n",
            "tensor(0.6262, dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                            a  \\\n",
              "2003-09-21  [tensor(0.0821, dtype=torch.float64, grad_fn=<...   \n",
              "2003-09-28  [tensor(0.0449, dtype=torch.float64, grad_fn=<...   \n",
              "2003-10-05  [tensor(-0.3851, dtype=torch.float64, grad_fn=...   \n",
              "2003-10-12  [tensor(-2.2452, dtype=torch.float64, grad_fn=...   \n",
              "2003-10-19  [tensor(-0.1125, dtype=torch.float64, grad_fn=...   \n",
              "...                                                       ...   \n",
              "2022-12-04  [tensor(-0.2526, dtype=torch.float64, grad_fn=...   \n",
              "2022-12-11  [tensor(0.1450, dtype=torch.float64, grad_fn=<...   \n",
              "2022-12-18  [tensor(-0.3266, dtype=torch.float64, grad_fn=...   \n",
              "2022-12-25  [tensor(-0.2599, dtype=torch.float64, grad_fn=...   \n",
              "2023-01-01  [tensor(0.4273, dtype=torch.float64, grad_fn=<...   \n",
              "\n",
              "                                                            b  \\\n",
              "2003-09-21  [tensor(0.2624, dtype=torch.float64, grad_fn=<...   \n",
              "2003-09-28  [tensor(0.2281, dtype=torch.float64, grad_fn=<...   \n",
              "2003-10-05  [tensor(-0.2060, dtype=torch.float64, grad_fn=...   \n",
              "2003-10-12  [tensor(-2.0383, dtype=torch.float64, grad_fn=...   \n",
              "2003-10-19  [tensor(0.0835, dtype=torch.float64, grad_fn=<...   \n",
              "...                                                       ...   \n",
              "2022-12-04  [tensor(-0.0017, dtype=torch.float64, grad_fn=...   \n",
              "2022-12-11  [tensor(0.3640, dtype=torch.float64, grad_fn=<...   \n",
              "2022-12-18  [tensor(-0.1347, dtype=torch.float64, grad_fn=...   \n",
              "2022-12-25  [tensor(-0.0761, dtype=torch.float64, grad_fn=...   \n",
              "2023-01-01  [tensor(0.6375, dtype=torch.float64, grad_fn=<...   \n",
              "\n",
              "                          prob  \n",
              "2003-09-21    0.67439726413072  \n",
              "2003-09-28  0.7049454524952933  \n",
              "2003-10-05  0.7705497222091419  \n",
              "2003-10-12  0.6773620347956015  \n",
              "2003-10-19  0.7100869972302286  \n",
              "...                        ...  \n",
              "2022-12-04  0.7126808019883828  \n",
              "2022-12-11  0.7776645092054683  \n",
              "2022-12-18  0.5156496882303369  \n",
              "2022-12-25  0.5884166089770279  \n",
              "2023-01-01  0.6261542988645438  \n",
              "\n",
              "[1007 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0d5adf97-9801-46ef-8b79-564016de4e47\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>a</th>\n",
              "      <th>b</th>\n",
              "      <th>prob</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2003-09-21</th>\n",
              "      <td>[tensor(0.0821, dtype=torch.float64, grad_fn=&lt;...</td>\n",
              "      <td>[tensor(0.2624, dtype=torch.float64, grad_fn=&lt;...</td>\n",
              "      <td>0.67439726413072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2003-09-28</th>\n",
              "      <td>[tensor(0.0449, dtype=torch.float64, grad_fn=&lt;...</td>\n",
              "      <td>[tensor(0.2281, dtype=torch.float64, grad_fn=&lt;...</td>\n",
              "      <td>0.7049454524952933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2003-10-05</th>\n",
              "      <td>[tensor(-0.3851, dtype=torch.float64, grad_fn=...</td>\n",
              "      <td>[tensor(-0.2060, dtype=torch.float64, grad_fn=...</td>\n",
              "      <td>0.7705497222091419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2003-10-12</th>\n",
              "      <td>[tensor(-2.2452, dtype=torch.float64, grad_fn=...</td>\n",
              "      <td>[tensor(-2.0383, dtype=torch.float64, grad_fn=...</td>\n",
              "      <td>0.6773620347956015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2003-10-19</th>\n",
              "      <td>[tensor(-0.1125, dtype=torch.float64, grad_fn=...</td>\n",
              "      <td>[tensor(0.0835, dtype=torch.float64, grad_fn=&lt;...</td>\n",
              "      <td>0.7100869972302286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-04</th>\n",
              "      <td>[tensor(-0.2526, dtype=torch.float64, grad_fn=...</td>\n",
              "      <td>[tensor(-0.0017, dtype=torch.float64, grad_fn=...</td>\n",
              "      <td>0.7126808019883828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-11</th>\n",
              "      <td>[tensor(0.1450, dtype=torch.float64, grad_fn=&lt;...</td>\n",
              "      <td>[tensor(0.3640, dtype=torch.float64, grad_fn=&lt;...</td>\n",
              "      <td>0.7776645092054683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-18</th>\n",
              "      <td>[tensor(-0.3266, dtype=torch.float64, grad_fn=...</td>\n",
              "      <td>[tensor(-0.1347, dtype=torch.float64, grad_fn=...</td>\n",
              "      <td>0.5156496882303369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-12-25</th>\n",
              "      <td>[tensor(-0.2599, dtype=torch.float64, grad_fn=...</td>\n",
              "      <td>[tensor(-0.0761, dtype=torch.float64, grad_fn=...</td>\n",
              "      <td>0.5884166089770279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-01-01</th>\n",
              "      <td>[tensor(0.4273, dtype=torch.float64, grad_fn=&lt;...</td>\n",
              "      <td>[tensor(0.6375, dtype=torch.float64, grad_fn=&lt;...</td>\n",
              "      <td>0.6261542988645438</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1007 rows √ó 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0d5adf97-9801-46ef-8b79-564016de4e47')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0d5adf97-9801-46ef-8b79-564016de4e47 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0d5adf97-9801-46ef-8b79-564016de4e47');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 426
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ks = np.arange(0, 1, 0.05)\n",
        "backtest_returns = pd.DataFrame(columns = ks)\n",
        "\n",
        "for date in daterange:  \n",
        "  print(date)\n",
        "  prob = parameters.loc[date]['prob']\n",
        "  rets = []\n",
        "  for k in ks:\n",
        "    weight = calculate_ml_portfolio_weights(prob, k)\n",
        "    rets.append(get_backtest_return(date, weight))\n",
        "  backtest_returns.loc[date] = rets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWaPoTs-NqAF",
        "outputId": "9cabc60c-c43e-4425-df04-7fbe156aebe9"
      },
      "execution_count": 427,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2003-09-21 00:00:00\n",
            "2003-09-28 00:00:00\n",
            "2003-10-05 00:00:00\n",
            "2003-10-12 00:00:00\n",
            "2003-10-19 00:00:00\n",
            "2003-10-26 00:00:00\n",
            "2003-11-02 00:00:00\n",
            "2003-11-09 00:00:00\n",
            "2003-11-16 00:00:00\n",
            "2003-11-23 00:00:00\n",
            "2003-11-30 00:00:00\n",
            "2003-12-07 00:00:00\n",
            "2003-12-14 00:00:00\n",
            "2003-12-21 00:00:00\n",
            "2003-12-28 00:00:00\n",
            "2004-01-04 00:00:00\n",
            "2004-01-11 00:00:00\n",
            "2004-01-18 00:00:00\n",
            "2004-01-25 00:00:00\n",
            "2004-02-01 00:00:00\n",
            "2004-02-08 00:00:00\n",
            "2004-02-15 00:00:00\n",
            "2004-02-22 00:00:00\n",
            "2004-02-29 00:00:00\n",
            "2004-03-07 00:00:00\n",
            "2004-03-14 00:00:00\n",
            "2004-03-21 00:00:00\n",
            "2004-03-28 00:00:00\n",
            "2004-04-04 00:00:00\n",
            "2004-04-11 00:00:00\n",
            "2004-04-18 00:00:00\n",
            "2004-04-25 00:00:00\n",
            "2004-05-02 00:00:00\n",
            "2004-05-09 00:00:00\n",
            "2004-05-16 00:00:00\n",
            "2004-05-23 00:00:00\n",
            "2004-05-30 00:00:00\n",
            "2004-06-06 00:00:00\n",
            "2004-06-13 00:00:00\n",
            "2004-06-20 00:00:00\n",
            "2004-06-27 00:00:00\n",
            "2004-07-04 00:00:00\n",
            "2004-07-11 00:00:00\n",
            "2004-07-18 00:00:00\n",
            "2004-07-25 00:00:00\n",
            "2004-08-01 00:00:00\n",
            "2004-08-08 00:00:00\n",
            "2004-08-15 00:00:00\n",
            "2004-08-22 00:00:00\n",
            "2004-08-29 00:00:00\n",
            "2004-09-05 00:00:00\n",
            "2004-09-12 00:00:00\n",
            "2004-09-19 00:00:00\n",
            "2004-09-26 00:00:00\n",
            "2004-10-03 00:00:00\n",
            "2004-10-10 00:00:00\n",
            "2004-10-17 00:00:00\n",
            "2004-10-24 00:00:00\n",
            "2004-10-31 00:00:00\n",
            "2004-11-07 00:00:00\n",
            "2004-11-14 00:00:00\n",
            "2004-11-21 00:00:00\n",
            "2004-11-28 00:00:00\n",
            "2004-12-05 00:00:00\n",
            "2004-12-12 00:00:00\n",
            "2004-12-19 00:00:00\n",
            "2004-12-26 00:00:00\n",
            "2005-01-02 00:00:00\n",
            "2005-01-09 00:00:00\n",
            "2005-01-16 00:00:00\n",
            "2005-01-23 00:00:00\n",
            "2005-01-30 00:00:00\n",
            "2005-02-06 00:00:00\n",
            "2005-02-13 00:00:00\n",
            "2005-02-20 00:00:00\n",
            "2005-02-27 00:00:00\n",
            "2005-03-06 00:00:00\n",
            "2005-03-13 00:00:00\n",
            "2005-03-20 00:00:00\n",
            "2005-03-27 00:00:00\n",
            "2005-04-03 00:00:00\n",
            "2005-04-10 00:00:00\n",
            "2005-04-17 00:00:00\n",
            "2005-04-24 00:00:00\n",
            "2005-05-01 00:00:00\n",
            "2005-05-08 00:00:00\n",
            "2005-05-15 00:00:00\n",
            "2005-05-22 00:00:00\n",
            "2005-05-29 00:00:00\n",
            "2005-06-05 00:00:00\n",
            "2005-06-12 00:00:00\n",
            "2005-06-19 00:00:00\n",
            "2005-06-26 00:00:00\n",
            "2005-07-03 00:00:00\n",
            "2005-07-10 00:00:00\n",
            "2005-07-17 00:00:00\n",
            "2005-07-24 00:00:00\n",
            "2005-07-31 00:00:00\n",
            "2005-08-07 00:00:00\n",
            "2005-08-14 00:00:00\n",
            "2005-08-21 00:00:00\n",
            "2005-08-28 00:00:00\n",
            "2005-09-04 00:00:00\n",
            "2005-09-11 00:00:00\n",
            "2005-09-18 00:00:00\n",
            "2005-09-25 00:00:00\n",
            "2005-10-02 00:00:00\n",
            "2005-10-09 00:00:00\n",
            "2005-10-16 00:00:00\n",
            "2005-10-23 00:00:00\n",
            "2005-10-30 00:00:00\n",
            "2005-11-06 00:00:00\n",
            "2005-11-13 00:00:00\n",
            "2005-11-20 00:00:00\n",
            "2005-11-27 00:00:00\n",
            "2005-12-04 00:00:00\n",
            "2005-12-11 00:00:00\n",
            "2005-12-18 00:00:00\n",
            "2005-12-25 00:00:00\n",
            "2006-01-01 00:00:00\n",
            "2006-01-08 00:00:00\n",
            "2006-01-15 00:00:00\n",
            "2006-01-22 00:00:00\n",
            "2006-01-29 00:00:00\n",
            "2006-02-05 00:00:00\n",
            "2006-02-12 00:00:00\n",
            "2006-02-19 00:00:00\n",
            "2006-02-26 00:00:00\n",
            "2006-03-05 00:00:00\n",
            "2006-03-12 00:00:00\n",
            "2006-03-19 00:00:00\n",
            "2006-03-26 00:00:00\n",
            "2006-04-02 00:00:00\n",
            "2006-04-09 00:00:00\n",
            "2006-04-16 00:00:00\n",
            "2006-04-23 00:00:00\n",
            "2006-04-30 00:00:00\n",
            "2006-05-07 00:00:00\n",
            "2006-05-14 00:00:00\n",
            "2006-05-21 00:00:00\n",
            "2006-05-28 00:00:00\n",
            "2006-06-04 00:00:00\n",
            "2006-06-11 00:00:00\n",
            "2006-06-18 00:00:00\n",
            "2006-06-25 00:00:00\n",
            "2006-07-02 00:00:00\n",
            "2006-07-09 00:00:00\n",
            "2006-07-16 00:00:00\n",
            "2006-07-23 00:00:00\n",
            "2006-07-30 00:00:00\n",
            "2006-08-06 00:00:00\n",
            "2006-08-13 00:00:00\n",
            "2006-08-20 00:00:00\n",
            "2006-08-27 00:00:00\n",
            "2006-09-03 00:00:00\n",
            "2006-09-10 00:00:00\n",
            "2006-09-17 00:00:00\n",
            "2006-09-24 00:00:00\n",
            "2006-10-01 00:00:00\n",
            "2006-10-08 00:00:00\n",
            "2006-10-15 00:00:00\n",
            "2006-10-22 00:00:00\n",
            "2006-10-29 00:00:00\n",
            "2006-11-05 00:00:00\n",
            "2006-11-12 00:00:00\n",
            "2006-11-19 00:00:00\n",
            "2006-11-26 00:00:00\n",
            "2006-12-03 00:00:00\n",
            "2006-12-10 00:00:00\n",
            "2006-12-17 00:00:00\n",
            "2006-12-24 00:00:00\n",
            "2006-12-31 00:00:00\n",
            "2007-01-07 00:00:00\n",
            "2007-01-14 00:00:00\n",
            "2007-01-21 00:00:00\n",
            "2007-01-28 00:00:00\n",
            "2007-02-04 00:00:00\n",
            "2007-02-11 00:00:00\n",
            "2007-02-18 00:00:00\n",
            "2007-02-25 00:00:00\n",
            "2007-03-04 00:00:00\n",
            "2007-03-11 00:00:00\n",
            "2007-03-18 00:00:00\n",
            "2007-03-25 00:00:00\n",
            "2007-04-01 00:00:00\n",
            "2007-04-08 00:00:00\n",
            "2007-04-15 00:00:00\n",
            "2007-04-22 00:00:00\n",
            "2007-04-29 00:00:00\n",
            "2007-05-06 00:00:00\n",
            "2007-05-13 00:00:00\n",
            "2007-05-20 00:00:00\n",
            "2007-05-27 00:00:00\n",
            "2007-06-03 00:00:00\n",
            "2007-06-10 00:00:00\n",
            "2007-06-17 00:00:00\n",
            "2007-06-24 00:00:00\n",
            "2007-07-01 00:00:00\n",
            "2007-07-08 00:00:00\n",
            "2007-07-15 00:00:00\n",
            "2007-07-22 00:00:00\n",
            "2007-07-29 00:00:00\n",
            "2007-08-05 00:00:00\n",
            "2007-08-12 00:00:00\n",
            "2007-08-19 00:00:00\n",
            "2007-08-26 00:00:00\n",
            "2007-09-02 00:00:00\n",
            "2007-09-09 00:00:00\n",
            "2007-09-16 00:00:00\n",
            "2007-09-23 00:00:00\n",
            "2007-09-30 00:00:00\n",
            "2007-10-07 00:00:00\n",
            "2007-10-14 00:00:00\n",
            "2007-10-21 00:00:00\n",
            "2007-10-28 00:00:00\n",
            "2007-11-04 00:00:00\n",
            "2007-11-11 00:00:00\n",
            "2007-11-18 00:00:00\n",
            "2007-11-25 00:00:00\n",
            "2007-12-02 00:00:00\n",
            "2007-12-09 00:00:00\n",
            "2007-12-16 00:00:00\n",
            "2007-12-23 00:00:00\n",
            "2007-12-30 00:00:00\n",
            "2008-01-06 00:00:00\n",
            "2008-01-13 00:00:00\n",
            "2008-01-20 00:00:00\n",
            "2008-01-27 00:00:00\n",
            "2008-02-03 00:00:00\n",
            "2008-02-10 00:00:00\n",
            "2008-02-17 00:00:00\n",
            "2008-02-24 00:00:00\n",
            "2008-03-02 00:00:00\n",
            "2008-03-09 00:00:00\n",
            "2008-03-16 00:00:00\n",
            "2008-03-23 00:00:00\n",
            "2008-03-30 00:00:00\n",
            "2008-04-06 00:00:00\n",
            "2008-04-13 00:00:00\n",
            "2008-04-20 00:00:00\n",
            "2008-04-27 00:00:00\n",
            "2008-05-04 00:00:00\n",
            "2008-05-11 00:00:00\n",
            "2008-05-18 00:00:00\n",
            "2008-05-25 00:00:00\n",
            "2008-06-01 00:00:00\n",
            "2008-06-08 00:00:00\n",
            "2008-06-15 00:00:00\n",
            "2008-06-22 00:00:00\n",
            "2008-06-29 00:00:00\n",
            "2008-07-06 00:00:00\n",
            "2008-07-13 00:00:00\n",
            "2008-07-20 00:00:00\n",
            "2008-07-27 00:00:00\n",
            "2008-08-03 00:00:00\n",
            "2008-08-10 00:00:00\n",
            "2008-08-17 00:00:00\n",
            "2008-08-24 00:00:00\n",
            "2008-08-31 00:00:00\n",
            "2008-09-07 00:00:00\n",
            "2008-09-14 00:00:00\n",
            "2008-09-21 00:00:00\n",
            "2008-09-28 00:00:00\n",
            "2008-10-05 00:00:00\n",
            "2008-10-12 00:00:00\n",
            "2008-10-19 00:00:00\n",
            "2008-10-26 00:00:00\n",
            "2008-11-02 00:00:00\n",
            "2008-11-09 00:00:00\n",
            "2008-11-16 00:00:00\n",
            "2008-11-23 00:00:00\n",
            "2008-11-30 00:00:00\n",
            "2008-12-07 00:00:00\n",
            "2008-12-14 00:00:00\n",
            "2008-12-21 00:00:00\n",
            "2008-12-28 00:00:00\n",
            "2009-01-04 00:00:00\n",
            "2009-01-11 00:00:00\n",
            "2009-01-18 00:00:00\n",
            "2009-01-25 00:00:00\n",
            "2009-02-01 00:00:00\n",
            "2009-02-08 00:00:00\n",
            "2009-02-15 00:00:00\n",
            "2009-02-22 00:00:00\n",
            "2009-03-01 00:00:00\n",
            "2009-03-08 00:00:00\n",
            "2009-03-15 00:00:00\n",
            "2009-03-22 00:00:00\n",
            "2009-03-29 00:00:00\n",
            "2009-04-05 00:00:00\n",
            "2009-04-12 00:00:00\n",
            "2009-04-19 00:00:00\n",
            "2009-04-26 00:00:00\n",
            "2009-05-03 00:00:00\n",
            "2009-05-10 00:00:00\n",
            "2009-05-17 00:00:00\n",
            "2009-05-24 00:00:00\n",
            "2009-05-31 00:00:00\n",
            "2009-06-07 00:00:00\n",
            "2009-06-14 00:00:00\n",
            "2009-06-21 00:00:00\n",
            "2009-06-28 00:00:00\n",
            "2009-07-05 00:00:00\n",
            "2009-07-12 00:00:00\n",
            "2009-07-19 00:00:00\n",
            "2009-07-26 00:00:00\n",
            "2009-08-02 00:00:00\n",
            "2009-08-09 00:00:00\n",
            "2009-08-16 00:00:00\n",
            "2009-08-23 00:00:00\n",
            "2009-08-30 00:00:00\n",
            "2009-09-06 00:00:00\n",
            "2009-09-13 00:00:00\n",
            "2009-09-20 00:00:00\n",
            "2009-09-27 00:00:00\n",
            "2009-10-04 00:00:00\n",
            "2009-10-11 00:00:00\n",
            "2009-10-18 00:00:00\n",
            "2009-10-25 00:00:00\n",
            "2009-11-01 00:00:00\n",
            "2009-11-08 00:00:00\n",
            "2009-11-15 00:00:00\n",
            "2009-11-22 00:00:00\n",
            "2009-11-29 00:00:00\n",
            "2009-12-06 00:00:00\n",
            "2009-12-13 00:00:00\n",
            "2009-12-20 00:00:00\n",
            "2009-12-27 00:00:00\n",
            "2010-01-03 00:00:00\n",
            "2010-01-10 00:00:00\n",
            "2010-01-17 00:00:00\n",
            "2010-01-24 00:00:00\n",
            "2010-01-31 00:00:00\n",
            "2010-02-07 00:00:00\n",
            "2010-02-14 00:00:00\n",
            "2010-02-21 00:00:00\n",
            "2010-02-28 00:00:00\n",
            "2010-03-07 00:00:00\n",
            "2010-03-14 00:00:00\n",
            "2010-03-21 00:00:00\n",
            "2010-03-28 00:00:00\n",
            "2010-04-04 00:00:00\n",
            "2010-04-11 00:00:00\n",
            "2010-04-18 00:00:00\n",
            "2010-04-25 00:00:00\n",
            "2010-05-02 00:00:00\n",
            "2010-05-09 00:00:00\n",
            "2010-05-16 00:00:00\n",
            "2010-05-23 00:00:00\n",
            "2010-05-30 00:00:00\n",
            "2010-06-06 00:00:00\n",
            "2010-06-13 00:00:00\n",
            "2010-06-20 00:00:00\n",
            "2010-06-27 00:00:00\n",
            "2010-07-04 00:00:00\n",
            "2010-07-11 00:00:00\n",
            "2010-07-18 00:00:00\n",
            "2010-07-25 00:00:00\n",
            "2010-08-01 00:00:00\n",
            "2010-08-08 00:00:00\n",
            "2010-08-15 00:00:00\n",
            "2010-08-22 00:00:00\n",
            "2010-08-29 00:00:00\n",
            "2010-09-05 00:00:00\n",
            "2010-09-12 00:00:00\n",
            "2010-09-19 00:00:00\n",
            "2010-09-26 00:00:00\n",
            "2010-10-03 00:00:00\n",
            "2010-10-10 00:00:00\n",
            "2010-10-17 00:00:00\n",
            "2010-10-24 00:00:00\n",
            "2010-10-31 00:00:00\n",
            "2010-11-07 00:00:00\n",
            "2010-11-14 00:00:00\n",
            "2010-11-21 00:00:00\n",
            "2010-11-28 00:00:00\n",
            "2010-12-05 00:00:00\n",
            "2010-12-12 00:00:00\n",
            "2010-12-19 00:00:00\n",
            "2010-12-26 00:00:00\n",
            "2011-01-02 00:00:00\n",
            "2011-01-09 00:00:00\n",
            "2011-01-16 00:00:00\n",
            "2011-01-23 00:00:00\n",
            "2011-01-30 00:00:00\n",
            "2011-02-06 00:00:00\n",
            "2011-02-13 00:00:00\n",
            "2011-02-20 00:00:00\n",
            "2011-02-27 00:00:00\n",
            "2011-03-06 00:00:00\n",
            "2011-03-13 00:00:00\n",
            "2011-03-20 00:00:00\n",
            "2011-03-27 00:00:00\n",
            "2011-04-03 00:00:00\n",
            "2011-04-10 00:00:00\n",
            "2011-04-17 00:00:00\n",
            "2011-04-24 00:00:00\n",
            "2011-05-01 00:00:00\n",
            "2011-05-08 00:00:00\n",
            "2011-05-15 00:00:00\n",
            "2011-05-22 00:00:00\n",
            "2011-05-29 00:00:00\n",
            "2011-06-05 00:00:00\n",
            "2011-06-12 00:00:00\n",
            "2011-06-19 00:00:00\n",
            "2011-06-26 00:00:00\n",
            "2011-07-03 00:00:00\n",
            "2011-07-10 00:00:00\n",
            "2011-07-17 00:00:00\n",
            "2011-07-24 00:00:00\n",
            "2011-07-31 00:00:00\n",
            "2011-08-07 00:00:00\n",
            "2011-08-14 00:00:00\n",
            "2011-08-21 00:00:00\n",
            "2011-08-28 00:00:00\n",
            "2011-09-04 00:00:00\n",
            "2011-09-11 00:00:00\n",
            "2011-09-18 00:00:00\n",
            "2011-09-25 00:00:00\n",
            "2011-10-02 00:00:00\n",
            "2011-10-09 00:00:00\n",
            "2011-10-16 00:00:00\n",
            "2011-10-23 00:00:00\n",
            "2011-10-30 00:00:00\n",
            "2011-11-06 00:00:00\n",
            "2011-11-13 00:00:00\n",
            "2011-11-20 00:00:00\n",
            "2011-11-27 00:00:00\n",
            "2011-12-04 00:00:00\n",
            "2011-12-11 00:00:00\n",
            "2011-12-18 00:00:00\n",
            "2011-12-25 00:00:00\n",
            "2012-01-01 00:00:00\n",
            "2012-01-08 00:00:00\n",
            "2012-01-15 00:00:00\n",
            "2012-01-22 00:00:00\n",
            "2012-01-29 00:00:00\n",
            "2012-02-05 00:00:00\n",
            "2012-02-12 00:00:00\n",
            "2012-02-19 00:00:00\n",
            "2012-02-26 00:00:00\n",
            "2012-03-04 00:00:00\n",
            "2012-03-11 00:00:00\n",
            "2012-03-18 00:00:00\n",
            "2012-03-25 00:00:00\n",
            "2012-04-01 00:00:00\n",
            "2012-04-08 00:00:00\n",
            "2012-04-15 00:00:00\n",
            "2012-04-22 00:00:00\n",
            "2012-04-29 00:00:00\n",
            "2012-05-06 00:00:00\n",
            "2012-05-13 00:00:00\n",
            "2012-05-20 00:00:00\n",
            "2012-05-27 00:00:00\n",
            "2012-06-03 00:00:00\n",
            "2012-06-10 00:00:00\n",
            "2012-06-17 00:00:00\n",
            "2012-06-24 00:00:00\n",
            "2012-07-01 00:00:00\n",
            "2012-07-08 00:00:00\n",
            "2012-07-15 00:00:00\n",
            "2012-07-22 00:00:00\n",
            "2012-07-29 00:00:00\n",
            "2012-08-05 00:00:00\n",
            "2012-08-12 00:00:00\n",
            "2012-08-19 00:00:00\n",
            "2012-08-26 00:00:00\n",
            "2012-09-02 00:00:00\n",
            "2012-09-09 00:00:00\n",
            "2012-09-16 00:00:00\n",
            "2012-09-23 00:00:00\n",
            "2012-09-30 00:00:00\n",
            "2012-10-07 00:00:00\n",
            "2012-10-14 00:00:00\n",
            "2012-10-21 00:00:00\n",
            "2012-10-28 00:00:00\n",
            "2012-11-04 00:00:00\n",
            "2012-11-11 00:00:00\n",
            "2012-11-18 00:00:00\n",
            "2012-11-25 00:00:00\n",
            "2012-12-02 00:00:00\n",
            "2012-12-09 00:00:00\n",
            "2012-12-16 00:00:00\n",
            "2012-12-23 00:00:00\n",
            "2012-12-30 00:00:00\n",
            "2013-01-06 00:00:00\n",
            "2013-01-13 00:00:00\n",
            "2013-01-20 00:00:00\n",
            "2013-01-27 00:00:00\n",
            "2013-02-03 00:00:00\n",
            "2013-02-10 00:00:00\n",
            "2013-02-17 00:00:00\n",
            "2013-02-24 00:00:00\n",
            "2013-03-03 00:00:00\n",
            "2013-03-10 00:00:00\n",
            "2013-03-17 00:00:00\n",
            "2013-03-24 00:00:00\n",
            "2013-03-31 00:00:00\n",
            "2013-04-07 00:00:00\n",
            "2013-04-14 00:00:00\n",
            "2013-04-21 00:00:00\n",
            "2013-04-28 00:00:00\n",
            "2013-05-05 00:00:00\n",
            "2013-05-12 00:00:00\n",
            "2013-05-19 00:00:00\n",
            "2013-05-26 00:00:00\n",
            "2013-06-02 00:00:00\n",
            "2013-06-09 00:00:00\n",
            "2013-06-16 00:00:00\n",
            "2013-06-23 00:00:00\n",
            "2013-06-30 00:00:00\n",
            "2013-07-07 00:00:00\n",
            "2013-07-14 00:00:00\n",
            "2013-07-21 00:00:00\n",
            "2013-07-28 00:00:00\n",
            "2013-08-04 00:00:00\n",
            "2013-08-11 00:00:00\n",
            "2013-08-18 00:00:00\n",
            "2013-08-25 00:00:00\n",
            "2013-09-01 00:00:00\n",
            "2013-09-08 00:00:00\n",
            "2013-09-15 00:00:00\n",
            "2013-09-22 00:00:00\n",
            "2013-09-29 00:00:00\n",
            "2013-10-06 00:00:00\n",
            "2013-10-13 00:00:00\n",
            "2013-10-20 00:00:00\n",
            "2013-10-27 00:00:00\n",
            "2013-11-03 00:00:00\n",
            "2013-11-10 00:00:00\n",
            "2013-11-17 00:00:00\n",
            "2013-11-24 00:00:00\n",
            "2013-12-01 00:00:00\n",
            "2013-12-08 00:00:00\n",
            "2013-12-15 00:00:00\n",
            "2013-12-22 00:00:00\n",
            "2013-12-29 00:00:00\n",
            "2014-01-05 00:00:00\n",
            "2014-01-12 00:00:00\n",
            "2014-01-19 00:00:00\n",
            "2014-01-26 00:00:00\n",
            "2014-02-02 00:00:00\n",
            "2014-02-09 00:00:00\n",
            "2014-02-16 00:00:00\n",
            "2014-02-23 00:00:00\n",
            "2014-03-02 00:00:00\n",
            "2014-03-09 00:00:00\n",
            "2014-03-16 00:00:00\n",
            "2014-03-23 00:00:00\n",
            "2014-03-30 00:00:00\n",
            "2014-04-06 00:00:00\n",
            "2014-04-13 00:00:00\n",
            "2014-04-20 00:00:00\n",
            "2014-04-27 00:00:00\n",
            "2014-05-04 00:00:00\n",
            "2014-05-11 00:00:00\n",
            "2014-05-18 00:00:00\n",
            "2014-05-25 00:00:00\n",
            "2014-06-01 00:00:00\n",
            "2014-06-08 00:00:00\n",
            "2014-06-15 00:00:00\n",
            "2014-06-22 00:00:00\n",
            "2014-06-29 00:00:00\n",
            "2014-07-06 00:00:00\n",
            "2014-07-13 00:00:00\n",
            "2014-07-20 00:00:00\n",
            "2014-07-27 00:00:00\n",
            "2014-08-03 00:00:00\n",
            "2014-08-10 00:00:00\n",
            "2014-08-17 00:00:00\n",
            "2014-08-24 00:00:00\n",
            "2014-08-31 00:00:00\n",
            "2014-09-07 00:00:00\n",
            "2014-09-14 00:00:00\n",
            "2014-09-21 00:00:00\n",
            "2014-09-28 00:00:00\n",
            "2014-10-05 00:00:00\n",
            "2014-10-12 00:00:00\n",
            "2014-10-19 00:00:00\n",
            "2014-10-26 00:00:00\n",
            "2014-11-02 00:00:00\n",
            "2014-11-09 00:00:00\n",
            "2014-11-16 00:00:00\n",
            "2014-11-23 00:00:00\n",
            "2014-11-30 00:00:00\n",
            "2014-12-07 00:00:00\n",
            "2014-12-14 00:00:00\n",
            "2014-12-21 00:00:00\n",
            "2014-12-28 00:00:00\n",
            "2015-01-04 00:00:00\n",
            "2015-01-11 00:00:00\n",
            "2015-01-18 00:00:00\n",
            "2015-01-25 00:00:00\n",
            "2015-02-01 00:00:00\n",
            "2015-02-08 00:00:00\n",
            "2015-02-15 00:00:00\n",
            "2015-02-22 00:00:00\n",
            "2015-03-01 00:00:00\n",
            "2015-03-08 00:00:00\n",
            "2015-03-15 00:00:00\n",
            "2015-03-22 00:00:00\n",
            "2015-03-29 00:00:00\n",
            "2015-04-05 00:00:00\n",
            "2015-04-12 00:00:00\n",
            "2015-04-19 00:00:00\n",
            "2015-04-26 00:00:00\n",
            "2015-05-03 00:00:00\n",
            "2015-05-10 00:00:00\n",
            "2015-05-17 00:00:00\n",
            "2015-05-24 00:00:00\n",
            "2015-05-31 00:00:00\n",
            "2015-06-07 00:00:00\n",
            "2015-06-14 00:00:00\n",
            "2015-06-21 00:00:00\n",
            "2015-06-28 00:00:00\n",
            "2015-07-05 00:00:00\n",
            "2015-07-12 00:00:00\n",
            "2015-07-19 00:00:00\n",
            "2015-07-26 00:00:00\n",
            "2015-08-02 00:00:00\n",
            "2015-08-09 00:00:00\n",
            "2015-08-16 00:00:00\n",
            "2015-08-23 00:00:00\n",
            "2015-08-30 00:00:00\n",
            "2015-09-06 00:00:00\n",
            "2015-09-13 00:00:00\n",
            "2015-09-20 00:00:00\n",
            "2015-09-27 00:00:00\n",
            "2015-10-04 00:00:00\n",
            "2015-10-11 00:00:00\n",
            "2015-10-18 00:00:00\n",
            "2015-10-25 00:00:00\n",
            "2015-11-01 00:00:00\n",
            "2015-11-08 00:00:00\n",
            "2015-11-15 00:00:00\n",
            "2015-11-22 00:00:00\n",
            "2015-11-29 00:00:00\n",
            "2015-12-06 00:00:00\n",
            "2015-12-13 00:00:00\n",
            "2015-12-20 00:00:00\n",
            "2015-12-27 00:00:00\n",
            "2016-01-03 00:00:00\n",
            "2016-01-10 00:00:00\n",
            "2016-01-17 00:00:00\n",
            "2016-01-24 00:00:00\n",
            "2016-01-31 00:00:00\n",
            "2016-02-07 00:00:00\n",
            "2016-02-14 00:00:00\n",
            "2016-02-21 00:00:00\n",
            "2016-02-28 00:00:00\n",
            "2016-03-06 00:00:00\n",
            "2016-03-13 00:00:00\n",
            "2016-03-20 00:00:00\n",
            "2016-03-27 00:00:00\n",
            "2016-04-03 00:00:00\n",
            "2016-04-10 00:00:00\n",
            "2016-04-17 00:00:00\n",
            "2016-04-24 00:00:00\n",
            "2016-05-01 00:00:00\n",
            "2016-05-08 00:00:00\n",
            "2016-05-15 00:00:00\n",
            "2016-05-22 00:00:00\n",
            "2016-05-29 00:00:00\n",
            "2016-06-05 00:00:00\n",
            "2016-06-12 00:00:00\n",
            "2016-06-19 00:00:00\n",
            "2016-06-26 00:00:00\n",
            "2016-07-03 00:00:00\n",
            "2016-07-10 00:00:00\n",
            "2016-07-17 00:00:00\n",
            "2016-07-24 00:00:00\n",
            "2016-07-31 00:00:00\n",
            "2016-08-07 00:00:00\n",
            "2016-08-14 00:00:00\n",
            "2016-08-21 00:00:00\n",
            "2016-08-28 00:00:00\n",
            "2016-09-04 00:00:00\n",
            "2016-09-11 00:00:00\n",
            "2016-09-18 00:00:00\n",
            "2016-09-25 00:00:00\n",
            "2016-10-02 00:00:00\n",
            "2016-10-09 00:00:00\n",
            "2016-10-16 00:00:00\n",
            "2016-10-23 00:00:00\n",
            "2016-10-30 00:00:00\n",
            "2016-11-06 00:00:00\n",
            "2016-11-13 00:00:00\n",
            "2016-11-20 00:00:00\n",
            "2016-11-27 00:00:00\n",
            "2016-12-04 00:00:00\n",
            "2016-12-11 00:00:00\n",
            "2016-12-18 00:00:00\n",
            "2016-12-25 00:00:00\n",
            "2017-01-01 00:00:00\n",
            "2017-01-08 00:00:00\n",
            "2017-01-15 00:00:00\n",
            "2017-01-22 00:00:00\n",
            "2017-01-29 00:00:00\n",
            "2017-02-05 00:00:00\n",
            "2017-02-12 00:00:00\n",
            "2017-02-19 00:00:00\n",
            "2017-02-26 00:00:00\n",
            "2017-03-05 00:00:00\n",
            "2017-03-12 00:00:00\n",
            "2017-03-19 00:00:00\n",
            "2017-03-26 00:00:00\n",
            "2017-04-02 00:00:00\n",
            "2017-04-09 00:00:00\n",
            "2017-04-16 00:00:00\n",
            "2017-04-23 00:00:00\n",
            "2017-04-30 00:00:00\n",
            "2017-05-07 00:00:00\n",
            "2017-05-14 00:00:00\n",
            "2017-05-21 00:00:00\n",
            "2017-05-28 00:00:00\n",
            "2017-06-04 00:00:00\n",
            "2017-06-11 00:00:00\n",
            "2017-06-18 00:00:00\n",
            "2017-06-25 00:00:00\n",
            "2017-07-02 00:00:00\n",
            "2017-07-09 00:00:00\n",
            "2017-07-16 00:00:00\n",
            "2017-07-23 00:00:00\n",
            "2017-07-30 00:00:00\n",
            "2017-08-06 00:00:00\n",
            "2017-08-13 00:00:00\n",
            "2017-08-20 00:00:00\n",
            "2017-08-27 00:00:00\n",
            "2017-09-03 00:00:00\n",
            "2017-09-10 00:00:00\n",
            "2017-09-17 00:00:00\n",
            "2017-09-24 00:00:00\n",
            "2017-10-01 00:00:00\n",
            "2017-10-08 00:00:00\n",
            "2017-10-15 00:00:00\n",
            "2017-10-22 00:00:00\n",
            "2017-10-29 00:00:00\n",
            "2017-11-05 00:00:00\n",
            "2017-11-12 00:00:00\n",
            "2017-11-19 00:00:00\n",
            "2017-11-26 00:00:00\n",
            "2017-12-03 00:00:00\n",
            "2017-12-10 00:00:00\n",
            "2017-12-17 00:00:00\n",
            "2017-12-24 00:00:00\n",
            "2017-12-31 00:00:00\n",
            "2018-01-07 00:00:00\n",
            "2018-01-14 00:00:00\n",
            "2018-01-21 00:00:00\n",
            "2018-01-28 00:00:00\n",
            "2018-02-04 00:00:00\n",
            "2018-02-11 00:00:00\n",
            "2018-02-18 00:00:00\n",
            "2018-02-25 00:00:00\n",
            "2018-03-04 00:00:00\n",
            "2018-03-11 00:00:00\n",
            "2018-03-18 00:00:00\n",
            "2018-03-25 00:00:00\n",
            "2018-04-01 00:00:00\n",
            "2018-04-08 00:00:00\n",
            "2018-04-15 00:00:00\n",
            "2018-04-22 00:00:00\n",
            "2018-04-29 00:00:00\n",
            "2018-05-06 00:00:00\n",
            "2018-05-13 00:00:00\n",
            "2018-05-20 00:00:00\n",
            "2018-05-27 00:00:00\n",
            "2018-06-03 00:00:00\n",
            "2018-06-10 00:00:00\n",
            "2018-06-17 00:00:00\n",
            "2018-06-24 00:00:00\n",
            "2018-07-01 00:00:00\n",
            "2018-07-08 00:00:00\n",
            "2018-07-15 00:00:00\n",
            "2018-07-22 00:00:00\n",
            "2018-07-29 00:00:00\n",
            "2018-08-05 00:00:00\n",
            "2018-08-12 00:00:00\n",
            "2018-08-19 00:00:00\n",
            "2018-08-26 00:00:00\n",
            "2018-09-02 00:00:00\n",
            "2018-09-09 00:00:00\n",
            "2018-09-16 00:00:00\n",
            "2018-09-23 00:00:00\n",
            "2018-09-30 00:00:00\n",
            "2018-10-07 00:00:00\n",
            "2018-10-14 00:00:00\n",
            "2018-10-21 00:00:00\n",
            "2018-10-28 00:00:00\n",
            "2018-11-04 00:00:00\n",
            "2018-11-11 00:00:00\n",
            "2018-11-18 00:00:00\n",
            "2018-11-25 00:00:00\n",
            "2018-12-02 00:00:00\n",
            "2018-12-09 00:00:00\n",
            "2018-12-16 00:00:00\n",
            "2018-12-23 00:00:00\n",
            "2018-12-30 00:00:00\n",
            "2019-01-06 00:00:00\n",
            "2019-01-13 00:00:00\n",
            "2019-01-20 00:00:00\n",
            "2019-01-27 00:00:00\n",
            "2019-02-03 00:00:00\n",
            "2019-02-10 00:00:00\n",
            "2019-02-17 00:00:00\n",
            "2019-02-24 00:00:00\n",
            "2019-03-03 00:00:00\n",
            "2019-03-10 00:00:00\n",
            "2019-03-17 00:00:00\n",
            "2019-03-24 00:00:00\n",
            "2019-03-31 00:00:00\n",
            "2019-04-07 00:00:00\n",
            "2019-04-14 00:00:00\n",
            "2019-04-21 00:00:00\n",
            "2019-04-28 00:00:00\n",
            "2019-05-05 00:00:00\n",
            "2019-05-12 00:00:00\n",
            "2019-05-19 00:00:00\n",
            "2019-05-26 00:00:00\n",
            "2019-06-02 00:00:00\n",
            "2019-06-09 00:00:00\n",
            "2019-06-16 00:00:00\n",
            "2019-06-23 00:00:00\n",
            "2019-06-30 00:00:00\n",
            "2019-07-07 00:00:00\n",
            "2019-07-14 00:00:00\n",
            "2019-07-21 00:00:00\n",
            "2019-07-28 00:00:00\n",
            "2019-08-04 00:00:00\n",
            "2019-08-11 00:00:00\n",
            "2019-08-18 00:00:00\n",
            "2019-08-25 00:00:00\n",
            "2019-09-01 00:00:00\n",
            "2019-09-08 00:00:00\n",
            "2019-09-15 00:00:00\n",
            "2019-09-22 00:00:00\n",
            "2019-09-29 00:00:00\n",
            "2019-10-06 00:00:00\n",
            "2019-10-13 00:00:00\n",
            "2019-10-20 00:00:00\n",
            "2019-10-27 00:00:00\n",
            "2019-11-03 00:00:00\n",
            "2019-11-10 00:00:00\n",
            "2019-11-17 00:00:00\n",
            "2019-11-24 00:00:00\n",
            "2019-12-01 00:00:00\n",
            "2019-12-08 00:00:00\n",
            "2019-12-15 00:00:00\n",
            "2019-12-22 00:00:00\n",
            "2019-12-29 00:00:00\n",
            "2020-01-05 00:00:00\n",
            "2020-01-12 00:00:00\n",
            "2020-01-19 00:00:00\n",
            "2020-01-26 00:00:00\n",
            "2020-02-02 00:00:00\n",
            "2020-02-09 00:00:00\n",
            "2020-02-16 00:00:00\n",
            "2020-02-23 00:00:00\n",
            "2020-03-01 00:00:00\n",
            "2020-03-08 00:00:00\n",
            "2020-03-15 00:00:00\n",
            "2020-03-22 00:00:00\n",
            "2020-03-29 00:00:00\n",
            "2020-04-05 00:00:00\n",
            "2020-04-12 00:00:00\n",
            "2020-04-19 00:00:00\n",
            "2020-04-26 00:00:00\n",
            "2020-05-03 00:00:00\n",
            "2020-05-10 00:00:00\n",
            "2020-05-17 00:00:00\n",
            "2020-05-24 00:00:00\n",
            "2020-05-31 00:00:00\n",
            "2020-06-07 00:00:00\n",
            "2020-06-14 00:00:00\n",
            "2020-06-21 00:00:00\n",
            "2020-06-28 00:00:00\n",
            "2020-07-05 00:00:00\n",
            "2020-07-12 00:00:00\n",
            "2020-07-19 00:00:00\n",
            "2020-07-26 00:00:00\n",
            "2020-08-02 00:00:00\n",
            "2020-08-09 00:00:00\n",
            "2020-08-16 00:00:00\n",
            "2020-08-23 00:00:00\n",
            "2020-08-30 00:00:00\n",
            "2020-09-06 00:00:00\n",
            "2020-09-13 00:00:00\n",
            "2020-09-20 00:00:00\n",
            "2020-09-27 00:00:00\n",
            "2020-10-04 00:00:00\n",
            "2020-10-11 00:00:00\n",
            "2020-10-18 00:00:00\n",
            "2020-10-25 00:00:00\n",
            "2020-11-01 00:00:00\n",
            "2020-11-08 00:00:00\n",
            "2020-11-15 00:00:00\n",
            "2020-11-22 00:00:00\n",
            "2020-11-29 00:00:00\n",
            "2020-12-06 00:00:00\n",
            "2020-12-13 00:00:00\n",
            "2020-12-20 00:00:00\n",
            "2020-12-27 00:00:00\n",
            "2021-01-03 00:00:00\n",
            "2021-01-10 00:00:00\n",
            "2021-01-17 00:00:00\n",
            "2021-01-24 00:00:00\n",
            "2021-01-31 00:00:00\n",
            "2021-02-07 00:00:00\n",
            "2021-02-14 00:00:00\n",
            "2021-02-21 00:00:00\n",
            "2021-02-28 00:00:00\n",
            "2021-03-07 00:00:00\n",
            "2021-03-14 00:00:00\n",
            "2021-03-21 00:00:00\n",
            "2021-03-28 00:00:00\n",
            "2021-04-04 00:00:00\n",
            "2021-04-11 00:00:00\n",
            "2021-04-18 00:00:00\n",
            "2021-04-25 00:00:00\n",
            "2021-05-02 00:00:00\n",
            "2021-05-09 00:00:00\n",
            "2021-05-16 00:00:00\n",
            "2021-05-23 00:00:00\n",
            "2021-05-30 00:00:00\n",
            "2021-06-06 00:00:00\n",
            "2021-06-13 00:00:00\n",
            "2021-06-20 00:00:00\n",
            "2021-06-27 00:00:00\n",
            "2021-07-04 00:00:00\n",
            "2021-07-11 00:00:00\n",
            "2021-07-18 00:00:00\n",
            "2021-07-25 00:00:00\n",
            "2021-08-01 00:00:00\n",
            "2021-08-08 00:00:00\n",
            "2021-08-15 00:00:00\n",
            "2021-08-22 00:00:00\n",
            "2021-08-29 00:00:00\n",
            "2021-09-05 00:00:00\n",
            "2021-09-12 00:00:00\n",
            "2021-09-19 00:00:00\n",
            "2021-09-26 00:00:00\n",
            "2021-10-03 00:00:00\n",
            "2021-10-10 00:00:00\n",
            "2021-10-17 00:00:00\n",
            "2021-10-24 00:00:00\n",
            "2021-10-31 00:00:00\n",
            "2021-11-07 00:00:00\n",
            "2021-11-14 00:00:00\n",
            "2021-11-21 00:00:00\n",
            "2021-11-28 00:00:00\n",
            "2021-12-05 00:00:00\n",
            "2021-12-12 00:00:00\n",
            "2021-12-19 00:00:00\n",
            "2021-12-26 00:00:00\n",
            "2022-01-02 00:00:00\n",
            "2022-01-09 00:00:00\n",
            "2022-01-16 00:00:00\n",
            "2022-01-23 00:00:00\n",
            "2022-01-30 00:00:00\n",
            "2022-02-06 00:00:00\n",
            "2022-02-13 00:00:00\n",
            "2022-02-20 00:00:00\n",
            "2022-02-27 00:00:00\n",
            "2022-03-06 00:00:00\n",
            "2022-03-13 00:00:00\n",
            "2022-03-20 00:00:00\n",
            "2022-03-27 00:00:00\n",
            "2022-04-03 00:00:00\n",
            "2022-04-10 00:00:00\n",
            "2022-04-17 00:00:00\n",
            "2022-04-24 00:00:00\n",
            "2022-05-01 00:00:00\n",
            "2022-05-08 00:00:00\n",
            "2022-05-15 00:00:00\n",
            "2022-05-22 00:00:00\n",
            "2022-05-29 00:00:00\n",
            "2022-06-05 00:00:00\n",
            "2022-06-12 00:00:00\n",
            "2022-06-19 00:00:00\n",
            "2022-06-26 00:00:00\n",
            "2022-07-03 00:00:00\n",
            "2022-07-10 00:00:00\n",
            "2022-07-17 00:00:00\n",
            "2022-07-24 00:00:00\n",
            "2022-07-31 00:00:00\n",
            "2022-08-07 00:00:00\n",
            "2022-08-14 00:00:00\n",
            "2022-08-21 00:00:00\n",
            "2022-08-28 00:00:00\n",
            "2022-09-04 00:00:00\n",
            "2022-09-11 00:00:00\n",
            "2022-09-18 00:00:00\n",
            "2022-09-25 00:00:00\n",
            "2022-10-02 00:00:00\n",
            "2022-10-09 00:00:00\n",
            "2022-10-16 00:00:00\n",
            "2022-10-23 00:00:00\n",
            "2022-10-30 00:00:00\n",
            "2022-11-06 00:00:00\n",
            "2022-11-13 00:00:00\n",
            "2022-11-20 00:00:00\n",
            "2022-11-27 00:00:00\n",
            "2022-12-04 00:00:00\n",
            "2022-12-11 00:00:00\n",
            "2022-12-18 00:00:00\n",
            "2022-12-25 00:00:00\n",
            "2023-01-01 00:00:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "backtest_mean = backtest_returns.mean()\n",
        "backtest_var = backtest_returns.var()"
      ],
      "metadata": {
        "id": "1QJznmoa50iU"
      },
      "execution_count": 428,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NoPoints = len(backtest_mean)\n",
        "\n",
        "colours = \"blue\"\n",
        "area = np.pi*3\n",
        "\n",
        "plt.title('Efficient Frontier for ML Portfolio')\n",
        "plt.xlabel('Annualized Risk(%)')\n",
        "plt.ylabel('Annualized Expected Portfolio Return(%)' )\n",
        "plt.scatter(backtest_mean * (trading_days_in_year /5), np.sqrt(backtest_var * (trading_days_in_year /5)), s=area, c=colours, alpha =0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "Uy0n-yp8SPt5",
        "outputId": "3350ba2e-60cb-48e5-8277-50aebdacb251"
      },
      "execution_count": 429,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MV Portfolio"
      ],
      "metadata": {
        "id": "a-aPJVsE3U1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ms = np.arange(0.02, 0.1, 0.005)\n",
        "mv_backtest_weights = pd.DataFrame(columns = ms)\n",
        "mv_backtest_returns = pd.DataFrame(columns = ms)\n",
        "\n",
        "for date in daterange:\n",
        "  print(date)\n",
        "  dataset = get_mv_dataset_for_date(date)\n",
        "  r, cov = get_sample_return_and_covariance(dataset)\n",
        "  weights = []\n",
        "  rets = []\n",
        "  for m in ms:\n",
        "    weight = MinimizeRiskConstr(r, cov, 2, m)\n",
        "    high_risk_weight = weight.x[0]\n",
        "    ret = get_mv_backtest_return(date, high_risk_weight)\n",
        "    weights.append(high_risk_weight)\n",
        "    rets.append(ret)\n",
        "  mv_backtest_weights.loc[date] = weights\n",
        "  mv_backtest_returns.loc[date] = rets\n",
        "  \n",
        "  # a,b = train_and_get_a_b(dataset[:-1])  \n",
        "  # with torch.no_grad():\n",
        "  #   y_test = torch.exp(torch.matmul(torch.from_numpy(dataset[-1][:-1]), a)) / (torch.exp (torch.matmul(torch.from_numpy(dataset[-1][:-1]), a)) + torch.exp(torch.matmul(torch.from_numpy(dataset[-1][:-1]), b)))\n",
        "  #   print(y_test)\n",
        "  #   parameters.loc[date] = [a,b,y_test.numpy()]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XevciqKG3aJ0",
        "outputId": "65407205-e570-4cf2-f0f4-698f009b3d34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2003-09-21 00:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_hessian_update_strategy.py:182: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  warn('delta_grad == 0.0. Check if the approximated '\n",
            "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_trustregion_constr/projections.py:181: UserWarning: Singular Jacobian matrix. Using SVD decomposition to perform the factorizations.\n",
            "  warn('Singular Jacobian matrix. Using SVD decomposition to ' +\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2003-09-28 00:00:00\n",
            "2003-10-05 00:00:00\n",
            "2003-10-12 00:00:00\n",
            "2003-10-19 00:00:00\n",
            "2003-10-26 00:00:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross validation"
      ],
      "metadata": {
        "id": "59JkxM_-c9uk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model = torch.exp(torch.matmul(x, a)) / (torch.exp (torch.matmul(x, a)) + torch.exp(torch.matmul(x, b)))\n",
        "\n",
        "def train_epoch(dataloader):\n",
        "    train_loss,train_correct=0.0,0\n",
        "    for data in dataloader:\n",
        "\n",
        "        x = data[:,:-1]\n",
        "        y = data[:,-1]\n",
        "\n",
        "        y_output = torch.exp(torch.matmul(x, a)) / (torch.exp (torch.matmul(x, a)) + torch.exp(torch.matmul(x, b)))\n",
        "        loss = loss_fn(y_output, y)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        train_loss += loss.item() * x.size(0)\n",
        "\n",
        "    return train_loss\n",
        "  \n",
        "def valid_epoch(dataloader):\n",
        "\n",
        "    valid_loss, val_correct = 0.0, 0\n",
        "    for data in dataloader:\n",
        "\n",
        "        x = data[:,:-1]\n",
        "        y = data[:,-1]\n",
        "\n",
        "        y_output = torch.exp(torch.matmul(x, a)) / (torch.exp (torch.matmul(x, a)) + torch.exp(torch.matmul(x, b)))\n",
        "\n",
        "        loss = loss_fn(y_output, y)\n",
        "\n",
        "        valid_loss+=loss.item() * x.size(0)\n",
        "\n",
        "    return valid_loss"
      ],
      "metadata": {
        "id": "kBobZ4qo53Lo"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = {'train_loss': [], 'test_loss': []}\n",
        " \n",
        "for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(dataset)))):\n",
        "\n",
        "    print('Fold {}'.format(fold + 1))\n",
        "\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    test_sampler = SubsetRandomSampler(val_idx)\n",
        "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
        "    test_loader = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "        train_loss=train_epoch(train_loader)\n",
        "        test_loss=valid_epoch(test_loader)\n",
        "\n",
        "        train_loss = train_loss / len(train_loader.sampler)\n",
        "        test_loss = test_loss / len(test_loader.sampler)\n",
        "\n",
        "        if epoch % 1 == 0:\n",
        "            print(\"Epoch:{}/{} AVG Training Loss:{:.3f} AVG Test Loss:{:.3f}\".format(epoch + 1,n_epochs,train_loss,test_loss))\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['test_loss'].append(test_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "DeuyMHGu_Lmy",
        "outputId": "11c663c7-9cc6-41d6-d789-6e05591addaf"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-c07c40cb1bde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mtest_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-3768fb6a26b4>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(dataloader)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'optimizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(a, b)"
      ],
      "metadata": {
        "id": "Nsd-ud7DWRC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build efficient frontier"
      ],
      "metadata": {
        "id": "9Y7k33C-85KF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  y_test = torch.exp(torch.matmul(X_tensor, a)) / (torch.exp (torch.matmul(X_tensor, a)) + torch.exp(torch.matmul(X_tensor, b)))\n",
        "  print(y_test)"
      ],
      "metadata": {
        "id": "vazYH62WF8H4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build ML Portfolio"
      ],
      "metadata": {
        "id": "nwtZLY8wFvv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prob = pd.DataFrame(y_test).astype(\"float\")\n",
        "display(prob)\n",
        "rolling_prob = prob.rolling(25).mean().iloc[-1]\n",
        "display(rolling_prob.to_numpy()[0])"
      ],
      "metadata": {
        "id": "X1Fa8qX06tzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_ml_portfolio_weights(x, k):\n",
        "  return 0 if x < k else 1\n",
        "\n",
        "for k in np.arange(0, 1, 0.1):\n",
        "  print(calculate_ml_portfolio_weights(rolling_prob.to_numpy()[0], k))"
      ],
      "metadata": {
        "id": "FjPZY76E93xQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = 0.5\n",
        "calculate_ml_portfolio_weights_lambda = lambda x: 0 if x < k else 1\n",
        "calculate_ml_portfolio_weights = np.vectorize(calculate_ml_portfolio_weights_lambda)\n",
        "# vfunc(x)\n",
        "# calculate_ml_portfolio_weights = functorch.vmap(ml_portfolio_weights, out_dims=1)\n",
        "# forecast = \n",
        "# portfolio_weights = calculate_ml_portfolio_weights(y_test.numpy())\n",
        "# print(portfolio_weights)\n",
        "\n",
        "portfolio_weights = y_test.apply_(calculate_ml_portfolio_weights_lambda)\n",
        "print(portfolio_weights)"
      ],
      "metadata": {
        "id": "fvjJR_gI9A8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xt = torch.from_numpy(to_X_train_features(low_risk, high_risk).T[-1])\n",
        "Xt"
      ],
      "metadata": {
        "id": "w0Gjrcdn1YoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  y_test = torch.exp(torch.matmul(Xt, a)) / (torch.exp (torch.matmul(Xt, a)) + torch.exp(torch.matmul(Xt, b)))\n",
        "  print(y_test)"
      ],
      "metadata": {
        "id": "dQDb3MWz4rzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = 0.5\n",
        "calculate_ml_portfolio_weights_lambda = lambda x: 0 if x < k else 1\n",
        "calculate_ml_portfolio_weights = np.vectorize(calculate_ml_portfolio_weights_lambda)\n",
        "\n",
        "portfolio_weights = y_test.apply_(calculate_ml_portfolio_weights_lambda)\n",
        "print(portfolio_weights)"
      ],
      "metadata": {
        "id": "wo9vQZJS5BCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MV Portfolio"
      ],
      "metadata": {
        "id": "Yx3K4DmgFpd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_daily_return(market_data):\n",
        "    market_data[\"Pct Return\"]  = market_data['Close'].pct_change()\n",
        "\n",
        "add_daily_return(high_risk)\n",
        "add_daily_return(low_risk)"
      ],
      "metadata": {
        "id": "-vUNbKE2TUN2"
      },
      "execution_count": 325,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "high_risk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "s8neZvz8jlwa",
        "outputId": "7f97288f-1f6e-46f6-b1a4-6dbf3115c196"
      },
      "execution_count": 326,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            Date        Open        High         Low       Close   Adj Close  \\\n",
              "25    2002-09-04   88.610001   90.250000   88.059998   89.540001   60.436001   \n",
              "26    2002-09-05   88.489998   89.430000   87.500000   88.779999   59.923054   \n",
              "27    2002-09-06   89.750000   90.570000   89.339996   90.000000   60.746498   \n",
              "28    2002-09-09   89.099998   91.349998   88.800003   90.660004   61.191929   \n",
              "29    2002-09-10   91.139999   91.779999   90.559998   91.699997   61.893936   \n",
              "...          ...         ...         ...         ...         ...         ...   \n",
              "5142  2022-12-30  380.640015  382.579987  378.429993  382.429993  382.429993   \n",
              "5143  2023-01-03  384.369995  386.429993  377.829987  380.820007  380.820007   \n",
              "5144  2023-01-04  383.179993  385.880005  380.000000  383.760010  383.760010   \n",
              "5145  2023-01-05  381.720001  381.839996  378.760010  379.380005  379.380005   \n",
              "5146  2023-01-06  382.609985  389.250000  379.410004  388.079987  388.079987   \n",
              "\n",
              "        Volume  Daily Return        MA  Next Close       ROE  ROE Binary  \\\n",
              "25    0.550024      0.479605  0.276737   88.779999 -0.008488           0   \n",
              "26    0.723874      0.149555  0.229367   90.000000  0.013742           1   \n",
              "27    0.415721      0.128926  0.522307   90.660004  0.007333           1   \n",
              "28    0.365951      0.804501  0.929931   91.699997  0.011471           1   \n",
              "29    0.445799      0.288793  1.338801   91.129997 -0.006216           0   \n",
              "...        ...           ...       ...         ...       ...         ...   \n",
              "5142  0.903889      0.923099 -1.149319  380.820007 -0.004210           1   \n",
              "5143  0.805676     -1.830743 -1.654172  383.760010  0.007720           1   \n",
              "5144  0.924976      0.299117 -1.185467  379.380005 -0.011413           0   \n",
              "5145  0.828493     -1.206745 -1.374945  388.079987  0.022932           1   \n",
              "5146  1.119878      2.820901 -2.212630         NaN       NaN           1   \n",
              "\n",
              "      Pct Return  \n",
              "25           NaN  \n",
              "26     -0.008488  \n",
              "27      0.013742  \n",
              "28      0.007333  \n",
              "29      0.011471  \n",
              "...          ...  \n",
              "5142   -0.002634  \n",
              "5143   -0.004210  \n",
              "5144    0.007720  \n",
              "5145   -0.011413  \n",
              "5146    0.022932  \n",
              "\n",
              "[5122 rows x 13 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f908cc87-e47e-4d40-ad37-74328a281d42\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Daily Return</th>\n",
              "      <th>MA</th>\n",
              "      <th>Next Close</th>\n",
              "      <th>ROE</th>\n",
              "      <th>ROE Binary</th>\n",
              "      <th>Pct Return</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>2002-09-04</td>\n",
              "      <td>88.610001</td>\n",
              "      <td>90.250000</td>\n",
              "      <td>88.059998</td>\n",
              "      <td>89.540001</td>\n",
              "      <td>60.436001</td>\n",
              "      <td>0.550024</td>\n",
              "      <td>0.479605</td>\n",
              "      <td>0.276737</td>\n",
              "      <td>88.779999</td>\n",
              "      <td>-0.008488</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>2002-09-05</td>\n",
              "      <td>88.489998</td>\n",
              "      <td>89.430000</td>\n",
              "      <td>87.500000</td>\n",
              "      <td>88.779999</td>\n",
              "      <td>59.923054</td>\n",
              "      <td>0.723874</td>\n",
              "      <td>0.149555</td>\n",
              "      <td>0.229367</td>\n",
              "      <td>90.000000</td>\n",
              "      <td>0.013742</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.008488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>2002-09-06</td>\n",
              "      <td>89.750000</td>\n",
              "      <td>90.570000</td>\n",
              "      <td>89.339996</td>\n",
              "      <td>90.000000</td>\n",
              "      <td>60.746498</td>\n",
              "      <td>0.415721</td>\n",
              "      <td>0.128926</td>\n",
              "      <td>0.522307</td>\n",
              "      <td>90.660004</td>\n",
              "      <td>0.007333</td>\n",
              "      <td>1</td>\n",
              "      <td>0.013742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>2002-09-09</td>\n",
              "      <td>89.099998</td>\n",
              "      <td>91.349998</td>\n",
              "      <td>88.800003</td>\n",
              "      <td>90.660004</td>\n",
              "      <td>61.191929</td>\n",
              "      <td>0.365951</td>\n",
              "      <td>0.804501</td>\n",
              "      <td>0.929931</td>\n",
              "      <td>91.699997</td>\n",
              "      <td>0.011471</td>\n",
              "      <td>1</td>\n",
              "      <td>0.007333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>2002-09-10</td>\n",
              "      <td>91.139999</td>\n",
              "      <td>91.779999</td>\n",
              "      <td>90.559998</td>\n",
              "      <td>91.699997</td>\n",
              "      <td>61.893936</td>\n",
              "      <td>0.445799</td>\n",
              "      <td>0.288793</td>\n",
              "      <td>1.338801</td>\n",
              "      <td>91.129997</td>\n",
              "      <td>-0.006216</td>\n",
              "      <td>0</td>\n",
              "      <td>0.011471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5142</th>\n",
              "      <td>2022-12-30</td>\n",
              "      <td>380.640015</td>\n",
              "      <td>382.579987</td>\n",
              "      <td>378.429993</td>\n",
              "      <td>382.429993</td>\n",
              "      <td>382.429993</td>\n",
              "      <td>0.903889</td>\n",
              "      <td>0.923099</td>\n",
              "      <td>-1.149319</td>\n",
              "      <td>380.820007</td>\n",
              "      <td>-0.004210</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.002634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5143</th>\n",
              "      <td>2023-01-03</td>\n",
              "      <td>384.369995</td>\n",
              "      <td>386.429993</td>\n",
              "      <td>377.829987</td>\n",
              "      <td>380.820007</td>\n",
              "      <td>380.820007</td>\n",
              "      <td>0.805676</td>\n",
              "      <td>-1.830743</td>\n",
              "      <td>-1.654172</td>\n",
              "      <td>383.760010</td>\n",
              "      <td>0.007720</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.004210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5144</th>\n",
              "      <td>2023-01-04</td>\n",
              "      <td>383.179993</td>\n",
              "      <td>385.880005</td>\n",
              "      <td>380.000000</td>\n",
              "      <td>383.760010</td>\n",
              "      <td>383.760010</td>\n",
              "      <td>0.924976</td>\n",
              "      <td>0.299117</td>\n",
              "      <td>-1.185467</td>\n",
              "      <td>379.380005</td>\n",
              "      <td>-0.011413</td>\n",
              "      <td>0</td>\n",
              "      <td>0.007720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5145</th>\n",
              "      <td>2023-01-05</td>\n",
              "      <td>381.720001</td>\n",
              "      <td>381.839996</td>\n",
              "      <td>378.760010</td>\n",
              "      <td>379.380005</td>\n",
              "      <td>379.380005</td>\n",
              "      <td>0.828493</td>\n",
              "      <td>-1.206745</td>\n",
              "      <td>-1.374945</td>\n",
              "      <td>388.079987</td>\n",
              "      <td>0.022932</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.011413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5146</th>\n",
              "      <td>2023-01-06</td>\n",
              "      <td>382.609985</td>\n",
              "      <td>389.250000</td>\n",
              "      <td>379.410004</td>\n",
              "      <td>388.079987</td>\n",
              "      <td>388.079987</td>\n",
              "      <td>1.119878</td>\n",
              "      <td>2.820901</td>\n",
              "      <td>-2.212630</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>0.022932</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5122 rows √ó 13 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f908cc87-e47e-4d40-ad37-74328a281d42')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f908cc87-e47e-4d40-ad37-74328a281d42 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f908cc87-e47e-4d40-ad37-74328a281d42');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 326
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "high_risk_return_annual = high_risk[\"Pct Return\"].mean() * trading_days_in_year\n",
        "low_risk_return_annual = low_risk[\"Pct Return\"].mean() * trading_days_in_year\n",
        "print(high_risk_return_annual)\n",
        "print(low_risk_return_annual)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Pw_6fyZUZLW",
        "outputId": "b889cde5-6bf0-454d-e20e-541ef89400ef"
      },
      "execution_count": 327,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.09088309481924065\n",
            "0.009396836284394002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "high_risk_var_daily = high_risk[\"Pct Return\"].var()\n",
        "low_risk_var_daily = low_risk[\"Pct Return\"].var()\n",
        "print(high_risk_var_daily)\n",
        "print(low_risk_var_daily)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHNvPvKeU552",
        "outputId": "0f0ad777-fbfa-471c-b99d-8a1302ba4dc7"
      },
      "execution_count": 328,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.00014831962827593315\n",
            "1.840738816881264e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build data for high and low risk"
      ],
      "metadata": {
        "id": "z1A4Jk4uWHDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mv_data = pd.DataFrame(data={'high': high_risk['Close'], 'low':low_risk['Close']})\n",
        "mv_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "e5kg5IleWKbF",
        "outputId": "902f30c3-a1c6-4bdc-96cd-4913329c171c"
      },
      "execution_count": 329,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            high        low\n",
              "25     89.540001  85.199997\n",
              "26     88.779999  85.540001\n",
              "27     90.000000  84.879997\n",
              "28     90.660004  84.760002\n",
              "29     91.699997  85.059998\n",
              "...          ...        ...\n",
              "5142  382.429993  95.779999\n",
              "5143  380.820007  96.529999\n",
              "5144  383.760010  97.269997\n",
              "5145  379.380005  97.129997\n",
              "5146  388.079987  98.379997\n",
              "\n",
              "[5122 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0c9216fc-e2b2-43e2-a4fc-f0b1e76b0c9b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>89.540001</td>\n",
              "      <td>85.199997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>88.779999</td>\n",
              "      <td>85.540001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>90.000000</td>\n",
              "      <td>84.879997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>90.660004</td>\n",
              "      <td>84.760002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>91.699997</td>\n",
              "      <td>85.059998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5142</th>\n",
              "      <td>382.429993</td>\n",
              "      <td>95.779999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5143</th>\n",
              "      <td>380.820007</td>\n",
              "      <td>96.529999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5144</th>\n",
              "      <td>383.760010</td>\n",
              "      <td>97.269997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5145</th>\n",
              "      <td>379.380005</td>\n",
              "      <td>97.129997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5146</th>\n",
              "      <td>388.079987</td>\n",
              "      <td>98.379997</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5122 rows √ó 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0c9216fc-e2b2-43e2-a4fc-f0b1e76b0c9b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0c9216fc-e2b2-43e2-a4fc-f0b1e76b0c9b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0c9216fc-e2b2-43e2-a4fc-f0b1e76b0c9b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 329
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PtpR6k1c1ccD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4kyrIwXLVWiP"
      },
      "execution_count": 330,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r, cov = get_annual_sample_return_and_covariance(mv_data)\n",
        "display(r)\n",
        "display(cov)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "zI1us-ILNW0l",
        "outputId": "4c12ad82-3315-47bd-9961-b5abbb29a070"
      },
      "execution_count": 331,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "high    0.090883\n",
              "low     0.009397\n",
              "dtype: float64"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "          high       low\n",
              "high  0.037377 -0.004467\n",
              "low  -0.004467  0.004639"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fff8c9a0-6e6c-4026-a129-56ed77510896\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>high</th>\n",
              "      <td>0.037377</td>\n",
              "      <td>-0.004467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>low</th>\n",
              "      <td>-0.004467</td>\n",
              "      <td>0.004639</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fff8c9a0-6e6c-4026-a129-56ed77510896')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fff8c9a0-6e6c-4026-a129-56ed77510896 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fff8c9a0-6e6c-4026-a129-56ed77510896');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sample_return_and_covariance(data):\n",
        "    daily_return = data.pct_change().mean()\n",
        "    daily_covariance = data.pct_change().cov()\n",
        "    return daily_return, daily_covariance"
      ],
      "metadata": {
        "id": "SA4aHkEINXGO"
      },
      "execution_count": 332,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r, cov = get_sample_return_and_covariance(mv_data)\n",
        "display(r)\n",
        "display(cov)"
      ],
      "metadata": {
        "id": "-7yPFP8lXtiM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "077c9962-d0b6-4d8e-c298-231558c55d02"
      },
      "execution_count": 333,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "high    0.000361\n",
              "low     0.000037\n",
              "dtype: float64"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "          high       low\n",
              "high  0.000148 -0.000018\n",
              "low  -0.000018  0.000018"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-89f8a119-6649-4a40-a6d6-3d3732362fcd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>high</th>\n",
              "      <td>0.000148</td>\n",
              "      <td>-0.000018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>low</th>\n",
              "      <td>-0.000018</td>\n",
              "      <td>0.000018</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-89f8a119-6649-4a40-a6d6-3d3732362fcd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-89f8a119-6649-4a40-a6d6-3d3732362fcd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-89f8a119-6649-4a40-a6d6-3d3732362fcd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimization using linear programming\n",
        "\n",
        "(Reference: https://www.kaggle.com/code/vijipai/lesson-5-mean-variance-optimization-of-portfolios)\n"
      ],
      "metadata": {
        "id": "8BOYoifDBaqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TERMINATION = 10**-9"
      ],
      "metadata": {
        "id": "G_t1dGnAtHYV"
      },
      "execution_count": 334,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function obtains maximal return portfolio using linear programming\n",
        "\n",
        "def MaximizeReturns(MeanReturns, PortfolioSize):\n",
        "    \n",
        "    #dependencies\n",
        "    from scipy.optimize import linprog\n",
        "    import numpy as np\n",
        "    \n",
        "    c = (np.multiply(-1, MeanReturns))\n",
        "    A = np.ones([PortfolioSize,1]).T\n",
        "    b=[1] \n",
        "    res = linprog(c, A_ub = A, b_ub = b, bounds = (0,1), method = 'simplex') \n",
        "    \n",
        "    return res"
      ],
      "metadata": {
        "id": "65SYaxYzFVOi"
      },
      "execution_count": 335,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function obtains minimal risk portfolio \n",
        "\n",
        "from scipy import optimize \n",
        "\n",
        "def MinimizeRisk(CovarReturns, PortfolioSize):\n",
        "    \n",
        "    def  f(x, CovarReturns):\n",
        "        func = np.matmul(np.matmul(x, CovarReturns), x.T) \n",
        "        return func\n",
        "\n",
        "    def constraintEq(x):\n",
        "        A=np.ones(x.shape)\n",
        "        b=1\n",
        "        constraintVal = np.matmul(A,x.T)-b \n",
        "        return constraintVal\n",
        "    \n",
        "    xinit=np.repeat(0.1, PortfolioSize)\n",
        "    cons = ({'type': 'eq', 'fun':constraintEq})\n",
        "    lb = 0\n",
        "    ub = 1\n",
        "    bnds = tuple([(lb,ub) for x in xinit])\n",
        "\n",
        "    opt = optimize.minimize (f, x0 = xinit, args = (CovarReturns),  bounds = bnds, \\\n",
        "                             constraints = cons, tol = TERMINATION)\n",
        "    \n",
        "    return opt"
      ],
      "metadata": {
        "id": "ESkZdqjLb_MH"
      },
      "execution_count": 336,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_min_variance_portfolio(mean_returns, cov_returns):\n",
        "    number_of_assets = len(mean_returns)\n",
        "    result = MinimizeRisk(cov_returns, number_of_assets)\n",
        "\n",
        "    print()\n",
        "    minRiskWeights = result.x\n",
        "    minRiskExpPortfolioReturn = np.matmul(mean_returns.T, minRiskWeights)\n",
        "    print(\"Expected Return of Minimum Risk Portfolio:  %7.6f\" % minRiskExpPortfolioReturn)\n",
        "    minRisk = np.matmul(np.matmul(minRiskWeights, cov_returns), minRiskWeights.T) \n",
        "    print(\"Variance of Minimum Risk Portfolio : %7.6f\" % minRisk)\n",
        "    print(\"S.D. of Minimum Risk Portfolio : %7.6f\" % np.sqrt(minRisk))\n",
        "    threshold = 1e-3\n",
        "    print(\"Weights (showing only those > %.6f): \" % threshold)\n",
        "    for i in range(0, number_of_assets):\n",
        "        if result.x[i] > threshold:\n",
        "            print(f\"{mean_returns.index[i]}\\t{result.x[i]:.6f}\")\n",
        "    print('Assets Considered:')\n",
        "    print(mean_returns.index.to_numpy())"
      ],
      "metadata": {
        "id": "i6r-NvbtYDDn"
      },
      "execution_count": 337,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function obtains Minimal risk and Maximum return portfolios\n",
        "\n",
        "#dependencies\n",
        "import numpy as np\n",
        "from scipy import optimize \n",
        "\n",
        "def MinimizeRiskConstr(MeanReturns, CovarReturns, PortfolioSize, R):\n",
        "    \n",
        "    def  f(x,CovarReturns):\n",
        "         \n",
        "        func = np.matmul(np.matmul(x,CovarReturns ), x.T)\n",
        "        return func\n",
        "\n",
        "    def constraintEq(x):\n",
        "        AEq=np.ones(x.shape)\n",
        "        bEq=1\n",
        "        EqconstraintVal = np.matmul(AEq,x.T)-bEq \n",
        "        return EqconstraintVal\n",
        "    \n",
        "    def constraintIneq(x, MeanReturns, R):\n",
        "        AIneq = np.array(MeanReturns)\n",
        "        bIneq = R\n",
        "        IneqconstraintVal = np.matmul(AIneq,x.T) - bIneq\n",
        "        return IneqconstraintVal\n",
        "    \n",
        "\n",
        "    xinit=np.repeat(0.1, PortfolioSize)\n",
        "    cons = ({'type': 'eq', 'fun':constraintEq},\n",
        "            {'type':'ineq', 'fun':constraintIneq, 'args':(MeanReturns,R) })\n",
        "    lb = 0\n",
        "    ub = 1\n",
        "    bnds = tuple([(lb,ub) for x in xinit])\n",
        "\n",
        "    opt = optimize.minimize (f, args = (CovarReturns), method ='trust-constr',  \\\n",
        "                        x0 = xinit,   bounds = bnds, constraints = cons, tol = TERMINATION)\n",
        "    \n",
        "    return  opt"
      ],
      "metadata": {
        "id": "DNuu3KurcJDL"
      },
      "execution_count": 338,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_min_variance_portfolio(r, cov)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRkZ69QycGDx",
        "outputId": "397d45e6-87fe-42e6-80bd-a9318a09359d"
      },
      "execution_count": 339,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Expected Return of Minimum Risk Portfolio:  0.000095\n",
            "Variance of Minimum Risk Portfolio : 0.000012\n",
            "S.D. of Minimum Risk Portfolio : 0.003457\n",
            "Weights (showing only those > 0.001000): \n",
            "high\t0.178717\n",
            "low\t0.821283\n",
            "Assets Considered:\n",
            "['high' 'low']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Maximal expected portfolio return computation for the k-portfolio\n",
        "result1 = MaximizeReturns(r, 2)\n",
        "maxReturnWeights = result1.x\n",
        "maxExpPortfolioReturn = np.matmul(r.T, maxReturnWeights)\n",
        "print(\"Maximal Expected Portfolio Return:   %7.6f\" % maxExpPortfolioReturn )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvkOfW5-GXpS",
        "outputId": "c5aef8a7-0545-4113-db0a-8168d89a3848"
      },
      "execution_count": 340,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximal Expected Portfolio Return:   0.000361\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#expected portfolio return computation for the minimum risk k-portfolio \n",
        "result2 = MinimizeRisk(cov, 2)\n",
        "minRiskWeights = result2.x\n",
        "minRiskExpPortfolioReturn = np.matmul(r.T, minRiskWeights)\n",
        "print(\"Expected Return of Minimum Risk Portfolio:  %7.6f\" % minRiskExpPortfolioReturn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnMjflTbGaO7",
        "outputId": "e2db7135-9fe0-4a12-b37c-b6d2d775adf4"
      },
      "execution_count": 341,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected Return of Minimum Risk Portfolio:  0.000095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#compute efficient set for the maximum return and minimum risk portfolios\n",
        "increment = 0.000001\n",
        "low = minRiskExpPortfolioReturn\n",
        "high = maxExpPortfolioReturn\n",
        "\n",
        "#initialize optimal weight set and risk-return point set\n",
        "xOptimal =[]\n",
        "minRiskPoint = []\n",
        "expPortfolioReturnPoint =[]\n",
        "\n",
        "#repeated execution of function MinimizeRiskConstr to determine the efficient set \n",
        "while (low < high):\n",
        "    \n",
        "    result3 = MinimizeRiskConstr(r, cov, 2, low)\n",
        "    xOptimal.append(result3.x)\n",
        "    expPortfolioReturnPoint.append(low)\n",
        "    low = low+increment\n",
        "    \n",
        "#gather optimal weight set    \n",
        "xOptimalArray = np.array(xOptimal)\n",
        "\n",
        "#obtain annualized risk for the efficient set portfolios \n",
        "#for trading days = 251\n",
        "minRiskPoint = np.diagonal(np.matmul((np.matmul(xOptimalArray,cov)),\\\n",
        "                                     np.transpose(xOptimalArray)))\n",
        "riskPoint =   np.sqrt(minRiskPoint*trading_days_in_year) \n",
        "\n",
        "#obtain expected portfolio annualized return for the \n",
        "#efficient set portfolios, for trading days = 251\n",
        "retPoint = trading_days_in_year*np.array(expPortfolioReturnPoint) \n",
        "\n",
        "#display efficient set portfolio parameters\n",
        "print(\"Size of the  efficient set:\", xOptimalArray.shape )\n",
        "print(\"Optimal weights of the efficient set portfolios: \\n\", xOptimalArray)\n",
        "print(\"Annualized Risk and Return of the efficient set portfolios: \\n\", \\\n",
        "                                                np.c_[riskPoint, retPoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Kw9InlwDh3v",
        "outputId": "79996ad3-a1a9-4284-a168-c9caea707347"
      },
      "execution_count": 342,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_hessian_update_strategy.py:182: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
            "  warn('delta_grad == 0.0. Check if the approximated '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of the  efficient set: (266, 2)\n",
            "Optimal weights of the efficient set portfolios: \n",
            " [[0.19147597 0.80852403]\n",
            " [0.19303935 0.80696065]\n",
            " [0.19949414 0.80050586]\n",
            " [0.20124187 0.79875813]\n",
            " [0.20309764 0.79690236]\n",
            " [0.20505857 0.79494143]\n",
            " [0.20712071 0.79287929]\n",
            " [0.2092793  0.7907207 ]\n",
            " [0.21152886 0.78847114]\n",
            " [0.21386345 0.78613655]\n",
            " [0.21627678 0.78372322]\n",
            " [0.21876244 0.78123756]\n",
            " [0.23163842 0.76836158]\n",
            " [0.23388388 0.76611612]\n",
            " [0.23618429 0.76381571]\n",
            " [0.23892453 0.76107547]\n",
            " [0.24130016 0.75869984]\n",
            " [0.24372328 0.75627672]\n",
            " [0.24619299 0.75380701]\n",
            " [0.24870346 0.75129654]\n",
            " [0.25122177 0.74877823]\n",
            " [0.25381337 0.74618663]\n",
            " [0.2564398  0.7435602 ]\n",
            " [0.25909879 0.74090121]\n",
            " [0.26179442 0.73820558]\n",
            " [0.2645222  0.7354778 ]\n",
            " [0.26727472 0.73272528]\n",
            " [0.27005015 0.72994985]\n",
            " [0.27284665 0.72715335]\n",
            " [0.27566273 0.72433727]\n",
            " [0.27849678 0.72150322]\n",
            " [0.28134749 0.71865251]\n",
            " [0.28421348 0.71578652]\n",
            " [0.28709361 0.71290639]\n",
            " [0.28998677 0.71001323]\n",
            " [0.29319427 0.70680573]\n",
            " [0.29580812 0.70419188]\n",
            " [0.29873455 0.70126545]\n",
            " [0.3016704  0.6983296 ]\n",
            " [0.30090936 0.69909064]\n",
            " [0.30392871 0.69607129]\n",
            " [0.30695381 0.69304619]\n",
            " [0.30998412 0.69001588]\n",
            " [0.31301918 0.68698082]\n",
            " [0.31605855 0.68394145]\n",
            " [0.31910184 0.68089816]\n",
            " [0.32214871 0.67785129]\n",
            " [0.32519882 0.67480118]\n",
            " [0.3282519  0.6717481 ]\n",
            " [0.33130768 0.66869232]\n",
            " [0.33436593 0.66563407]\n",
            " [0.33742642 0.66257358]\n",
            " [0.34048898 0.65951102]\n",
            " [0.34877007 0.65122993]\n",
            " [0.35168    0.64832   ]\n",
            " [0.3545982  0.6454018 ]\n",
            " [0.35752411 0.64247589]\n",
            " [0.36045749 0.63954251]\n",
            " [0.36339795 0.63660205]\n",
            " [0.36634505 0.63365495]\n",
            " [0.36929852 0.63070148]\n",
            " [0.37225805 0.62774195]\n",
            " [0.3752233  0.6247767 ]\n",
            " [0.37819403 0.62180597]\n",
            " [0.38116989 0.61883011]\n",
            " [0.38415069 0.61584931]\n",
            " [0.38713616 0.61286384]\n",
            " [0.3901261  0.6098739 ]\n",
            " [0.39312021 0.60687979]\n",
            " [0.39611836 0.60388164]\n",
            " [0.39912031 0.60087969]\n",
            " [0.4021259  0.5978741 ]\n",
            " [0.40513494 0.59486506]\n",
            " [0.40814725 0.59185275]\n",
            " [0.41116269 0.58883731]\n",
            " [0.41418112 0.58581888]\n",
            " [0.41720235 0.58279765]\n",
            " [0.42022629 0.57977371]\n",
            " [0.42325233 0.57674767]\n",
            " [0.42627937 0.57372063]\n",
            " [0.42930876 0.57069124]\n",
            " [0.43234042 0.56765958]\n",
            " [0.43537423 0.56462577]\n",
            " [0.43841011 0.56158989]\n",
            " [0.44144798 0.55855202]\n",
            " [0.44448774 0.55551226]\n",
            " [0.44752931 0.55247069]\n",
            " [0.49753134 0.50246866]\n",
            " [0.45361761 0.54638239]\n",
            " [0.45666418 0.54333582]\n",
            " [0.45971228 0.54028772]\n",
            " [0.46276185 0.53723815]\n",
            " [0.46581283 0.53418717]\n",
            " [0.46886515 0.53113485]\n",
            " [0.47191876 0.52808124]\n",
            " [0.47497361 0.52502639]\n",
            " [0.47802965 0.52197035]\n",
            " [0.48108683 0.51891317]\n",
            " [0.48414513 0.51585487]\n",
            " [0.48720443 0.51279557]\n",
            " [0.49026478 0.50973522]\n",
            " [0.49332608 0.50667392]\n",
            " [0.49638832 0.50361168]\n",
            " [0.49945145 0.50054855]\n",
            " [0.50251544 0.49748456]\n",
            " [0.50558027 0.49441973]\n",
            " [0.50864588 0.49135412]\n",
            " [0.51171227 0.48828773]\n",
            " [0.51477939 0.48522061]\n",
            " [0.51784722 0.48215278]\n",
            " [0.5209501  0.4790499 ]\n",
            " [0.52401943 0.47598057]\n",
            " [0.52708934 0.47291066]\n",
            " [0.5301598  0.4698402 ]\n",
            " [0.53323078 0.46676922]\n",
            " [0.53630226 0.46369774]\n",
            " [0.53937424 0.46062576]\n",
            " [0.54244669 0.45755331]\n",
            " [0.5455196  0.4544804 ]\n",
            " [0.54859294 0.45140706]\n",
            " [0.55166671 0.44833329]\n",
            " [0.55474088 0.44525912]\n",
            " [0.55781545 0.44218455]\n",
            " [0.5608904  0.4391096 ]\n",
            " [0.56396572 0.43603428]\n",
            " [0.5670414  0.4329586 ]\n",
            " [0.57011742 0.42988258]\n",
            " [0.57319377 0.42680623]\n",
            " [0.57627045 0.42372955]\n",
            " [0.57934744 0.42065256]\n",
            " [0.58242473 0.41757527]\n",
            " [0.58550232 0.41449768]\n",
            " [0.58858019 0.41141981]\n",
            " [0.59165834 0.40834166]\n",
            " [0.59473675 0.40526325]\n",
            " [0.59781542 0.40218458]\n",
            " [0.60089435 0.39910565]\n",
            " [0.60397356 0.39602644]\n",
            " [0.60705292 0.39294708]\n",
            " [0.61013256 0.38986744]\n",
            " [0.61321242 0.38678758]\n",
            " [0.61629249 0.38370751]\n",
            " [0.61937278 0.38062722]\n",
            " [0.62245328 0.37754672]\n",
            " [0.62553397 0.37446603]\n",
            " [0.63466872 0.36533128]\n",
            " [0.63769822 0.36230178]\n",
            " [0.64072871 0.35927129]\n",
            " [0.64376023 0.35623977]\n",
            " [0.64679252 0.35320748]\n",
            " [0.64982576 0.35017424]\n",
            " [0.65285988 0.34714012]\n",
            " [0.65513064 0.34486936]\n",
            " [0.65895358 0.34104642]\n",
            " [0.6619829  0.3380171 ]\n",
            " [0.6650132  0.3349868 ]\n",
            " [0.66804446 0.33195554]\n",
            " [0.67107668 0.32892332]\n",
            " [0.67410983 0.32589017]\n",
            " [0.6771439  0.3228561 ]\n",
            " [0.68017888 0.31982112]\n",
            " [0.68321474 0.31678526]\n",
            " [0.68625149 0.31374851]\n",
            " [0.68928909 0.31071091]\n",
            " [0.69232754 0.30767246]\n",
            " [0.69536683 0.30463317]\n",
            " [0.69840693 0.30159307]\n",
            " [0.70144784 0.29855216]\n",
            " [0.70448955 0.29551045]\n",
            " [0.70753203 0.29246797]\n",
            " [0.71057528 0.28942472]\n",
            " [0.71361929 0.28638071]\n",
            " [0.71666403 0.28333597]\n",
            " [0.7197095  0.2802905 ]\n",
            " [0.72275569 0.27724431]\n",
            " [0.72580259 0.27419741]\n",
            " [0.72885017 0.27114983]\n",
            " [0.73189843 0.26810157]\n",
            " [0.73489465 0.26510535]\n",
            " [0.73794584 0.26205416]\n",
            " [0.74099758 0.25900242]\n",
            " [0.74404988 0.25595012]\n",
            " [0.74710271 0.25289729]\n",
            " [0.75015606 0.24984394]\n",
            " [0.75320993 0.24679007]\n",
            " [0.7562643  0.2437357 ]\n",
            " [0.75931916 0.24068084]\n",
            " [0.76237451 0.23762549]\n",
            " [0.76543032 0.23456968]\n",
            " [0.7684866  0.2315134 ]\n",
            " [0.77154333 0.22845667]\n",
            " [0.77460051 0.22539949]\n",
            " [0.77765813 0.22234187]\n",
            " [0.78071617 0.21928383]\n",
            " [0.78377465 0.21622535]\n",
            " [0.78683355 0.21316645]\n",
            " [0.78989287 0.21010713]\n",
            " [0.79295262 0.20704738]\n",
            " [0.79214736 0.20785264]\n",
            " [0.79522708 0.20477292]\n",
            " [0.79830704 0.20169296]\n",
            " [0.80138777 0.19861223]\n",
            " [0.80447005 0.19552995]\n",
            " [0.80755263 0.19244737]\n",
            " [0.81063549 0.18936451]\n",
            " [0.81371864 0.18628136]\n",
            " [0.81680207 0.18319793]\n",
            " [0.81988577 0.18011423]\n",
            " [0.82296973 0.17703027]\n",
            " [0.82605397 0.17394603]\n",
            " [0.82913846 0.17086154]\n",
            " [0.83626279 0.16373721]\n",
            " [0.8390181  0.1609819 ]\n",
            " [0.84177342 0.15822658]\n",
            " [0.84452873 0.15547127]\n",
            " [0.84728404 0.15271596]\n",
            " [0.84765081 0.15234919]\n",
            " [0.8507371  0.1492629 ]\n",
            " [0.85382365 0.14617635]\n",
            " [0.85830531 0.14169469]\n",
            " [0.86002193 0.13997807]\n",
            " [0.86613053 0.13386947]\n",
            " [0.86619314 0.13380686]\n",
            " [0.87279815 0.12720185]\n",
            " [0.87573373 0.12426627]\n",
            " [0.8787993  0.1212007 ]\n",
            " [0.88186139 0.11813861]\n",
            " [0.88490606 0.11509394]\n",
            " [0.88793442 0.11206558]\n",
            " [0.89096277 0.10903723]\n",
            " [0.89399112 0.10600888]\n",
            " [0.89701948 0.10298052]\n",
            " [0.90004783 0.09995217]\n",
            " [0.90156742 0.09843258]\n",
            " [0.90446751 0.09553249]\n",
            " [0.9073676  0.0926324 ]\n",
            " [0.9102677  0.0897323 ]\n",
            " [0.9131678  0.0868322 ]\n",
            " [0.91557052 0.08442948]\n",
            " [0.9186614  0.0813386 ]\n",
            " [0.92175219 0.07824781]\n",
            " [0.92484284 0.07515716]\n",
            " [0.92793329 0.07206671]\n",
            " [0.93444858 0.06555142]\n",
            " [0.93714833 0.06285167]\n",
            " [0.93984807 0.06015193]\n",
            " [0.94254782 0.05745218]\n",
            " [0.9433711  0.0566289 ]\n",
            " [0.94924771 0.05075229]\n",
            " [0.95230884 0.04769116]\n",
            " [0.95536997 0.04463003]\n",
            " [0.9584311  0.0415689 ]\n",
            " [0.96149222 0.03850778]\n",
            " [0.96268267 0.03731733]\n",
            " [0.96520615 0.03479385]\n",
            " [0.96733112 0.03266888]\n",
            " [0.97078627 0.02921373]\n",
            " [0.97654727 0.02345273]\n",
            " [0.97926454 0.02073546]\n",
            " [0.98220441 0.01779559]\n",
            " [0.98345294 0.01654706]\n",
            " [0.9869469  0.0130531 ]\n",
            " [0.99046078 0.00953922]\n",
            " [0.99267546 0.00732454]\n",
            " [0.99550229 0.00449771]\n",
            " [0.99867034 0.00132966]]\n",
            "Annualized Risk and Return of the efficient set portfolios: \n",
            " [[0.05495152 0.02395981]\n",
            " [0.05497115 0.02421181]\n",
            " [0.05507604 0.02446381]\n",
            " [0.05511103 0.02471581]\n",
            " [0.05515125 0.02496781]\n",
            " [0.05519718 0.02521981]\n",
            " [0.05524925 0.02547181]\n",
            " [0.05530791 0.02572381]\n",
            " [0.05537353 0.02597581]\n",
            " [0.05544647 0.02622781]\n",
            " [0.05552703 0.02647981]\n",
            " [0.05561546 0.02673181]\n",
            " [0.05616108 0.02698381]\n",
            " [0.05627106 0.02723581]\n",
            " [0.05638824 0.02748781]\n",
            " [0.05653373 0.02773981]\n",
            " [0.05666502 0.02799181]\n",
            " [0.05680384 0.02824381]\n",
            " [0.05695038 0.02849581]\n",
            " [0.05710454 0.02874781]\n",
            " [0.05726439 0.02899981]\n",
            " [0.05743431 0.02925181]\n",
            " [0.05761206 0.02950381]\n",
            " [0.05779764 0.02975581]\n",
            " [0.05799152 0.03000781]\n",
            " [0.05819354 0.03025981]\n",
            " [0.05840325 0.03051181]\n",
            " [0.05862062 0.03076381]\n",
            " [0.05884558 0.03101581]\n",
            " [0.05907805 0.03126781]\n",
            " [0.05931797 0.03151981]\n",
            " [0.05956526 0.03177181]\n",
            " [0.05981982 0.03202381]\n",
            " [0.06008156 0.03227581]\n",
            " [0.0603504  0.03252781]\n",
            " [0.06065527 0.03277981]\n",
            " [0.06090895 0.03303181]\n",
            " [0.06119847 0.03328381]\n",
            " [0.06149468 0.03353581]\n",
            " [0.06141734 0.03378781]\n",
            " [0.0617264  0.03403981]\n",
            " [0.06204201 0.03429181]\n",
            " [0.06236406 0.03454381]\n",
            " [0.06269244 0.03479581]\n",
            " [0.06302703 0.03504781]\n",
            " [0.06336772 0.03529981]\n",
            " [0.06371441 0.03555181]\n",
            " [0.06406699 0.03580381]\n",
            " [0.06442534 0.03605581]\n",
            " [0.06478936 0.03630781]\n",
            " [0.06515896 0.03655981]\n",
            " [0.06553401 0.03681181]\n",
            " [0.06591443 0.03706381]\n",
            " [0.066968   0.03731581]\n",
            " [0.06734662 0.03756781]\n",
            " [0.06773059 0.03781981]\n",
            " [0.0681198  0.03807181]\n",
            " [0.06851417 0.03832381]\n",
            " [0.06891361 0.03857581]\n",
            " [0.06931803 0.03882781]\n",
            " [0.06972733 0.03907981]\n",
            " [0.07014143 0.03933181]\n",
            " [0.07056023 0.03958381]\n",
            " [0.07098366 0.03983581]\n",
            " [0.07141162 0.04008781]\n",
            " [0.07184403 0.04033981]\n",
            " [0.0722808  0.04059181]\n",
            " [0.07272185 0.04084381]\n",
            " [0.07316709 0.04109581]\n",
            " [0.07361645 0.04134781]\n",
            " [0.07406985 0.04159981]\n",
            " [0.0745272  0.04185181]\n",
            " [0.07498843 0.04210381]\n",
            " [0.07545347 0.04235581]\n",
            " [0.07592223 0.04260781]\n",
            " [0.07639465 0.04285981]\n",
            " [0.07687065 0.04311181]\n",
            " [0.07735016 0.04336381]\n",
            " [0.07783304 0.04361581]\n",
            " [0.07831906 0.04386781]\n",
            " [0.07880839 0.04411981]\n",
            " [0.07930096 0.04437181]\n",
            " [0.07979672 0.04462381]\n",
            " [0.08029559 0.04487581]\n",
            " [0.08079753 0.04512781]\n",
            " [0.08130246 0.04537981]\n",
            " [0.08181034 0.04563181]\n",
            " [0.09049824 0.04588381]\n",
            " [0.08283469 0.04613581]\n",
            " [0.08335106 0.04638781]\n",
            " [0.08387014 0.04663981]\n",
            " [0.08439189 0.04689181]\n",
            " [0.08491626 0.04714381]\n",
            " [0.0854432  0.04739581]\n",
            " [0.08597265 0.04764781]\n",
            " [0.08650456 0.04789981]\n",
            " [0.0870389  0.04815181]\n",
            " [0.08757561 0.04840381]\n",
            " [0.08811466 0.04865581]\n",
            " [0.08865598 0.04890781]\n",
            " [0.08919954 0.04915981]\n",
            " [0.0897453  0.04941181]\n",
            " [0.09029322 0.04966381]\n",
            " [0.09084326 0.04991581]\n",
            " [0.09139537 0.05016781]\n",
            " [0.09194951 0.05041981]\n",
            " [0.09250566 0.05067181]\n",
            " [0.09306376 0.05092381]\n",
            " [0.09362379 0.05117581]\n",
            " [0.09418571 0.05142781]\n",
            " [0.09475581 0.05167981]\n",
            " [0.09532146 0.05193181]\n",
            " [0.09588887 0.05218381]\n",
            " [0.09645803 0.05243581]\n",
            " [0.0970289  0.05268781]\n",
            " [0.09760145 0.05293981]\n",
            " [0.09817564 0.05319181]\n",
            " [0.09875146 0.05344381]\n",
            " [0.09932886 0.05369581]\n",
            " [0.09990783 0.05394781]\n",
            " [0.10048833 0.05419981]\n",
            " [0.10107033 0.05445181]\n",
            " [0.10165382 0.05470381]\n",
            " [0.10223875 0.05495581]\n",
            " [0.10282512 0.05520781]\n",
            " [0.10341289 0.05545981]\n",
            " [0.10400203 0.05571181]\n",
            " [0.10459254 0.05596381]\n",
            " [0.10518437 0.05621581]\n",
            " [0.10577751 0.05646781]\n",
            " [0.10637194 0.05671981]\n",
            " [0.10696763 0.05697181]\n",
            " [0.10756456 0.05722381]\n",
            " [0.10816271 0.05747581]\n",
            " [0.10876207 0.05772781]\n",
            " [0.10936261 0.05797981]\n",
            " [0.1099643  0.05823181]\n",
            " [0.11056715 0.05848381]\n",
            " [0.11117111 0.05873581]\n",
            " [0.11177617 0.05898781]\n",
            " [0.11238232 0.05923981]\n",
            " [0.11298955 0.05949181]\n",
            " [0.11359782 0.05974381]\n",
            " [0.11420712 0.05999581]\n",
            " [0.11481744 0.06024781]\n",
            " [0.11663274 0.06049981]\n",
            " [0.11723658 0.06075181]\n",
            " [0.11784149 0.06100381]\n",
            " [0.11844747 0.06125581]\n",
            " [0.11905445 0.06150781]\n",
            " [0.11966245 0.06175981]\n",
            " [0.12027146 0.06201181]\n",
            " [0.12072777 0.06226381]\n",
            " [0.12149702 0.06251581]\n",
            " [0.12210746 0.06276781]\n",
            " [0.12271888 0.06301981]\n",
            " [0.12333125 0.06327181]\n",
            " [0.12394456 0.06352381]\n",
            " [0.12455881 0.06377581]\n",
            " [0.12517397 0.06402781]\n",
            " [0.12579004 0.06427981]\n",
            " [0.126407   0.06453181]\n",
            " [0.12702484 0.06478381]\n",
            " [0.12764354 0.06503581]\n",
            " [0.1282631  0.06528781]\n",
            " [0.1288835  0.06553981]\n",
            " [0.12950472 0.06579181]\n",
            " [0.13012677 0.06604381]\n",
            " [0.13074962 0.06629581]\n",
            " [0.13137327 0.06654781]\n",
            " [0.1319977  0.06679981]\n",
            " [0.1326229  0.06705181]\n",
            " [0.13324887 0.06730381]\n",
            " [0.13387558 0.06755581]\n",
            " [0.13450304 0.06780781]\n",
            " [0.13513123 0.06805981]\n",
            " [0.13576013 0.06831181]\n",
            " [0.13638975 0.06856381]\n",
            " [0.13700917 0.06881581]\n",
            " [0.13764049 0.06906781]\n",
            " [0.13827249 0.06931981]\n",
            " [0.13890513 0.06957181]\n",
            " [0.13953843 0.06982381]\n",
            " [0.14017235 0.07007581]\n",
            " [0.14080691 0.07032781]\n",
            " [0.14144207 0.07057981]\n",
            " [0.14207785 0.07083181]\n",
            " [0.14271423 0.07108381]\n",
            " [0.1433512  0.07133581]\n",
            " [0.14398875 0.07158781]\n",
            " [0.14462688 0.07183981]\n",
            " [0.14526557 0.07209181]\n",
            " [0.14590482 0.07234381]\n",
            " [0.14654463 0.07259581]\n",
            " [0.14718498 0.07284781]\n",
            " [0.14782587 0.07309981]\n",
            " [0.14846729 0.07335181]\n",
            " [0.14910924 0.07360381]\n",
            " [0.14894025 0.07385581]\n",
            " [0.14958672 0.07410781]\n",
            " [0.15023367 0.07435981]\n",
            " [0.15088121 0.07461181]\n",
            " [0.1515295  0.07486381]\n",
            " [0.15217827 0.07511581]\n",
            " [0.15282752 0.07536781]\n",
            " [0.15347723 0.07561981]\n",
            " [0.15412741 0.07587181]\n",
            " [0.15477804 0.07612381]\n",
            " [0.15542912 0.07637581]\n",
            " [0.15608065 0.07662781]\n",
            " [0.15673262 0.07687981]\n",
            " [0.15823992 0.07713181]\n",
            " [0.1588234  0.07738381]\n",
            " [0.15940716 0.07763581]\n",
            " [0.15999122 0.07788781]\n",
            " [0.16057556 0.07813981]\n",
            " [0.16065336 0.07839181]\n",
            " [0.16130827 0.07864381]\n",
            " [0.16196358 0.07889581]\n",
            " [0.16291569 0.07914781]\n",
            " [0.16328058 0.07939981]\n",
            " [0.16457985 0.07965181]\n",
            " [0.16459317 0.07990381]\n",
            " [0.16599948 0.08015581]\n",
            " [0.16662499 0.08040781]\n",
            " [0.1672785  0.08065981]\n",
            " [0.16793157 0.08091181]\n",
            " [0.16858123 0.08116381]\n",
            " [0.1692277  0.08141581]\n",
            " [0.16987446 0.08166781]\n",
            " [0.17052151 0.08191981]\n",
            " [0.17116884 0.08217181]\n",
            " [0.17181646 0.08242381]\n",
            " [0.17214152 0.08267581]\n",
            " [0.1727621  0.08292781]\n",
            " [0.17338293 0.08317981]\n",
            " [0.174004   0.08343181]\n",
            " [0.17462532 0.08368381]\n",
            " [0.17514027 0.08393581]\n",
            " [0.17580294 0.08418781]\n",
            " [0.17646587 0.08443981]\n",
            " [0.17712903 0.08469181]\n",
            " [0.17779241 0.08494381]\n",
            " [0.1791918  0.08519581]\n",
            " [0.179772   0.08544781]\n",
            " [0.18035239 0.08569981]\n",
            " [0.18093297 0.08595181]\n",
            " [0.18111006 0.08620381]\n",
            " [0.1823746  0.08645581]\n",
            " [0.18303365 0.08670781]\n",
            " [0.18369294 0.08695981]\n",
            " [0.18435245 0.08721181]\n",
            " [0.1850122  0.08746381]\n",
            " [0.18526883 0.08771581]\n",
            " [0.18581294 0.08796781]\n",
            " [0.18627124 0.08821981]\n",
            " [0.18701666 0.08847181]\n",
            " [0.18826017 0.08872381]\n",
            " [0.18884696 0.08897581]\n",
            " [0.189482   0.08922781]\n",
            " [0.18975176 0.08947981]\n",
            " [0.19050685 0.08973181]\n",
            " [0.19126651 0.08998381]\n",
            " [0.19174544 0.09023581]\n",
            " [0.1923569  0.09048781]\n",
            " [0.19304238 0.09073981]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Graph Efficient Frontier\n",
        "\n",
        "NoPoints = riskPoint.size\n",
        "\n",
        "colours = \"green\"\n",
        "area = np.pi*3\n",
        "\n",
        "plt.title('Efficient Frontier for MV Portfolio')\n",
        "plt.xlabel('Annualized Risk(%)')\n",
        "plt.ylabel('Annualized Expected Portfolio Return(%)' )\n",
        "plt.scatter(riskPoint, retPoint, s=area, c=colours, alpha =0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "2_0opIvqHAG1",
        "outputId": "ea1ceb76-74ea-4a6d-8605-0a27e51aeb78"
      },
      "execution_count": 343,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Portfolio"
      ],
      "metadata": {
        "id": "ZMoyxpN5dY_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "naive_risks = []\n",
        "naive_returns = []\n",
        "\n",
        "for x in np.arange(0, 1, 0.01):\n",
        "  weights = [x, 1-x]\n",
        "  risk = np.matmul((np.matmul(weights,cov)),np.transpose(weights)) * trading_days_in_year\n",
        "  naive_risks.append(np.sqrt(risk))\n",
        "\n",
        "  #obtain expected portfolio annualized return for the \n",
        "  #efficient set portfolios, for trading days = 251\n",
        "  ret = trading_days_in_year*(np.matmul(weights,r))\n",
        "  naive_returns.append(ret)\n",
        "\n",
        "#display efficient set portfolio parameters\n",
        "# print(\"Size of the  efficient set:\", xOptimalArray.shape )\n",
        "# print(\"Optimal weights of the efficient set portfolios: \\n\", xOptimalArray)\n",
        "print(\"Annualized Risk and Return of the efficient set portfolios: \\n\", \\\n",
        "                                                np.c_[naive_risks, naive_returns])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGhdT3AWHGa-",
        "outputId": "ab7b91a5-8641-46cb-fa29-486e845a4021"
      },
      "execution_count": 344,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Annualized Risk and Return of the efficient set portfolios: \n",
            " [[0.06810772 0.00939684]\n",
            " [0.06679558 0.0102117 ]\n",
            " [0.06553492 0.01102656]\n",
            " [0.06432878 0.01184142]\n",
            " [0.06318029 0.01265629]\n",
            " [0.06209263 0.01347115]\n",
            " [0.06106907 0.01428601]\n",
            " [0.06011286 0.01510087]\n",
            " [0.05922729 0.01591574]\n",
            " [0.05841555 0.0167306 ]\n",
            " [0.05768077 0.01754546]\n",
            " [0.05702592 0.01836032]\n",
            " [0.05645379 0.01917519]\n",
            " [0.0559669  0.01999005]\n",
            " [0.0555675  0.02080491]\n",
            " [0.0552575  0.02161978]\n",
            " [0.05503839 0.02243464]\n",
            " [0.05491126 0.0232495 ]\n",
            " [0.05487677 0.02406436]\n",
            " [0.05493507 0.02487923]\n",
            " [0.05508588 0.02569409]\n",
            " [0.05532844 0.02650895]\n",
            " [0.05566154 0.02732381]\n",
            " [0.05608358 0.02813868]\n",
            " [0.05659257 0.02895354]\n",
            " [0.05718619 0.0297684 ]\n",
            " [0.05786182 0.03058326]\n",
            " [0.05861664 0.03139813]\n",
            " [0.05944763 0.03221299]\n",
            " [0.06035164 0.03302785]\n",
            " [0.06132544 0.03384271]\n",
            " [0.06236577 0.03465758]\n",
            " [0.06346934 0.03547244]\n",
            " [0.06463293 0.0362873 ]\n",
            " [0.06585336 0.03710216]\n",
            " [0.06712751 0.03791703]\n",
            " [0.06845239 0.03873189]\n",
            " [0.06982512 0.03954675]\n",
            " [0.07124293 0.04036161]\n",
            " [0.07270318 0.04117648]\n",
            " [0.07420336 0.04199134]\n",
            " [0.0757411  0.0428062 ]\n",
            " [0.07731417 0.04362106]\n",
            " [0.07892045 0.04443593]\n",
            " [0.08055794 0.04525079]\n",
            " [0.0822248  0.04606565]\n",
            " [0.08391926 0.04688052]\n",
            " [0.0856397  0.04769538]\n",
            " [0.08738456 0.04851024]\n",
            " [0.08915243 0.0493251 ]\n",
            " [0.09094196 0.05013997]\n",
            " [0.0927519  0.05095483]\n",
            " [0.09458106 0.05176969]\n",
            " [0.09642837 0.05258455]\n",
            " [0.0982928  0.05339942]\n",
            " [0.10017338 0.05421428]\n",
            " [0.10206923 0.05502914]\n",
            " [0.10397952 0.055844  ]\n",
            " [0.10590346 0.05665887]\n",
            " [0.10784032 0.05747373]\n",
            " [0.10978941 0.05828859]\n",
            " [0.11175011 0.05910345]\n",
            " [0.1137218  0.05991832]\n",
            " [0.11570392 0.06073318]\n",
            " [0.11769596 0.06154804]\n",
            " [0.1196974  0.0623629 ]\n",
            " [0.1217078  0.06317777]\n",
            " [0.1237267  0.06399263]\n",
            " [0.12575371 0.06480749]\n",
            " [0.12778844 0.06562235]\n",
            " [0.12983052 0.06643722]\n",
            " [0.13187961 0.06725208]\n",
            " [0.13393539 0.06806694]\n",
            " [0.13599756 0.06888181]\n",
            " [0.13806584 0.06969667]\n",
            " [0.14013994 0.07051153]\n",
            " [0.14221962 0.07132639]\n",
            " [0.14430463 0.07214126]\n",
            " [0.14639475 0.07295612]\n",
            " [0.14848976 0.07377098]\n",
            " [0.15058946 0.07458584]\n",
            " [0.15269365 0.07540071]\n",
            " [0.15480215 0.07621557]\n",
            " [0.15691479 0.07703043]\n",
            " [0.1590314  0.07784529]\n",
            " [0.16115182 0.07866016]\n",
            " [0.16327592 0.07947502]\n",
            " [0.16540353 0.08028988]\n",
            " [0.16753454 0.08110474]\n",
            " [0.16966881 0.08191961]\n",
            " [0.17180622 0.08273447]\n",
            " [0.17394666 0.08354933]\n",
            " [0.17609002 0.08436419]\n",
            " [0.17823618 0.08517906]\n",
            " [0.18038506 0.08599392]\n",
            " [0.18253655 0.08680878]\n",
            " [0.18469056 0.08762364]\n",
            " [0.18684701 0.08843851]\n",
            " [0.18900581 0.08925337]\n",
            " [0.19116688 0.09006823]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NoPoints = len(naive_risks)\n",
        "\n",
        "colours = \"blue\"\n",
        "area = np.pi*3\n",
        "\n",
        "plt.title('Efficient Frontier for Naive Portfolio')\n",
        "plt.xlabel('Annualized Risk(%)')\n",
        "plt.ylabel('Annualized Expected Portfolio Return(%)' )\n",
        "plt.scatter(naive_risks, naive_returns, s=area, c=colours, alpha =0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "2wJgflp2eJ2c",
        "outputId": "7f43703b-ee4e-48d3-da87-92f61f217484"
      },
      "execution_count": 345,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combined Graph"
      ],
      "metadata": {
        "id": "A-x_ZCjwg57n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NoPoints = riskPoint.size\n",
        "\n",
        "colours = \"blue\"\n",
        "area = np.pi*3\n",
        "\n",
        "\n",
        "fig = plt.figure()\n",
        "ax1 = fig.add_subplot(111)\n",
        "\n",
        "plt.title('Efficient Frontier for MV and Naive Portfolio')\n",
        "plt.xlabel('Annualized Risk(%)')\n",
        "plt.ylabel('Annualized Expected Portfolio Return(%)' )\n",
        "ax1.scatter(riskPoint, retPoint, s=area, c=\"green\", alpha =0.5)\n",
        "ax1.scatter(naive_risks, naive_returns, s=area, c=\"blue\", alpha =0.5)\n",
        "# plt.xlim(riskPoint.min(), riskPoint.max())\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "DpqNZRCtgYQK",
        "outputId": "f33e6263-d501-4a56-8a96-c0e505313dbb"
      },
      "execution_count": 346,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cov"
      ],
      "metadata": {
        "id": "3sTlOQOIhTCZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "outputId": "395181da-b114-44e6-87ac-84f7e165d76f"
      },
      "execution_count": 347,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          high       low\n",
              "high  0.000148 -0.000018\n",
              "low  -0.000018  0.000018"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1fd062db-6821-4a7d-8a29-687a680e5cff\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>high</th>\n",
              "      <td>0.000148</td>\n",
              "      <td>-0.000018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>low</th>\n",
              "      <td>-0.000018</td>\n",
              "      <td>0.000018</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1fd062db-6821-4a7d-8a29-687a680e5cff')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1fd062db-6821-4a7d-8a29-687a680e5cff button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1fd062db-6821-4a7d-8a29-687a680e5cff');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 347
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(high_risk[\"Close\"].var())\n",
        "print(high_risk[\"Close\"].var() * trading_days_in_year)\n",
        "print(high_risk[\"Close\"].pct_change().var())\n",
        "print(high_risk[\"Close\"].pct_change().var() * trading_days_in_year)\n",
        "print(np.sqrt(high_risk[\"Close\"].pct_change().var() * trading_days_in_year))"
      ],
      "metadata": {
        "id": "-5N-KuYQxAmQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1de7928b-f473-45ce-ea2c-8de1c48d3f4e"
      },
      "execution_count": 348,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9864.007665920406\n",
            "2485729.9318119423\n",
            "0.00014831962827593315\n",
            "0.03737654632553515\n",
            "0.19333014851681862\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CU2640mM0RnU"
      },
      "execution_count": 348,
      "outputs": []
    }
  ]
}